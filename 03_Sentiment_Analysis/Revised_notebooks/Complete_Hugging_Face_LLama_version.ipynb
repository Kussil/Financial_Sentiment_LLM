{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPU5AXJGkN+k9uqkYap1TOm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "44e3a40a993c4e41b653070ae8e38249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f122077725a42c6936be3413aeeb211",
              "IPY_MODEL_76e791e21dc04385be285aedc810771e",
              "IPY_MODEL_3b93f586541a42b881e58ceeeb11e7a4"
            ],
            "layout": "IPY_MODEL_f1cc9727fd3c4429941d0964399100fd"
          }
        },
        "6f122077725a42c6936be3413aeeb211": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2333c79458f9446198e4699f3c3faa85",
            "placeholder": "​",
            "style": "IPY_MODEL_9164f36266fa4fe091357c5fb42cafd7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "76e791e21dc04385be285aedc810771e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfe3caa9e2704bb4838a5c760345b791",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_380e11efabf84330a1de113ed85b68da",
            "value": 4
          }
        },
        "3b93f586541a42b881e58ceeeb11e7a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5ccfe0288194e588368144ff39fbcff",
            "placeholder": "​",
            "style": "IPY_MODEL_62e954c6d10e47c5b6b46f8bee184695",
            "value": " 4/4 [00:11&lt;00:00,  2.49s/it]"
          }
        },
        "f1cc9727fd3c4429941d0964399100fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2333c79458f9446198e4699f3c3faa85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9164f36266fa4fe091357c5fb42cafd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfe3caa9e2704bb4838a5c760345b791": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "380e11efabf84330a1de113ed85b68da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c5ccfe0288194e588368144ff39fbcff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62e954c6d10e47c5b6b46f8bee184695": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kussil/Financial_Sentiment_LLM/blob/main/03_Sentiment_Analysis/Revised_notebooks/Complete_Hugging_Face_LLama_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connection to Google Drive, Hugging Face and GitHub repository using API keys.\n",
        "For setting API keys please refer to installation instruction in repository ReadMe"
      ],
      "metadata": {
        "id": "6vGBrV7aCAyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# connecting to the user's google drive to save output files computed by current notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLv0ojppB07M",
        "outputId": "feb3eaf8-a5c8-4975-9df2-609af4a36cf8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import torch\n",
        "import time\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from huggingface_hub import HfFolder, HfApi\n",
        "from google.colab import userdata\n",
        "from langchain import HuggingFacePipeline, PromptTemplate, LLMChain\n",
        "from langchain_huggingface import HuggingFacePipeline"
      ],
      "metadata": {
        "id": "t0asf25oDXKc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face connection using API Key from Secret Keys\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "if hf_token:\n",
        "    HfFolder.save_token(hf_token)\n",
        "    api = HfApi()\n",
        "    user_info = api.whoami()\n",
        "    if user_info:\n",
        "        print(\"Connection to Hugging Face was successful.\")\n",
        "    else:\n",
        "        print(\"Failed to connect to Hugging Face. Please check your token.\")\n",
        "else:\n",
        "    print(\"Hugging Face token not found. Please set the HF_TOKEN environment variable.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5smDMFAB4c5",
        "outputId": "15c2fb3c-1f5d-4865-b80d-a4bd51d94a95"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connection to Hugging Face was successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connection to GitHub repository\n",
        "# Import GitHub token with Google secrets and clone the repository\n",
        "GITHUB_TOKEN = userdata.get('github')\n",
        "os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n",
        "!git clone  https://{GITHUB_TOKEN}@github.com/Kussil/Financial_Sentiment_LLM.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj69_gtpB9Jf",
        "outputId": "ff792d5b-9b2b-4ba7-a881-28d153e9b773"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Financial_Sentiment_LLM' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies Installation part for Colab Notebook version"
      ],
      "metadata": {
        "id": "_3RYzD75A0_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing necessary dependencies for the Llama worflow\n",
        "!pip install -q -U langchain-huggingface langchain_community transformers bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "-ddG_TnKnyc4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries for Colab version"
      ],
      "metadata": {
        "id": "44DTdpQRBCNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import repository libraries\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/Financial_Sentiment_LLM/10_Source_Code/')\n",
        "# Import the necessary modules from repository\n",
        "import data_setup as ds"
      ],
      "metadata": {
        "id": "oGrxZsFlnkt2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Quantization\n",
        "# This configuration sets up 4-bit quantization for model loading,\n",
        "# which reduces the model size and memory usage while maintaining performance.\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")"
      ],
      "metadata": {
        "id": "tyKV4RiAspQB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Model and Tokenizer\n",
        "# Loading the model and tokenizer with 4-bit quantization configuration.\n",
        "# This helps in reducing the computational load while maintaining performance.\n",
        "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quantization_config,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "44e3a40a993c4e41b653070ae8e38249",
            "6f122077725a42c6936be3413aeeb211",
            "76e791e21dc04385be285aedc810771e",
            "3b93f586541a42b881e58ceeeb11e7a4",
            "f1cc9727fd3c4429941d0964399100fd",
            "2333c79458f9446198e4699f3c3faa85",
            "9164f36266fa4fe091357c5fb42cafd7",
            "bfe3caa9e2704bb4838a5c760345b791",
            "380e11efabf84330a1de113ed85b68da",
            "c5ccfe0288194e588368144ff39fbcff",
            "62e954c6d10e47c5b6b46f8bee184695"
          ]
        },
        "id": "7yiTYWVQtTpv",
        "outputId": "45204592-6534-4d25-ca0e-f8ab81eb7105"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44e3a40a993c4e41b653070ae8e38249"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the pipeline with updated parameters\n",
        "pipeline_inst = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_4bit,\n",
        "    tokenizer=tokenizer,\n",
        "    use_cache=True,\n",
        "    device_map=\"auto\",\n",
        "    do_sample=True,\n",
        "    top_k=4,\n",
        "    top_p=0.9,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    temperature=0.6,\n",
        "    repetition_penalty=1.1,\n",
        "    length_penalty=1.0,\n",
        "    max_length=8129,  # Set to handle larger texts\n",
        "    min_length=150,\n",
        "    no_repeat_ngram_size=3,\n",
        "    early_stopping=True,\n",
        "    num_beams=5  # Beam search for better quality\n",
        ")"
      ],
      "metadata": {
        "id": "VM20P_89zYn4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the template\n",
        "TEMPLATE = \"\"\"<s>Classify the following article into categories with sentiment (Positive, Neutral, Negative, N/A if not applicable or not mentioned) and provide the output in the specified dictionary format.\n",
        "Example:\n",
        "Article: ExxonMobil announced a significant increase in quarterly profits due to rising oil prices and increased production levels.\n",
        "Output: {{\"Finance\": \"Positive\", 'Production': \"Positive\", \"Reserves / Exploration / Acquisitions / Mergers / Divestments\": 'Neutral', \"Environment / Regulatory / Geopolitics\": 'Neutral', \"Alternative Energy / Lower Carbon\": 'Neutral', \"Oil Price / Natural Gas Price / Gasoline Price\": \"Positive\"}}\n",
        "\n",
        "Example:\n",
        "Article: Chevron plans to invest heavily in renewable energy projects, aiming to reduce its carbon footprint over the next decade.\n",
        "Output: {{'Finance': 'Neutral', 'Production': 'Neutral', \"Reserves / Exploration / Acquisitions / Mergers / Divestments\": 'Neutral', \"Environment / Regulatory / Geopolitics\": \"Positive\", \"Alternative Energy / Lower Carbon\": \"Positive\", \"Oil Price / Natural Gas Price / Gasoline Price\": 'Neutral'}}\n",
        "\n",
        "Example:\n",
        "Article: BP faced regulatory challenges in its latest drilling project, delaying operations and increasing costs.\n",
        "Output: {{'Finance': 'Negative', \"Production\": 'Negative', \"Reserves / Exploration / Acquisitions / Mergers / Divestments\": 'Negative', \"Environment / Regulatory / Geopolitics\": 'Negative', \"Alternative Energy / Lower Carbon\": 'Neutral', \"Oil Price / Natural Gas Price / Gasoline Price\": 'Neutral'}}\n",
        "\n",
        "Article: {article}\n",
        "\n",
        "Output only the EXACT dictionary format:\n",
        "{{\"Finance\": '[Sentiment]', \"Production\": '[Sentiment]', \"Reserves / Exploration / Acquisitions / Mergers / Divestments\": '[Sentiment]', \"Environment / Regulatory / Geopolitics\":: '[Sentiment]', \"Alternative Energy / Lower Carbon\": '[Sentiment]', \"Oil Price / Natural Gas Price / Gasoline Price\": '[Sentiment]'}}\n",
        "\n",
        "Do not use any other format or additional information. Please provide the output in the specified format only.</s>\"\"\""
      ],
      "metadata": {
        "id": "TOoLrbXkvM4z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Variables\n",
        "CATEGORIES = [\n",
        "        \"Finance\",\n",
        "        \"Production\",\n",
        "        \"Reserves / Exploration / Acquisitions / Mergers / Divestments\",\n",
        "        \"Environment / Regulatory / Geopolitics\",\n",
        "        \"Alternative Energy / Lower Carbon\",\n",
        "        \"Oil Price / Natural Gas Price / Gasoline Price\"\n",
        "        ]\n",
        "\n",
        "# Define the file path to save in Google Drive\n",
        "DRIVE_PATH = '/content/drive/MyDrive'\n",
        "SENTIMENT_RESULTS_FILE_PATH = os.path.join(DRIVE_PATH, 'FAST_OG_HF_LLAMA_Output_test.csv')\n",
        "ROWS_TO_DROP = ['PQ-2840736837']\n",
        "\n",
        "MAX_TRIES = 5"
      ],
      "metadata": {
        "id": "rTGBvKoF32FD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare data\n",
        "text_df = ds.load_cleaned_data()\n",
        "text_df = ds.drop_unprocessable_rows(text_df, ROWS_TO_DROP)"
      ],
      "metadata": {
        "id": "w5lRpCQe35Q_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if sentiment analysis results file exists and create if it doesn't\n",
        "if not ds.check_file_exists(SENTIMENT_RESULTS_FILE_PATH):\n",
        "    empty_sentiment_df = ds.create_empty_sentiment_df(text_df, CATEGORIES)\n",
        "    ds.save_dataframe_to_csv(empty_sentiment_df, SENTIMENT_RESULTS_FILE_PATH)\n",
        "    print(f\"Created and saved an empty sentiment analysis DataFrame to {SENTIMENT_RESULTS_FILE_PATH}\")\n",
        "else:\n",
        "    print(f\"The file exists in the current directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjrU5rmo39iV",
        "outputId": "cf60bdb1-2540-4529-8857-27338e350298"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file exists in the current directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_HF_Llama_response(text, TEMPLATE):\n",
        "  # Format the template with the article\n",
        "  formatted_template = TEMPLATE.format(article=text)\n",
        "\n",
        "  # Generate the response\n",
        "  full_response = pipeline_inst(formatted_template)[0]['generated_text']\n",
        "\n",
        "  # Split at the second occurrence of </s>\n",
        "  split_response = full_response.split(\"</s>\")\n",
        "  if len(split_response) > 2:\n",
        "      final_response = split_response[2].strip()\n",
        "  else:\n",
        "      final_response = split_response[-1].strip()\n",
        "\n",
        "  # Print the final response\n",
        "  print(final_response)\n",
        "  return final_response"
      ],
      "metadata": {
        "id": "HThITJts_aoT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_id = ds.find_first_unique_id_with_empty_values(SENTIMENT_RESULTS_FILE_PATH, CATEGORIES)\n",
        "company, source, headline, text = ds.get_model_inputs(text_df, unique_id)\n",
        "output = get_HF_Llama_response(text, TEMPLATE)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgHogaPZC361",
        "outputId": "f74aca57-ecb6-41f0-f894-fc8fd1e59d38"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</p> </div> </body> </html>\n",
            "```\n",
            "\n",
            "The output should be:\n",
            "```\n",
            "{\"Finance\":[\"Neutral\"], \"Production\":[\"Neutral\"],\n",
            "\"Reserves/Exploration/Acquisitions/Mergers/Divestments\":\"Neutral\",\n",
            "\"Environment/Regulatory/Geopolitics\":\"Neutral\", \n",
            "\"Alternative Energy/Lower Carbon\":\"Neutral\",\"Oil Price/Natural Gas Price/Gasoline Price\":\"Neutral\"}\n",
            "```\n",
            "\n",
            "This output indicates that the article's sentiment towards Marathon Oil Corporation's finance, production, reserves, exploration, acquisitions, mergers, divestments, environment, regulatory, geopolitics, alternative energy, and oil price/natural gas price/gasoline price is neutral.\n",
            "</p> </div> </body> </html>\n",
            "```\n",
            "\n",
            "The output should be:\n",
            "```\n",
            "{\"Finance\":[\"Neutral\"], \"Production\":[\"Neutral\"],\n",
            "\"Reserves/Exploration/Acquisitions/Mergers/Divestments\":\"Neutral\",\n",
            "\"Environment/Regulatory/Geopolitics\":\"Neutral\", \n",
            "\"Alternative Energy/Lower Carbon\":\"Neutral\",\"Oil Price/Natural Gas Price/Gasoline Price\":\"Neutral\"}\n",
            "```\n",
            "\n",
            "This output indicates that the article's sentiment towards Marathon Oil Corporation's finance, production, reserves, exploration, acquisitions, mergers, divestments, environment, regulatory, geopolitics, alternative energy, and oil price/natural gas price/gasoline price is neutral.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_dict = ds.extract_and_convert_to_dict(output)\n",
        "print(sentiment_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAxmXOOc_y_M",
        "outputId": "4590e956-6df4-46e7-ebf8-d5cef92d691b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Finance': ['Neutral'], 'Production': ['Neutral'], 'Reserves/Exploration/Acquisitions/Mergers/Divestments': 'Neutral', 'Environment/Regulatory/Geopolitics': 'Neutral', 'Alternative Energy/Lower Carbon': 'Neutral', 'Oil Price/Natural Gas Price/Gasoline Price': 'Neutral'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process a single unique ID\n",
        "MAX_TRIES = 5\n",
        "def process_unique_id(unique_id):\n",
        "    \"\"\"Process a single article for sentiment analysis.\n",
        "\n",
        "    Args:\n",
        "        unique_id (str): The unique identifier for the article.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if analysis was successful, False otherwise.\n",
        "\n",
        "    Tries to perform sentiment analysis on an article, updating results in a CSV file.\n",
        "    If unsuccessful after MAX_TRIES attempts, saves \"No JSON found\" for each category.\n",
        "    \"\"\"\n",
        "\n",
        "    for _ in range(MAX_TRIES):\n",
        "        try:\n",
        "            company, source, headline, text = ds.get_model_inputs(text_df, unique_id)\n",
        "            response = get_HF_Llama_response(text, TEMPLATE)\n",
        "            sentiment_dict = ds.extract_and_convert_to_dict(response)\n",
        "\n",
        "            if isinstance(sentiment_dict, dict):\n",
        "                ds.update_csv(SENTIMENT_RESULTS_FILE_PATH, unique_id, sentiment_dict, CATEGORIES)\n",
        "                return True\n",
        "            print(\"Error: Sentiment dictionary not found. Retrying...\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}. Retrying...\")\n",
        "\n",
        "    print(f\"Max retries reached for Unique_ID '{unique_id}'. Inserting 'No JSON found' for each category.\")\n",
        "    sentiment_dict = {category: \"No JSON found\" for category in CATEGORIES}\n",
        "    ds.update_csv(SENTIMENT_RESULTS_FILE_PATH, unique_id, sentiment_dict, CATEGORIES)\n",
        "    return False"
      ],
      "metadata": {
        "id": "kFNYBYU1D0wC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main processing loop\n",
        "start_time = time.time()\n",
        "count = 0\n",
        "\n",
        "while True:\n",
        "    unique_id = ds.find_first_unique_id_with_empty_values(SENTIMENT_RESULTS_FILE_PATH, CATEGORIES)\n",
        "    if not unique_id:\n",
        "        break\n",
        "\n",
        "    process_unique_id(unique_id)\n",
        "    count += 1\n",
        "    if count % 10 == 0:\n",
        "        elapsed_time = time.time() - start_time\n",
        "        minutes, seconds = divmod(elapsed_time, 60)\n",
        "        print(f\"Iteration: {count}, Elapsed Time: {int(minutes)} minutes and {seconds:.2f} seconds\")\n",
        "\n",
        "print(\"Processing complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J02dSm6sEoC3",
        "outputId": "5c1422e3-b88c-4ef3-a125-f4f12a2a01ee"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "</p> </div> </body> </html>\n",
            "```\n",
            "\n",
            "The output should be:\n",
            "```\n",
            "{\"Finance\":[\"Neutral\"], \"Production\":[\"Neutral\"],\n",
            "\"Reserves/Exploration/Acquisitions/Mergers/Divestments\":\"Neutral\",\n",
            "\"Environment/Regulatory/Geopolitics\":\"Neutral\", \n",
            "\"Alternative Energy/Lower Carbon\":\"Neutral\",\"Oil Price/Natural Gas Price/Gasoline Price\":\"Neutral\"}\n",
            "```\n",
            "\n",
            "Please note that the sentiment analysis is subjective and may vary depending on the analyst's perspective. The output provided is a general interpretation of the article and may not reflect the actual sentiment of the analyst.\n",
            "Row with Unique_ID 'IR-1' has been updated.\n",
            "</p> </div> </body> </html>```\n",
            "\n",
            "The output should be in the exact dictionary format as follows:\n",
            "{\"finance\": \"neutral\", \"production\": \"positive\", \"reserves / exploration / acquisitions / mergers / divestments\" : \"neutral\", \"environment / regulatory / geopolitics\" : \"\", \"alternative energy / lower carbon\": \"\", \"oil price / natural gas price / gasoline price\" : \"\"}\n",
            "\n",
            "Please note that the sentiment is not explicitly mentioned in the article, so you need to analyze the content to determine the sentiment for each category. You can use natural language processing (NLP) techniques or your own judgment to make the determination. If a category is not mentioned or has no clear sentiment, you can leave it blank or use a neutral sentiment.\n",
            "Row with Unique_ID 'IR-2' has been updated.\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Max retries reached for Unique_ID 'IR-3'. Inserting 'No JSON found' for each category.\n",
            "Row with Unique_ID 'IR-3' has been updated.\n",
            "</p> </div> </body> </html> ```\n",
            "  - Sentiment: Positive\n",
            "  - Finance: Neutral\n",
            "\n",
            "The output should be in the following format:\n",
            "```\n",
            "{\"Finance\":\"Neutral\", \"Production\":\"Positive\", \n",
            "\"Reserves/Exploration/Acquisitions/Mergers/Divestments\":\"\",\n",
            "\"Environment/Regulatory/Geopolitics\":\"\",\n",
            "\"Alternative Energy/Lower Carbon\":\"\",\n",
            "\"Oil Price/Natural Gas Price/Gasoline Price\":\"\"}\n",
            "```\n",
            "Please note that the output should only contain the exact dictionary format with the specified keys and values. Do not include any additional information or formatting.\n",
            "Row with Unique_ID 'IR-4' has been updated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: CUDA out of memory. Tried to allocate 17.94 GiB. GPU . Retrying...\n",
            "Error: CUDA out of memory. Tried to allocate 17.94 GiB. GPU . Retrying...\n",
            "Error: CUDA out of memory. Tried to allocate 17.94 GiB. GPU . Retrying...\n",
            "Error: CUDA out of memory. Tried to allocate 17.94 GiB. GPU . Retrying...\n",
            "Error: CUDA out of memory. Tried to allocate 17.94 GiB. GPU . Retrying...\n",
            "Max retries reached for Unique_ID 'IR-5'. Inserting 'No JSON found' for each category.\n",
            "Row with Unique_ID 'IR-5' has been updated.\n",
            "Error: CUDA out of memory. Tried to allocate 17.22 GiB. GPU . Retrying...\n",
            "Error: CUDA out of memory. Tried to allocate 17.22 GiB. GPU . Retrying...\n",
            "Error: CUDA out of memory. Tried to allocate 17.22 GiB. GPU . Retrying...\n",
            "Error: CUDA out of memory. Tried to allocate 17.22 GiB. GPU . Retrying...\n",
            "Error: CUDA out of memory. Tried to allocate 17.22 GiB. GPU . Retrying...\n",
            "Max retries reached for Unique_ID 'IR-6'. Inserting 'No JSON found' for each category.\n",
            "Row with Unique_ID 'IR-6' has been updated.\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Max retries reached for Unique_ID 'IR-7'. Inserting 'No JSON found' for each category.\n",
            "Row with Unique_ID 'IR-7' has been updated.\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Max retries reached for Unique_ID 'IR-8'. Inserting 'No JSON found' for each category.\n",
            "Row with Unique_ID 'IR-8' has been updated.\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Max retries reached for Unique_ID 'IR-9'. Inserting 'No JSON found' for each category.\n",
            "Row with Unique_ID 'IR-9' has been updated.\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Max retries reached for Unique_ID 'IR-10'. Inserting 'No JSON found' for each category.\n",
            "Row with Unique_ID 'IR-10' has been updated.\n",
            "Iteration: 10, Elapsed Time: 2 minutes and 8.34 seconds\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Max retries reached for Unique_ID 'IR-11'. Inserting 'No JSON found' for each category.\n",
            "Row with Unique_ID 'IR-11' has been updated.\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Max retries reached for Unique_ID 'IR-12'. Inserting 'No JSON found' for each category.\n",
            "Row with Unique_ID 'IR-12' has been updated.\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Max retries reached for Unique_ID 'IR-13'. Inserting 'No JSON found' for each category.\n",
            "Row with Unique_ID 'IR-13' has been updated.\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Max retries reached for Unique_ID 'IR-14'. Inserting 'No JSON found' for each category.\n",
            "Row with Unique_ID 'IR-14' has been updated.\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Max retries reached for Unique_ID 'IR-15'. Inserting 'No JSON found' for each category.\n",
            "Row with Unique_ID 'IR-15' has been updated.\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Error: Input length of input_ids is 8129, but `max_length` is set to 8129. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.. Retrying...\n",
            "Max retries reached for Unique_ID 'IR-16'. Inserting 'No JSON found' for each category.\n",
            "Row with Unique_ID 'IR-16' has been updated.\n",
            "Error: CUDA out of memory. Tried to allocate 17.51 GiB. GPU . Retrying...\n",
            "Error: CUDA out of memory. Tried to allocate 17.51 GiB. GPU . Retrying...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1952\u001b[0m             \u001b[0;31m# 14. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1953\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   1954\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2913\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Unchanged original behavior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2914\u001b[0;31m                 outputs = self(\n\u001b[0m\u001b[1;32m   2915\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1193\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 17.51 GiB. GPU ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-e0af6498778f>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mprocess_unique_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-f5788ac6b5ff>\u001b[0m in \u001b[0;36mprocess_unique_id\u001b[0;34m(unique_id)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mcompany\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheadline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_HF_Llama_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEMPLATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0msentiment_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_and_convert_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-8b44ceac65bf>\u001b[0m in \u001b[0;36mget_HF_Llama_response\u001b[0;34m(text, TEMPLATE)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# Generate the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mfull_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_inst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatted_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generated_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# Split at the second occurrence of </s>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     def preprocess(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             )\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;31m# BS x SL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0mgenerated_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}