{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local LLama notebook\n",
    "\n",
    "Current notebook utilizes local LLama model installed on a local machine.\\\n",
    "Requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "project_dir = os\n",
    "# Determine the project directory from the current working directory\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "source_code_dir = os.path.join(project_dir, '10_Source_Code')\n",
    "# Add the path to the directory containing your module to the system path\n",
    "sys.path.append(source_code_dir)\n",
    "import llama_setup as ls\n",
    "import data_setup as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "CATEGORIES = [\n",
    "        \"Finance\",\n",
    "        \"Production\",\n",
    "        \"Reserves / Exploration / Acquisitions / Mergers / Divestments\",\n",
    "        \"Environment / Regulatory / Geopolitics\",\n",
    "        \"Alternative Energy / Lower Carbon\",\n",
    "        \"Oil Price / Natural Gas Price / Gasoline Price\"]\n",
    "\n",
    "SENTIMENT_RESULTS_FILE_PATH = 'Full_data_LLama_model_sentiment_analysis_results.csv'\n",
    "\n",
    "ROWS_TO_DROP = ['PQ-2840736837']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = ds.load_cleaned_data()\n",
    "text_df = ds.drop_unprocessable_rows(text_df, ROWS_TO_DROP)\n",
    "#print(f\"Dropped rows: {ROWS_TO_DROP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file exists in the current directory.\n"
     ]
    }
   ],
   "source": [
    "# Check if sentiment analysis results file exists\n",
    "file_exists = ds.check_file_exists(SENTIMENT_RESULTS_FILE_PATH)\n",
    "\n",
    "if file_exists:\n",
    "    print(f\"The file exists in the current directory.\")\n",
    "else:\n",
    "    print(f\"The file does not exist in the current directory.\")\n",
    "    empty_sentiment_df = ds.create_empty_sentiment_df(text_df, CATEGORIES)\n",
    "    ds.save_dataframe_to_csv(empty_sentiment_df, SENTIMENT_RESULTS_FILE_PATH)\n",
    "    print(f\"Created and saved an empty sentiment analysis DataFrame to {SENTIMENT_RESULTS_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Find the first unique ID with empty values\n",
    "unique_id = ds.find_first_unique_id_with_empty_values(SENTIMENT_RESULTS_FILE_PATH, CATEGORIES)\n",
    "print(unique_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get LLama inputs\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m company, source, headline, text \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print(f\"Company: {company}\\n\")\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# print(f\"Source: {source}\\n\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print(f\"Headline: {headline}\\n\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print(f\"Text:\\n{text}\")\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Financial_Sentiment_LLM/10_Source_Code/data_setup.py:168\u001b[0m, in \u001b[0;36mget_model_inputs\u001b[0;34m(text_df, unique_id)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03mRetrieves information from the DataFrame based on the unique ID and outputs company, source, headline, and text.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    tuple: A tuple containing company, source, headline, and text.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m row \u001b[38;5;241m=\u001b[39m text_df[text_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnique_ID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m unique_id]\n\u001b[0;32m--> 168\u001b[0m company \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTicker\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    169\u001b[0m source \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    170\u001b[0m headline \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle Headline\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# Get LLama inputs\n",
    "company, source, headline, text = ds.get_model_inputs(text_df, unique_id)\n",
    "# print(f\"Company: {company}\\n\")\n",
    "# print(f\"Source: {source}\\n\")\n",
    "# print(f\"Headline: {headline}\\n\")\n",
    "# print(f\"Text:\\n{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the template\n",
    "TEMPLATE = \"\"\"<s>Classify the following article into categories with sentiment (Positive, Neutral, Negative, N/A if not applicable or not mentioned) and provide the output in the specified dictionary format.\n",
    "Example:\n",
    "Article: ExxonMobil announced a significant increase in quarterly profits due to rising oil prices and increased production levels.\n",
    "Output: {{\"Finance\": \"Positive\", 'Production': \"Positive\", \"Reserves / Exploration / Acquisitions / Mergers / Divestments\": 'Neutral', \"Environment / Regulatory / Geopolitics\": 'Neutral', \"Alternative Energy / Lower Carbon\": 'Neutral', \"Oil Price / Natural Gas Price / Gasoline Price\": \"Positive\"}}\n",
    "\n",
    "Example:\n",
    "Article: Chevron plans to invest heavily in renewable energy projects, aiming to reduce its carbon footprint over the next decade.\n",
    "Output: {{'Finance': 'Neutral', 'Production': 'Neutral', \"Reserves / Exploration / Acquisitions / Mergers / Divestments\": 'Neutral', \"Environment / Regulatory / Geopolitics\": \"Positive\", \"Alternative Energy / Lower Carbon\": \"Positive\", \"Oil Price / Natural Gas Price / Gasoline Price\": 'Neutral'}}\n",
    "\n",
    "Example:\n",
    "Article: BP faced regulatory challenges in its latest drilling project, delaying operations and increasing costs.\n",
    "Output: {{'Finance': 'Negative', \"Production\": 'Negative', \"Reserves / Exploration / Acquisitions / Mergers / Divestments\": 'Negative', \"Environment / Regulatory / Geopolitics\": 'Negative', \"Alternative Energy / Lower Carbon\": 'Neutral', \"Oil Price / Natural Gas Price / Gasoline Price\": 'Neutral'}}\n",
    "\n",
    "Article: {article}\n",
    "\n",
    "Output only the EXACT dictionary format:\n",
    "{{\"Finance\": '[Sentiment]', \"Production\": '[Sentiment]', \"Reserves / Exploration / Acquisitions / Mergers / Divestments\": '[Sentiment]', \"Environment / Regulatory / Geopolitics\":: '[Sentiment]', \"Alternative Energy / Lower Carbon\": '[Sentiment]', \"Oil Price / Natural Gas Price / Gasoline Price\": '[Sentiment]'}}\n",
    "\n",
    "Do not use any other format or additional information. Please provide the output in the specified format only.</s>\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Prompt length exceeds maximum context length of 8192 tokens\n"
     ]
    }
   ],
   "source": [
    "response = ls.get_ollama_response(text, TEMPLATE)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Prompt length exceeds maximum context length of 8192 tokens\n",
      "No JSON object found in the response.\n",
      "Error: Sentiment dictionary not found. Skipping update.\n",
      "Row with Unique_ID 'SEC-119312519043841' has been updated.\n"
     ]
    }
   ],
   "source": [
    "response = ls.get_ollama_response(text, TEMPLATE)\n",
    "print(response)\n",
    "sentiment_dict = ds.extract_and_convert_to_dict(response)\n",
    "print(sentiment_dict)\n",
    "# Check if sentiment_dict is a dictionary before updating the CSV\n",
    "# Check if sentiment_dict is a dictionary before updating the CSV\n",
    "if isinstance(sentiment_dict, dict):\n",
    "    ds.update_csv(SENTIMENT_RESULTS_FILE_PATH, unique_id, sentiment_dict)\n",
    "else:\n",
    "    print(\"Error: Sentiment dictionary not found. Skipping update.\")\n",
    "    # Create a dictionary with 'No JSON found' for each category\n",
    "    sentiment_dict = {category: \"No JSON found\" for category in CATEGORIES}\n",
    "    ds.update_csv(SENTIMENT_RESULTS_FILE_PATH, unique_id, sentiment_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>URL</th>\n",
       "      <th>Finance</th>\n",
       "      <th>Production</th>\n",
       "      <th>Reserves / Exploration / Acquisitions / Mergers / Divestments</th>\n",
       "      <th>Environment / Regulatory / Geopolitics</th>\n",
       "      <th>Alternative Energy / Lower Carbon</th>\n",
       "      <th>Oil Price / Natural Gas Price / Gasoline Price</th>\n",
       "      <th>Reserves / Exploration / Acquisitions / Mergers / Divestitures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Investment Research</td>\n",
       "      <td>IR-1</td>\n",
       "      <td>MRO</td>\n",
       "      <td>2024-05-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Source Unique_ID Ticker        Date  URL  Finance Production  \\\n",
       "0  Investment Research      IR-1    MRO  2024-05-16  NaN  Neutral   Positive   \n",
       "\n",
       "  Reserves / Exploration / Acquisitions / Mergers / Divestments  \\\n",
       "0                                            Neutral              \n",
       "\n",
       "  Environment / Regulatory / Geopolitics Alternative Energy / Lower Carbon  \\\n",
       "0                                Neutral                    Not applicable   \n",
       "\n",
       "  Oil Price / Natural Gas Price / Gasoline Price  \\\n",
       "0                                        Neutral   \n",
       "\n",
       "  Reserves / Exploration / Acquisitions / Mergers / Divestitures  \n",
       "0                                                NaN              "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "review_df = pd.read_csv(SENTIMENT_RESULTS_FILE_PATH)\n",
    "review_df.query('Unique_ID == \"IR-1\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Sentiment dictionary not found. Skipping update.\n",
      "Error: Sentiment dictionary not found. Skipping update.\n",
      "Error: Sentiment dictionary not found. Skipping update.\n",
      "Error: Sentiment dictionary not found. Skipping update.\n",
      "Error: Sentiment dictionary not found. Skipping update.\n",
      "Max retries reached for Unique_ID 'SEC-119312519134334'. Inserting 'No JSON found' for each category.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "update_csv() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax retries reached for Unique_ID \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munique_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Inserting \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo JSON found\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for each category.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m     sentiment_dict \u001b[38;5;241m=\u001b[39m {category: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo JSON found\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m CATEGORIES}\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSENTIMENT_RESULTS_FILE_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentiment_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCATEGORIES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: update_csv() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# Main Loop\n",
    "start_time = time.time()\n",
    "unique_id = ds.find_first_unique_id_with_empty_values(SENTIMENT_RESULTS_FILE_PATH, CATEGORIES)\n",
    "count = 0\n",
    "max_tries = 5\n",
    "\n",
    "while unique_id:\n",
    "    retries = 0\n",
    "    success = False\n",
    "\n",
    "    while retries < max_tries and not success:\n",
    "        try:\n",
    "            company, source, headline, text = ds.get_model_inputs(text_df, unique_id)\n",
    "            response = ls.get_ollama_response(text, TEMPLATE)\n",
    "            sentiment_dict = ds.extract_and_convert_to_dict(response)\n",
    "\n",
    "            if isinstance(sentiment_dict, dict):\n",
    "                ds.update_csv(SENTIMENT_RESULTS_FILE_PATH, unique_id, sentiment_dict)\n",
    "                success = True\n",
    "            else:\n",
    "                print(\"Error: Sentiment dictionary not found. Skipping update.\")\n",
    "                retries += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            retries += 1\n",
    "    if not success:\n",
    "        print(f\"Max retries reached for Unique_ID '{unique_id}'. Inserting 'No JSON found' for each category.\")\n",
    "        sentiment_dict = {category: \"No JSON found\" for category in CATEGORIES}\n",
    "        ds.update_csv(SENTIMENT_RESULTS_FILE_PATH, unique_id, sentiment_dict, CATEGORIES)\n",
    "\n",
    "\n",
    "    count += 1\n",
    "    if count % 10 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        minutes, seconds = divmod(elapsed_time, 60)\n",
    "        print(f\"Iteration: {count}, Elapsed Time: {int(minutes)} minutes and {seconds:.2f} seconds\")\n",
    "\n",
    "    unique_id = ds.find_first_unique_id_with_empty_values(SENTIMENT_RESULTS_FILE_PATH, CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Sentiment dictionary not found. Skipping update.\n",
      "Error: Sentiment dictionary not found. Skipping update.\n",
      "Error: Sentiment dictionary not found. Skipping update.\n",
      "Error: Sentiment dictionary not found. Skipping update.\n",
      "Error: Sentiment dictionary not found. Skipping update.\n",
      "Max retries reached for Unique_ID 'SEC-119312519134334'. Inserting 'No JSON found' for each category.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "update_csv() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax retries reached for Unique_ID \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munique_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Inserting \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo JSON found\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for each category.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m     sentiment_dict \u001b[38;5;241m=\u001b[39m {category: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo JSON found\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m CATEGORIES}\n\u001b[0;32m---> 30\u001b[0m     \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSENTIMENT_RESULTS_FILE_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentiment_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCATEGORIES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: update_csv() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# Main Loop\n",
    "start_time = time.time()\n",
    "unique_id = ds.find_first_unique_id_with_empty_values(SENTIMENT_RESULTS_FILE_PATH, CATEGORIES)\n",
    "count = 0\n",
    "MAX_TRIES = 5\n",
    "\n",
    "while unique_id:\n",
    "    retries = 0\n",
    "    success = False\n",
    "\n",
    "    while retries < MAX_TRIES and not success:\n",
    "        try:\n",
    "            company, source, headline, text = ds.get_model_inputs(text_df, unique_id)\n",
    "            response = ls.get_ollama_response(text, TEMPLATE)\n",
    "            sentiment_dict = ds.extract_and_convert_to_dict(response)\n",
    "\n",
    "            if isinstance(sentiment_dict, dict):\n",
    "                update_csv(SENTIMENT_RESULTS_FILE_PATH, unique_id, sentiment_dict, CATEGORIES)\n",
    "                success = True\n",
    "            else:\n",
    "                print(\"Error: Sentiment dictionary not found. Skipping update.\")\n",
    "                retries += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            retries += 1\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Max retries reached for Unique_ID '{unique_id}'. Inserting 'No JSON found' for each category.\")\n",
    "        sentiment_dict = {category: \"No JSON found\" for category in CATEGORIES}\n",
    "        ds.update_csv(SENTIMENT_RESULTS_FILE_PATH, unique_id, sentiment_dict, CATEGORIES)\n",
    "\n",
    "    count += 1\n",
    "    if count % 10 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        minutes, seconds = divmod(elapsed_time, 60)\n",
    "        print(f\"Iteration: {count}, Elapsed Time: {int(minutes)} minutes and {seconds:.2f} seconds\")\n",
    "\n",
    "    unique_id = ds.find_first_unique_id_with_empty_values(SENTIMENT_RESULTS_FILE_PATH, CATEGORIES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_csv(file_path, unique_id, sentiment_dict, categories):\n",
    "    \"\"\"\n",
    "    Updates the columns of a CSV file based on the unique ID and sentiment dictionary.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        unique_id (str): The unique ID of the row to be updated.\n",
    "        sentiment_dict (dict): A dictionary with categories as keys and their corresponding sentiments as values.\n",
    "        categories (list): List of all possible categories.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    row_index = df[df['Unique_ID'] == unique_id].index\n",
    "    for category in categories:\n",
    "        sentiment = sentiment_dict.get(category, \"No JSON found\")\n",
    "        if sentiment == \"N/A\":\n",
    "            sentiment = \"Neutral\"\n",
    "        df.loc[row_index, category] = sentiment\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Row with Unique_ID '{unique_id}' has been updated.\")\n",
    "\n",
    "# Main Loop\n",
    "start_time = time.time()\n",
    "unique_id = ds.find_first_unique_id_with_empty_values(SENTIMENT_RESULTS_FILE_PATH, CATEGORIES)\n",
    "count = 0\n",
    "\n",
    "while unique_id:\n",
    "    retries = 0\n",
    "    success = False\n",
    "\n",
    "    while retries < MAX_TRIES and not success:\n",
    "        try:\n",
    "            company, source, headline, text = ds.get_model_inputs(text_df, unique_id)\n",
    "            response = ls.get_ollama_response(text, TEMPLATE)\n",
    "            sentiment_dict = ds.extract_and_convert_to_dict(response)\n",
    "\n",
    "            if isinstance(sentiment_dict, dict):\n",
    "                update_csv(SENTIMENT_RESULTS_FILE_PATH, unique_id, sentiment_dict, CATEGORIES)\n",
    "                success = True\n",
    "            else:\n",
    "                print(\"Error: Sentiment dictionary not found. Skipping update.\")\n",
    "                retries += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            retries += 1\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Max retries reached for Unique_ID '{unique_id}'. Inserting 'No JSON found' for each category.\")\n",
    "        sentiment_dict = {category: \"No JSON found\" for category in CATEGORIES}\n",
    "        update_csv(SENTIMENT_RESULTS_FILE_PATH, unique_id, sentiment_dict, CATEGORIES)\n",
    "\n",
    "    count += 1\n",
    "    if count % 10 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        minutes, seconds = divmod(elapsed_time, 60)\n",
    "        print(f\"Iteration: {count}, Elapsed Time: {int(minutes)} minutes and {seconds:.2f} seconds\")\n",
    "\n",
    "    unique_id = ds.find_first_unique_id_with_empty_values(SENTIMENT_RESULTS_FILE_PATH, CATEGORIES)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
