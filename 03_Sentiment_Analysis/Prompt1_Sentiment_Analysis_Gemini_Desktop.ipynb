{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy9ROy-SV-U8"
   },
   "source": [
    "# Import Libraries and Clone Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pRIDt_1bVnna"
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hq_SPv2LyIPv"
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "CATEGORIES = [\n",
    "        \"Finance\",\n",
    "        \"Production\",\n",
    "        \"Reserves / Exploration / Acquisitions / Mergers / Divestments\",\n",
    "        \"Environment / Regulatory / Geopolitics\",\n",
    "        \"Alternative Energy / Lower Carbon\",\n",
    "        \"Oil Price / Natural Gas Price / Gasoline Price\"]\n",
    "SENTIMENT_RESULTS_FILE_PATH = 'Prompt1_Sentiment_Analysis_Results.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzRf877uW7h0"
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "QQwgVqdqXFqZ",
    "outputId": "8d908241-04e1-4f45-e407-699f50d1f94b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10126, 7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Article Headline</th>\n",
       "      <th>Article Text</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Investment Research</td>\n",
       "      <td>IR-1</td>\n",
       "      <td>MRO</td>\n",
       "      <td>2024-05-16</td>\n",
       "      <td>Marathon Oil Corporation</td>\n",
       "      <td>Stock Report | May 16, 2024 | NYSESymbol: MRO ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Investment Research</td>\n",
       "      <td>IR-2</td>\n",
       "      <td>EOG</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>EOG Resources, Inc.</td>\n",
       "      <td>Stock Report | May 14, 2024 | NYSESymbol: EOG ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Investment Research</td>\n",
       "      <td>IR-3</td>\n",
       "      <td>EOG</td>\n",
       "      <td>2024-05-11</td>\n",
       "      <td>EOG Resources, Inc.</td>\n",
       "      <td>Stock Report | May 11, 2024 | NYSESymbol: EOG ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Investment Research</td>\n",
       "      <td>IR-4</td>\n",
       "      <td>DVN</td>\n",
       "      <td>2024-05-11</td>\n",
       "      <td>Devon Energy Corporation</td>\n",
       "      <td>Stock Report | May 11, 2024 | NYSESymbol: DVN ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Investment Research</td>\n",
       "      <td>IR-5</td>\n",
       "      <td>COP</td>\n",
       "      <td>2024-05-07</td>\n",
       "      <td>ConocoPhillips</td>\n",
       "      <td>Stock Report | May 07, 2024 | NYSESymbol: COP ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Source Unique_ID Ticker        Date          Article Headline  \\\n",
       "0  Investment Research      IR-1    MRO  2024-05-16  Marathon Oil Corporation   \n",
       "1  Investment Research      IR-2    EOG  2024-05-14       EOG Resources, Inc.   \n",
       "2  Investment Research      IR-3    EOG  2024-05-11       EOG Resources, Inc.   \n",
       "3  Investment Research      IR-4    DVN  2024-05-11  Devon Energy Corporation   \n",
       "4  Investment Research      IR-5    COP  2024-05-07            ConocoPhillips   \n",
       "\n",
       "                                        Article Text  URL  \n",
       "0  Stock Report | May 16, 2024 | NYSESymbol: MRO ...  NaN  \n",
       "1  Stock Report | May 14, 2024 | NYSESymbol: EOG ...  NaN  \n",
       "2  Stock Report | May 11, 2024 | NYSESymbol: EOG ...  NaN  \n",
       "3  Stock Report | May 11, 2024 | NYSESymbol: DVN ...  NaN  \n",
       "4  Stock Report | May 07, 2024 | NYSESymbol: COP ...  NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Article Headline</th>\n",
       "      <th>Article Text</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10121</th>\n",
       "      <td>Earnings Call Q&amp;A</td>\n",
       "      <td>EQ-338</td>\n",
       "      <td>XOM</td>\n",
       "      <td>Feb-02-2021</td>\n",
       "      <td>Exxon Mobil Corporation, Q4 2020 Earnings Call...</td>\n",
       "      <td>Question and Answer\\r\\nOperator\\r\\n[Operator I...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10122</th>\n",
       "      <td>Earnings Call Q&amp;A</td>\n",
       "      <td>EQ-339</td>\n",
       "      <td>COP</td>\n",
       "      <td>Feb-02-2021</td>\n",
       "      <td>ConocoPhillips, Q4 2020 Earnings Call, Feb 02,...</td>\n",
       "      <td>Question and Answer\\r\\nOperator\\r\\n[Operator I...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10123</th>\n",
       "      <td>Earnings Call Q&amp;A</td>\n",
       "      <td>EQ-340</td>\n",
       "      <td>EOG</td>\n",
       "      <td>May-03-2019</td>\n",
       "      <td>EOG Resources, Inc., Q1 2019 Earnings Call, Ma...</td>\n",
       "      <td>Question and Answer\\r\\nOperator\\r\\n[Operator I...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10124</th>\n",
       "      <td>Earnings Call Q&amp;A</td>\n",
       "      <td>EQ-341</td>\n",
       "      <td>SHEL</td>\n",
       "      <td>May-02-2019</td>\n",
       "      <td>Royal Dutch Shell plc, Q1 2019 Earnings Call, ...</td>\n",
       "      <td>Question and Answer\\r\\nOperator\\r\\n[Operator I...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10125</th>\n",
       "      <td>Earnings Call Q&amp;A</td>\n",
       "      <td>EQ-342</td>\n",
       "      <td>COP</td>\n",
       "      <td>Apr-30-2019</td>\n",
       "      <td>ConocoPhillips, Q1 2019 Earnings Call, Apr 30,...</td>\n",
       "      <td>Question and Answer\\r\\nOperator\\r\\n[Operator I...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Source Unique_ID Ticker         Date  \\\n",
       "10121  Earnings Call Q&A    EQ-338    XOM  Feb-02-2021   \n",
       "10122  Earnings Call Q&A    EQ-339    COP  Feb-02-2021   \n",
       "10123  Earnings Call Q&A    EQ-340    EOG  May-03-2019   \n",
       "10124  Earnings Call Q&A    EQ-341   SHEL  May-02-2019   \n",
       "10125  Earnings Call Q&A    EQ-342    COP  Apr-30-2019   \n",
       "\n",
       "                                        Article Headline  \\\n",
       "10121  Exxon Mobil Corporation, Q4 2020 Earnings Call...   \n",
       "10122  ConocoPhillips, Q4 2020 Earnings Call, Feb 02,...   \n",
       "10123  EOG Resources, Inc., Q1 2019 Earnings Call, Ma...   \n",
       "10124  Royal Dutch Shell plc, Q1 2019 Earnings Call, ...   \n",
       "10125  ConocoPhillips, Q1 2019 Earnings Call, Apr 30,...   \n",
       "\n",
       "                                            Article Text  URL  \n",
       "10121  Question and Answer\\r\\nOperator\\r\\n[Operator I...  NaN  \n",
       "10122  Question and Answer\\r\\nOperator\\r\\n[Operator I...  NaN  \n",
       "10123  Question and Answer\\r\\nOperator\\r\\n[Operator I...  NaN  \n",
       "10124  Question and Answer\\r\\nOperator\\r\\n[Operator I...  NaN  \n",
       "10125  Question and Answer\\r\\nOperator\\r\\n[Operator I...  NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import all cleaned data files\n",
    "invest_df1 = pd.read_csv('../02_Cleaned_Data/Investment_Research_Part1.csv')\n",
    "invest_df2 = pd.read_csv('../02_Cleaned_Data/Investment_Research_Part2.csv')\n",
    "proquest_df = pd.read_csv('../02_Cleaned_Data/ProQuest_Articles.csv')\n",
    "earnings_presentations = pd.read_csv('../02_Cleaned_Data/Earnings_Presentations.csv')\n",
    "earnings_qa = pd.read_csv('../02_Cleaned_Data/Earnings_QA.csv')\n",
    "sec_df = pd.read_csv('../02_Cleaned_Data/SEC_Filings.csv')\n",
    "\n",
    "# Merge into single df\n",
    "text_df = pd.concat([invest_df1, invest_df2, proquest_df, sec_df, earnings_presentations, earnings_qa], ignore_index=True)\n",
    "display(text_df.shape)\n",
    "display(text_df.head())\n",
    "display(text_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([5379], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "# Drop rows google gemini will not process\n",
    "rows_to_drop = ['PQ-2840736837']\n",
    "index_to_drops = text_df[text_df['Unique_ID'].isin(rows_to_drop)].index\n",
    "text_df.drop(index_to_drops, inplace=True)\n",
    "print(index_to_drops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqZ2XDGuWNLM"
   },
   "source": [
    "# Find or Create Results CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DgjxgwbZoY0b",
    "outputId": "bda7c259-5480-483f-d674-016402ddf9a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file exists in the current directory.\n"
     ]
    }
   ],
   "source": [
    "# Determine if the Sentiment Analysis Results file already exists\n",
    "file_exists = os.path.isfile(SENTIMENT_RESULTS_FILE_PATH)\n",
    "\n",
    "# Print the result\n",
    "if file_exists:\n",
    "    print(f\"The file exists in the current directory.\")\n",
    "else:\n",
    "    print(f\"The file does not exist in the current directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3_gsRCosWRfE"
   },
   "outputs": [],
   "source": [
    "# Create an empty file if the file does not exist\n",
    "if not file_exists:\n",
    "    # Copy text df and drop the text and headline column due to size\n",
    "    empty_sentiment_df = text_df.copy()\n",
    "    empty_sentiment_df = empty_sentiment_df.drop(['Article Text', 'Article Headline'], axis=1)\n",
    "\n",
    "    for category in CATEGORIES:\n",
    "        empty_sentiment_df[category] = \"\"\n",
    "        empty_sentiment_df[category] = empty_sentiment_df[category].astype('object')\n",
    "\n",
    "    # Display results\n",
    "    display(empty_sentiment_df.head())\n",
    "\n",
    "    # Save as CSV\n",
    "    empty_sentiment_df.to_csv(SENTIMENT_RESULTS_FILE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WQ56LlUp-rH"
   },
   "source": [
    "### NOTE: If you want to re-run the sentiment analysis, delete or archive the csv to create a blank one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMeL1ZOcXSz9"
   },
   "source": [
    "# Sentiment Analysis\n",
    "NOTE: Google gemini **currently** has a daily query limit of 1,500 requests per day.  As we have over 10,000 documents, the code will be designed to run over multiple days and pick up where we last left off.  After the code is run, the user will still need to manually download the csv and upload to github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmpkXUk-DZfr"
   },
   "source": [
    "**Gemini Free Rate Limits**\n",
    "*   15 RPM (requests per minute)\n",
    "*   1 million TPM (tokens per minute)\n",
    "*   1,500 RPD (requests per day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nVdZDCVf2iHr"
   },
   "outputs": [],
   "source": [
    "# Set up Gemini. (API key needs to be in your environment variables)\n",
    "key = 'GOOGLE_API_KEY'\n",
    "GOOGLE_API_KEY = os.getenv(key)\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# So it doesn't block the output\n",
    "safety_settings = [\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",},]\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest', safety_settings=safety_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22Zt879CulED",
    "outputId": "59e59419-e4a3-4ea9-a151-5d84a8799f0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PQ-2625995082\n"
     ]
    }
   ],
   "source": [
    "# Function to find the first empty row of the csv\n",
    "def find_first_unique_id_with_empty_values(file_path, categories):\n",
    "    \"\"\"\n",
    "    Finds the first unique ID where any of the specified columns have empty values in a CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        categories (list of str): List of column names to check for empty values.\n",
    "\n",
    "    Returns:\n",
    "        str: The first Unique_ID where any of the specified columns have empty values.\n",
    "        None: If no such row is found.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Iterate over each row to find the first unique ID with any empty values in the specified columns\n",
    "    for index, row in df.iterrows():\n",
    "        if row[categories].isnull().any() or (row[categories] == '').any():\n",
    "            return row['Unique_ID']\n",
    "\n",
    "    return None  # Return None if no such row is found\n",
    "\n",
    "# Test function\n",
    "unique_id = find_first_unique_id_with_empty_values(SENTIMENT_RESULTS_FILE_PATH, CATEGORIES)\n",
    "print(unique_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0Gg79X82NYq",
    "outputId": "dd6b7aa5-40c0-4155-bc76-d647f9b9d8e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: XOM\n",
      "\n",
      "Source: ProQuest\n",
      "\n",
      "Headline: Projects Capture Carbon Emissions Get New Boost\n",
      "\n",
      "Text:\n",
      "Petra Nova, once billed as the largest U.S. project to capture carbon-dioxide emissions from a coal-fired power plant, opened to considerable publicity in Texas in late 2016.\r\n",
      "Less than four years later, owner NRG Energy Inc. shut down the carbon-capture system, which cost $1 billion—not because the technology wasn't working but because the expected end use for the carbon was no longer economically viable. The coal plant continues to generate electricity and emit carbon.\r\n",
      "Carbon-capture projects are attracting renewed attention from investors and governments world-wide as concerns mount about the greenhouse-gas emissions linked to climate change. But the initiatives have a dismal record.\r\n",
      "More than 80% of proposed commercial carbon-capture efforts around the world have failed, primarily because the technology didn't work as expected or the projects proved too expensive to operate, according to a 2020 study by researchers at Canada's Carleton University, the University of California, San Diego and other institutions.\r\n",
      "The U.S. has spent $1.1 billion on carbon-capture demonstration projects since 2009, with uneven results, according to a December report from the Government Accountability Office. None of the eight coal projects selected for $684 million of the funding during that time is operating, the researchers found. Projects to capture carbon from heavy industries met with some success.\r\n",
      "While some early projects have demonstrated that it is technologically possible to collect carbon from power plants and industrial sites—or even directly out of the air—they have generally been very expensive. Many face a fundamental problem: there is no economic use for the carbon they capture.\r\n",
      "Currently, the only large-scale use for captured carbon is for pushing more oil and gas out of declining reservoirs, which in turn leads to additional emissions when the fossil fuels are burned for energy. In the U.S., there is no federal requirement that companies capture carbon emissions, or carbon taxes or other fees aimed at discouraging them from releasing the greenhouse gases into the atmosphere.\r\n",
      "As a result, most carbon-capture initiatives don't save companies money or generate profits, and they represent an added business expense. Still, some companies are pursuing the projects to reduce their carbon footprint under pressure from investors and activists concerned about climate change .\r\n",
      "A fresh round of U.S. carbon-capture projects is in the works, bolstered by around $12.1 billion in funding in the $1 trillion infrastructure bill signed into law last year by President Biden. Oil, power, chemicals and biofuels companies are kicking off a wave of new proposed carbon-capture investments, including carbon-transport pipelines in Iowa, a coal-power plant in North Dakota and a hydrogen plant in Louisiana.\r\n",
      "Large fossil-fuel companies including Exxon Mobil Corp. and Occidental Petroleum Corp. are touting carbon capture as a part of their future plans to reduce emissions —and lobbying Congress to increase a tax credit to make the projects more economically sustainable.\r\n",
      "Many companies and climate activists say governments need to nurture innovative technologies to capture emissions that would otherwise be hard to cut. Accelerating such projects, they argue, is the only realistic way to reach the targets of the international Paris agreement , which seeks to keep rising temperatures to well below 2 degrees Celsius from preindustrial levels to avoid the worst impacts of climate change.\r\n",
      "\"To meet the goals of the Paris climate accords, there's no way we can do it without direct-air capture,\" Occidental Chief Executive Vicki Hollub said in an interview. The company, which uses carbon to extract oil, plans to build facilities to capture it straight from the air, but considers the potential tax-credit expansion vital to its efforts.\r\n",
      "Exxon is proposing a project with other companies in Houston to capture and bury the carbon from an array of industries. But it would be difficult to launch at its proposed size without policy changes such as a larger tax credit, said Erik Oswald, a vice president at Exxon.\r\n",
      "Congress is considering boosting the credit for collecting carbon emissions from smokestacks by 70% to $85 for a metric ton if the carbon is stashed in saline geologic formations, or $60 if it is sent down oil wells. Direct air projects would get $180 for a metric ton if the carbon is stored, or $130 for oil.\r\n",
      "Less generous tax credits have been on the books since 2008 but have failed to create a real carbon-capture industry. \"There's been little material impact on the deployment of carbon capture and storage,\" said Scott Anderson, senior director of energy at the Environmental Defense Fund, a U.S.-based advocacy group.\r\n",
      "The infrastructure bill included funding for pipelines and storage to help build a missing puzzle piece: a spider's web of infrastructure that could gather and ship carbon from multiple sites.\r\n",
      "\"That's a massive step forward for carbon capture and carbon storage,\" said Cindy Crane, chief executive of Enchant Energy Corp., which plans to retrofit a coal-fired power plant in New Mexico with carbon-capture equipment for around $1.3 billion. The project would also require up to roughly $390 million in plant improvements, a pipeline and storage field.\r\n",
      "Globally, industries will have to raise carbon-capture capacity by a factor of 50 to 100 times over what is in the development pipeline to achieve what the International Energy Agency estimates is needed to reach \"net-zero\" carbon emissions by 2070, said John Bradford, professor of geophysics and vice president for global initiatives at the Colorado School of Mines.\r\n",
      "Building those projects—and keeping them running—can be costly. Petra Nova was a joint venture of NRG and JX Nippon Oil & Gas Exploration Corp. that captured some emissions from a coal plant near Houston and piped them about 80 miles to an oil field, where they were used to push more crude out of the ground. The government awarded the project around $195 million in a proof-of-concept grant.\r\n",
      "Petra Nova closed in 2020 after the pandemic reduced demand for fuel and led to a collapse in oil prices, which made the oil that the captured carbon was helping produce less economically viable. It remains in mothball status, though NRG said it proved the technology could work on a coal-fired plant.\r\n",
      "\"We continue to explore options to improve the economics,\" said NRG spokesman Chris Rimel.\r\n",
      "In Mississippi, a carbon-capture initiative by utility Southern Co. has turned into a white elephant. The project known as Kemper aimed to use locally mined lignite coal to fuel a power plant, and capture the resulting carbon emissions, which were then to be sent to oil fields to prime crude production. The Energy Department invested $387 million.\r\n",
      " Forecast to cost $3 billion in 2010, Kemper's costs spiraled above $7 billion . Once constructed, the coal-gasification technology never quite worked as intended, and Southern abandoned its initial plans , burning natural gas in the power plant instead.\r\n",
      "The company imploded coal and carbon-capture equipment that couldn't be dismantled for resale last October. Coal conveyors from Kemper are now available for sale online.\r\n",
      "\"It was the end of a long, bad experiment,\" said Mississippi Public Service Commissioner Brandon Presley, a Kemper critic. Mr. Presley said he favors innovation but believes government and business should bear the risk instead of utility ratepayers.\r\n",
      "Mr. Presley and other regulators didn't allow Southern to pass Kemper's full cost on to customers. The company, which had to assume some $6 billion on the project's cost, is paying for demolition of the carbon-capture part, estimated at $10 million to $20 million annually through 2025, said a spokesman for Mississippi Power, the utility arm behind the project.\r\n",
      "The federal government is now funding a $24 million feasibility study that includes the same plant—this time for capturing and storing carbon emissions from natural gas.\r\n",
      "Write to Jennifer Hiller at jennifer.hiller@wsj.com and Collin Eaton at collin.eaton@wsj.com \r\n",
      " Projects to Capture Carbon Emissions Get New Boost Despite Dismal Record\n"
     ]
    }
   ],
   "source": [
    "# Helper function to get Gemini query function inputs\n",
    "def get_gemini_inputs(text_df, unique_id):\n",
    "    \"\"\"\n",
    "    Retrieves information from the DataFrame based on the unique ID and outputs company, source, headline, and text.\n",
    "\n",
    "    Args:\n",
    "        text_df (pd.DataFrame): The DataFrame containing the text data.\n",
    "        unique_id (str): The unique ID to search for.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing company, source, headline, and text.\n",
    "    \"\"\"\n",
    "    # Find the row with the specified unique ID\n",
    "    row = text_df[text_df['Unique_ID'] == unique_id]\n",
    "\n",
    "    # Extract the required information\n",
    "    company = row['Ticker'].values[0]\n",
    "    source = row['Source'].values[0]\n",
    "    headline = row['Article Headline'].values[0]\n",
    "    text = row['Article Text'].values[0]\n",
    "\n",
    "    return company, source, headline, text\n",
    "\n",
    "# Test function\n",
    "company, source, headline, text = get_gemini_inputs(text_df, unique_id)\n",
    "print(f\"Company: {company}\\n\")\n",
    "print(f\"Source: {source}\\n\")\n",
    "print(f\"Headline: {headline}\\n\")\n",
    "print(f\"Text:\\n{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "id": "eRjhvk_9Xapi",
    "outputId": "2fe3961b-a3bb-4e4a-9255-f5b040302e4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Sentiment Analysis of Carbon Capture News Article\n",
      "\n",
      "This article focuses on the renewed interest in carbon capture technologies, outlining both the challenges and opportunities.  \n",
      "\n",
      "**Finance:**\n",
      "\n",
      "- **Sentiment:** Negative\n",
      "- **Reasoning:**  The article highlights the high costs associated with carbon capture projects, including the failure of previous initiatives like Petra Nova and Kemper.  The article states that many projects \"don't save companies money or generate profits,\" and the cost overruns of Kemper are specifically mentioned. \n",
      "- **Reference:** \"Many face a fundamental problem: there is no economic use for the carbon they capture.\", \"Still, some companies are pursuing the projects to reduce their carbon footprint under pressure from investors and activists concerned about climate change.\", \"Projects to Capture Carbon Emissions Get New Boost Despite Dismal Record\", \"Forecast to cost $3 billion in 2010, Kemper's costs spiraled above $7 billion\"\n",
      "\n",
      "**Production:**\n",
      "\n",
      "- **Sentiment:** Neutral\n",
      "- **Reasoning:**  While the article mentions projects related to oil and gas production, it doesn't directly comment on the overall impact of carbon capture on production levels.\n",
      "- **Reference:**  \"Currently, the only large-scale use for captured carbon is for pushing more oil and gas out of declining reservoirs, which in turn leads to additional emissions when the fossil fuels are burned for energy.\"\n",
      "\n",
      "**Reserves / Exploration / Acquisitions / Mergers / Divestments:**\n",
      "\n",
      "- **Sentiment:** Neutral\n",
      "- **Reasoning:**  The article doesn't mention any specific information related to reserves, exploration, acquisitions, mergers, or divestments.\n",
      "- **Reference:** N/A\n",
      "\n",
      "**Environment / Regulatory / Geopolitics:**\n",
      "\n",
      "- **Sentiment:** Positive\n",
      "- **Reasoning:**  The article emphasizes the potential of carbon capture to reduce greenhouse gas emissions and meet climate change goals, particularly in the context of the Paris Agreement.  The article also mentions government funding and policy changes as driving forces for the technology.\n",
      "- **Reference:**  \"Many companies and climate activists say governments need to nurture innovative technologies to capture emissions that would otherwise be hard to cut.\", \"Accelerating such projects, they argue, is the only realistic way to reach the targets of the international Paris agreement , which seeks to keep rising temperatures to well below 2 degrees Celsius from preindustrial levels to avoid the worst impacts of climate change.\", \"The infrastructure bill included funding for pipelines and storage to help build a missing puzzle piece: a spider's web of infrastructure that could gather and ship carbon from multiple sites.\"\n",
      "\n",
      "**Alternative Energy / Lower Carbon:**\n",
      "\n",
      "- **Sentiment:** Positive\n",
      "- **Reasoning:** The article strongly promotes carbon capture as a crucial technology for achieving lower carbon emissions and transitioning to a cleaner energy future. \n",
      "- **Reference:** \"To meet the goals of the Paris climate accords, there's no way we can do it without direct-air capture,\" \"Many companies and climate activists say governments need to nurture innovative technologies to capture emissions that would otherwise be hard to cut.\", \"Accelerating such projects, they argue, is the only realistic way to reach the targets of the international Paris agreement , which seeks to keep rising temperatures to well below 2 degrees Celsius from preindustrial levels to avoid the worst impacts of climate change.\"\n",
      "\n",
      "**Oil Price / Natural Gas Price / Gasoline Price:**\n",
      "\n",
      "- **Sentiment:** Neutral\n",
      "- **Reasoning:** The article mentions the impact of oil prices on the viability of projects like Petra Nova but doesn't offer any specific insights into future oil, natural gas, or gasoline price trends.\n",
      "- **Reference:** \"Petra Nova closed in 2020 after the pandemic reduced demand for fuel and led to a collapse in oil prices, which made the oil that the captured carbon was helping produce less economically viable.\"\n",
      "\n",
      "**Summary:**\n",
      "\n",
      "- Finance - Negative\n",
      "- Production - Neutral\n",
      "- Reserves / Exploration / Acquisitions / Mergers / Divestments - Neutral\n",
      "- Environment / Regulatory / Geopolitics - Positive\n",
      "- Alternative Energy / Lower Carbon - Positive\n",
      "- Oil Price / Natural Gas Price / Gasoline Price - Neutral \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to query Gemini\n",
    "def query_gemini(company, source, headline, text, model):\n",
    "    \"\"\"\n",
    "    Query Gemini to perform sentiment analysis on text from various sources about a company.\n",
    "\n",
    "    Parameters:\n",
    "    company (str): The name of the company the text is about.\n",
    "    source (str): The source of the text. Valid values are \"Investment Research\", \"ProQuest\", \"SEC Filings\", \"Earnings Call Presentations\", \"Earnings Call Q&A\".\n",
    "    headline (str): The headline or title of the text.\n",
    "    text (str): The body of the text to be analyzed.\n",
    "    model: The model object used to generate content and analyze the text.\n",
    "\n",
    "    Returns:\n",
    "    str: The sentiment analysis results for predefined categories in the specified format.\n",
    "\n",
    "    The function constructs a prompt based on the source of the text and performs sentiment analysis on the given text using the provided model.\n",
    "    It analyzes the content across multiple predefined categories, determining the sentiment (Positive, Neutral, Negative) for each category.\n",
    "    If a category is not mentioned or relevant based on the text content, it is marked as 'Neutral'.\n",
    "    The final output is summarized in a specified format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Source Variables\n",
    "    if source == \"Investment Research\":\n",
    "        text_source = \"an analyst report\"\n",
    "        text_source2 = \"the analyst report\"\n",
    "    elif source == \"ProQuest\":\n",
    "        text_source = \"a news article\"\n",
    "        text_source2 = \"the news article\"\n",
    "    elif source == \"SEC Filings\":\n",
    "        text_source = \"an SEC filing\"\n",
    "        text_source2 = \"the SEC filing\"\n",
    "    elif source == \"Earnings Call Presentations\":\n",
    "        text_source = \"an earnings call presentation\"\n",
    "        text_source2 = \"the earnings call presentation\"\n",
    "    elif source == \"Earnings Call Q&A\":\n",
    "        text_source = \"an earnings call Q&A session\"\n",
    "        text_source2 = \"the earnings call Q&A session\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt = f\"\"\"\n",
    "Given the text from {text_source} about {company}, analyze the content and perform sentiment analysis across multiple predefined categories.\n",
    "\n",
    "Sentiment options:\n",
    "  - Positive\n",
    "  - Neutral\n",
    "  - Negative\n",
    "\n",
    "Categories:\n",
    "  - Finance\n",
    "  - Production\n",
    "  - Reserves / Exploration / Acquisitions / Mergers / Divestments\n",
    "  - Environment / Regulatory / Geopolitics\n",
    "  - Alternative Energy / Lower Carbon\n",
    "  - Oil Price / Natural Gas Price / Gasoline Price\n",
    "\n",
    "Each category should be evaluated and given a sentiment output derived from the text.\n",
    "If a category is not mentioned or relevant based on the text content, mark it as 'Neutral'.\n",
    "\n",
    "Before giving your answer, explain your reasoning and reference the article.\n",
    "After going through all the categories, provide a summary in the following format:\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "\n",
    "Example Output:\n",
    "- Finance - Positive\n",
    "- Production - Neutral\n",
    "- Reserves / Exploration / Acquisitions / Mergers / Divestments - Negative\n",
    "- Environment / Regulatory / Geopolitics - Neutral\n",
    "- Alternative Energy / Lower Carbon - Positive\n",
    "- Oil Price / Natural Gas Price / Gasoline Price - Neutral\n",
    "\n",
    "Make sure to use plain text, do not bold or bullet the output summary.\n",
    "\n",
    "The text from {text_source2} is below:\n",
    "{headline}\n",
    "{text}\n",
    "\n",
    "Remember to summarize your final answers in the following format exactly:\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "\n",
    "Make sure to use plain text and stick to the given categories and sentiment options.\n",
    "DO NOT bold or bullet the output summary.\n",
    "    \"\"\"\n",
    "\n",
    "    # print(prompt)\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Test function\n",
    "response = query_gemini(company, source, headline, text, model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DbTgA_znnYIX",
    "outputId": "b1ef5cb2-60e2-47ac-abfc-5b66e194eee1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Finance': 'Negative', 'Production': 'Neutral', 'Reserves / Exploration / Acquisitions / Mergers / Divestments': 'Neutral', 'Environment / Regulatory / Geopolitics': 'Positive', 'Alternative Energy / Lower Carbon': 'Positive', 'Oil Price / Natural Gas Price / Gasoline Price': 'Neutral'}\n",
      "Did not find all categories\n"
     ]
    }
   ],
   "source": [
    "# Function to parse text\n",
    "def parse_sentiment(text, categories):\n",
    "    \"\"\"\n",
    "    Parses a given text for specified categories and their sentiments.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing categories and their sentiments.\n",
    "        categories (list of str): List of category names to look for in the text.\n",
    "\n",
    "    Returns:\n",
    "        dict or str: A dictionary with categories as keys and their corresponding sentiments as values,\n",
    "                     or \"Did not find all categories\" if any sentiment is not Positive, Neutral, or Negative.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    valid_sentiments = {\"Positive\", \"Neutral\", \"Negative\"}\n",
    "\n",
    "    for category in categories:\n",
    "        # Create a regex pattern to match the format \"- Category - Sentiment\"\n",
    "        # The category name is escaped to handle any special characters\n",
    "        pattern = rf\"- {re.escape(category)} - (\\w+)\"\n",
    "\n",
    "        # Search for the pattern in the text\n",
    "        match = re.search(pattern, text)\n",
    "\n",
    "        # If a match is found, extract the sentiment and add it to the results dictionary\n",
    "        if match:\n",
    "            sentiment = match.group(1)\n",
    "            if sentiment not in valid_sentiments:\n",
    "                return \"Did not find all categories\"\n",
    "            results[category] = sentiment\n",
    "        else:\n",
    "            return \"Did not find all categories\"\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test function\n",
    "fail_text = \"\"\"\n",
    "- Finance - Positive\n",
    "- Production - Neutral\n",
    "- Reserves / Exploration / Acquisitions / Mergers / Divestments - Bad\n",
    "- Environment / Regulatory / Geopolitics - Neutral\n",
    "- Alternative Energy / Lower Carbon - Positive\n",
    "- Oil Price / Natural Gas Price / Gasoline Price - Neutral\n",
    "\"\"\"\n",
    "\n",
    "sentiment_dict = parse_sentiment(response, CATEGORIES)\n",
    "fail_sentiment = parse_sentiment(\"fail_text\", CATEGORIES)\n",
    "print(sentiment_dict)\n",
    "print(fail_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rfIujTIrXj18",
    "outputId": "c6713621-57ff-4b11-9e19-ccd6675e55a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row with Unique_ID 'PQ-2625995082' has been updated.\n"
     ]
    }
   ],
   "source": [
    "# Function to update the csv\n",
    "def update_csv(file_path, unique_id, sentiment_dict):\n",
    "    \"\"\"\n",
    "    Updates the columns of a CSV file based on the unique ID and sentiment dictionary.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        unique_id (str): The unique ID of the row to be updated.\n",
    "        sentiment_dict (dict): A dictionary with categories as keys and their corresponding sentiments as values.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Find the index of the row with the specified unique ID\n",
    "    row_index = df[df['Unique_ID'] == unique_id].index\n",
    "\n",
    "    # Update the columns based on the sentiment dictionary\n",
    "    for category, sentiment in sentiment_dict.items():\n",
    "        df.loc[row_index, category] = sentiment\n",
    "\n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Row with Unique_ID '{unique_id}' has been updated.\")\n",
    "\n",
    "# Test function\n",
    "update_csv(SENTIMENT_RESULTS_FILE_PATH, unique_id, sentiment_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ANJgrUqEr1tE",
    "outputId": "3c81cf36-fad8-4a43-d935-8395884731e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row with Unique_ID 'PQ-2625959270' has been updated.\n",
      "Row with Unique_ID 'PQ-2625859566' has been updated.\n",
      "Row with Unique_ID 'PQ-2625372929' has been updated.\n",
      "Row with Unique_ID 'PQ-2635752236' has been updated.\n",
      "Row with Unique_ID 'PQ-2624957874' has been updated.\n",
      "Row with Unique_ID 'PQ-2635344606' has been updated.\n",
      "Row with Unique_ID 'PQ-2635343189' has been updated.\n",
      "Row with Unique_ID 'PQ-2624855836' has been updated.\n",
      "Row with Unique_ID 'PQ-2624561453' has been updated.\n",
      "Row with Unique_ID 'PQ-2624532259' has been updated.\n",
      "Iteration: 10, Elapsed Time: 2 minutes and 15.56 seconds\n",
      "Row with Unique_ID 'PQ-2624463729' has been updated.\n",
      "Row with Unique_ID 'PQ-2624242198' has been updated.\n",
      "Row with Unique_ID 'PQ-2624157749' has been updated.\n",
      "Row with Unique_ID 'PQ-2624092487' has been updated.\n",
      "Row with Unique_ID 'PQ-2624083488' has been updated.\n",
      "Row with Unique_ID 'PQ-2624054309' has been updated.\n",
      "Row with Unique_ID 'PQ-2623998915' has been updated.\n",
      "Row with Unique_ID 'PQ-2623895215' has been updated.\n",
      "Row with Unique_ID 'PQ-2623837782' has been updated.\n",
      "Row with Unique_ID 'PQ-2623809189' has been updated.\n",
      "Iteration: 20, Elapsed Time: 4 minutes and 16.12 seconds\n",
      "Row with Unique_ID 'PQ-2623775996' has been updated.\n",
      "Row with Unique_ID 'PQ-2623732613' has been updated.\n",
      "Row with Unique_ID 'PQ-2623575854' has been updated.\n",
      "Row with Unique_ID 'PQ-2621670082' has been updated.\n",
      "Row with Unique_ID 'PQ-2621097970' has been updated.\n",
      "Row with Unique_ID 'PQ-2629526890' has been updated.\n",
      "Row with Unique_ID 'PQ-2621037948' has been updated.\n",
      "Row with Unique_ID 'PQ-2620938694' has been updated.\n",
      "Row with Unique_ID 'PQ-2620819812' has been updated.\n",
      "Row with Unique_ID 'PQ-2620752943' has been updated.\n",
      "Iteration: 30, Elapsed Time: 6 minutes and 2.38 seconds\n",
      "Row with Unique_ID 'PQ-2620736647' has been updated.\n",
      "Row with Unique_ID 'PQ-2619617965' has been updated.\n",
      "Row with Unique_ID 'PQ-2618486662' has been updated.\n",
      "Row with Unique_ID 'PQ-2618485891' has been updated.\n",
      "Row with Unique_ID 'PQ-2618157687' has been updated.\n",
      "Row with Unique_ID 'PQ-2618130212' has been updated.\n",
      "Row with Unique_ID 'PQ-2618023648' has been updated.\n",
      "Row with Unique_ID 'PQ-2617927908' has been updated.\n",
      "Row with Unique_ID 'PQ-2617083046' has been updated.\n",
      "Row with Unique_ID 'PQ-2616985634' has been updated.\n",
      "Iteration: 40, Elapsed Time: 7 minutes and 49.94 seconds\n",
      "Row with Unique_ID 'PQ-2616921304' has been updated.\n",
      "Row with Unique_ID 'PQ-2616639258' has been updated.\n",
      "Row with Unique_ID 'PQ-2615910751' has been updated.\n",
      "Row with Unique_ID 'PQ-2615479089' has been updated.\n",
      "Row with Unique_ID 'PQ-2614816757' has been updated.\n",
      "Row with Unique_ID 'PQ-2614291124' has been updated.\n",
      "Row with Unique_ID 'PQ-2614102073' has been updated.\n",
      "Row with Unique_ID 'PQ-2613762451' has been updated.\n",
      "Row with Unique_ID 'PQ-2612734337' has been updated.\n",
      "Row with Unique_ID 'PQ-2611570191' has been updated.\n",
      "Iteration: 50, Elapsed Time: 9 minutes and 36.11 seconds\n",
      "Row with Unique_ID 'PQ-2609540248' has been updated.\n",
      "Row with Unique_ID 'PQ-2609720404' has been updated.\n",
      "Row with Unique_ID 'PQ-2609284140' has been updated.\n",
      "Row with Unique_ID 'PQ-2609256585' has been updated.\n",
      "Row with Unique_ID 'PQ-2609209148' has been updated.\n",
      "Row with Unique_ID 'PQ-2608875357' has been updated.\n",
      "Row with Unique_ID 'PQ-2608575157' has been updated.\n",
      "Row with Unique_ID 'PQ-2608476575' has been updated.\n",
      "Row with Unique_ID 'PQ-2607974000' has been updated.\n",
      "Row with Unique_ID 'PQ-2607535783' has been updated.\n",
      "Iteration: 60, Elapsed Time: 11 minutes and 31.91 seconds\n",
      "Row with Unique_ID 'PQ-2607496731' has been updated.\n",
      "Row with Unique_ID 'PQ-2607099772' has been updated.\n",
      "Row with Unique_ID 'PQ-2606720958' has been updated.\n",
      "Row with Unique_ID 'PQ-2605729703' has been updated.\n",
      "Row with Unique_ID 'PQ-2605734782' has been updated.\n",
      "Row with Unique_ID 'PQ-2615534736' has been updated.\n",
      "Row with Unique_ID 'PQ-2605392959' has been updated.\n",
      "Row with Unique_ID 'PQ-2605141555' has been updated.\n",
      "Row with Unique_ID 'PQ-2605069697' has been updated.\n",
      "Row with Unique_ID 'PQ-2605051254' has been updated.\n",
      "Iteration: 70, Elapsed Time: 13 minutes and 30.35 seconds\n",
      "Row with Unique_ID 'PQ-2605034098' has been updated.\n",
      "Row with Unique_ID 'PQ-2604805865' has been updated.\n",
      "Row with Unique_ID 'PQ-2604805863' has been updated.\n",
      "Row with Unique_ID 'PQ-2599235621' has been updated.\n",
      "Row with Unique_ID 'PQ-2597902027' has been updated.\n",
      "Row with Unique_ID 'PQ-2596221483' has been updated.\n",
      "Row with Unique_ID 'PQ-2595338839' has been updated.\n",
      "Row with Unique_ID 'PQ-2591323713' has been updated.\n",
      "Row with Unique_ID 'PQ-2591158652' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2589875135' has been updated.\n",
      "Iteration: 80, Elapsed Time: 16 minutes and 5.85 seconds\n",
      "Row with Unique_ID 'PQ-2589781328' has been updated.\n",
      "Row with Unique_ID 'PQ-2589048807' has been updated.\n",
      "Row with Unique_ID 'PQ-2774590730' has been updated.\n",
      "Row with Unique_ID 'PQ-2587948614' has been updated.\n",
      "Row with Unique_ID 'PQ-2587948613' has been updated.\n",
      "Row with Unique_ID 'PQ-2587697315' has been updated.\n",
      "Row with Unique_ID 'PQ-2587629444' has been updated.\n",
      "Row with Unique_ID 'PQ-2586963731' has been updated.\n",
      "Row with Unique_ID 'PQ-2586925859' has been updated.\n",
      "Row with Unique_ID 'PQ-2586572691' has been updated.\n",
      "Iteration: 90, Elapsed Time: 18 minutes and 0.29 seconds\n",
      "Row with Unique_ID 'PQ-2586461222' has been updated.\n",
      "Row with Unique_ID 'PQ-2586273659' has been updated.\n",
      "Row with Unique_ID 'PQ-2585981909' has been updated.\n",
      "Row with Unique_ID 'PQ-2585814870' has been updated.\n",
      "Row with Unique_ID 'PQ-2585787786' has been updated.\n",
      "Row with Unique_ID 'PQ-2625006485' has been updated.\n",
      "Row with Unique_ID 'PQ-2584868574' has been updated.\n",
      "Row with Unique_ID 'PQ-2584161468' has been updated.\n",
      "Row with Unique_ID 'PQ-2583942109' has been updated.\n",
      "Row with Unique_ID 'PQ-2583765526' has been updated.\n",
      "Iteration: 100, Elapsed Time: 19 minutes and 49.32 seconds\n",
      "Row with Unique_ID 'PQ-2583453438' has been updated.\n",
      "Row with Unique_ID 'PQ-2582791784' has been updated.\n",
      "Error encountered: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2580904027' has been updated.\n",
      "Row with Unique_ID 'PQ-2580852279' has been updated.\n",
      "Row with Unique_ID 'PQ-2578960594' has been updated.\n",
      "Row with Unique_ID 'PQ-2578679903' has been updated.\n",
      "Row with Unique_ID 'PQ-2578587675' has been updated.\n",
      "Row with Unique_ID 'PQ-2578182512' has been updated.\n",
      "Row with Unique_ID 'PQ-2578102237' has been updated.\n",
      "Row with Unique_ID 'PQ-2577116789' has been updated.\n",
      "Iteration: 110, Elapsed Time: 22 minutes and 26.31 seconds\n",
      "Row with Unique_ID 'PQ-2576914261' has been updated.\n",
      "Row with Unique_ID 'PQ-2576847305' has been updated.\n",
      "Row with Unique_ID 'PQ-2576661672' has been updated.\n",
      "Row with Unique_ID 'PQ-2576621655' has been updated.\n",
      "Row with Unique_ID 'PQ-2576542197' has been updated.\n",
      "Error encountered: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting. Retry 1/5\n",
      "Error encountered: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting. Retry 2/5\n",
      "Row with Unique_ID 'PQ-2576542102' has been updated.\n",
      "Row with Unique_ID 'PQ-2574785664' has been updated.\n",
      "Row with Unique_ID 'PQ-2574802920' has been updated.\n",
      "Row with Unique_ID 'PQ-2571716329' has been updated.\n",
      "Row with Unique_ID 'PQ-2569703110' has been updated.\n",
      "Iteration: 120, Elapsed Time: 25 minutes and 27.66 seconds\n",
      "Row with Unique_ID 'PQ-2568685392' has been updated.\n",
      "Row with Unique_ID 'PQ-2568692009' has been updated.\n",
      "Row with Unique_ID 'PQ-2567798168' has been updated.\n",
      "Row with Unique_ID 'PQ-2563474560' has been updated.\n",
      "Row with Unique_ID 'PQ-2560871917' has been updated.\n",
      "Row with Unique_ID 'PQ-2558340803' has been updated.\n",
      "Row with Unique_ID 'PQ-2558052918' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 2/5\n",
      "Row with Unique_ID 'PQ-2556734051' has been updated.\n",
      "Row with Unique_ID 'PQ-2556572801' has been updated.\n",
      "Row with Unique_ID 'PQ-2556421021' has been updated.\n",
      "Iteration: 130, Elapsed Time: 27 minutes and 31.67 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row with Unique_ID 'PQ-2556421016' has been updated.\n",
      "Row with Unique_ID 'PQ-2555868048' has been updated.\n",
      "Row with Unique_ID 'PQ-2555652642' has been updated.\n",
      "Row with Unique_ID 'PQ-2555496254' has been updated.\n",
      "Row with Unique_ID 'PQ-2555298056' has been updated.\n",
      "Row with Unique_ID 'PQ-2554997421' has been updated.\n",
      "Row with Unique_ID 'PQ-2554827063' has been updated.\n",
      "Row with Unique_ID 'PQ-2554615739' has been updated.\n",
      "Row with Unique_ID 'PQ-2553162752' has been updated.\n",
      "Row with Unique_ID 'PQ-2552977943' has been updated.\n",
      "Iteration: 140, Elapsed Time: 29 minutes and 33.69 seconds\n",
      "Row with Unique_ID 'PQ-2552837424' has been updated.\n",
      "Row with Unique_ID 'PQ-2551427202' has been updated.\n",
      "Row with Unique_ID 'PQ-2548654153' has been updated.\n",
      "Row with Unique_ID 'PQ-2547889634' has been updated.\n",
      "Row with Unique_ID 'PQ-2546868727' has been updated.\n",
      "Row with Unique_ID 'PQ-2546868715' has been updated.\n",
      "Row with Unique_ID 'PQ-2546770036' has been updated.\n",
      "Row with Unique_ID 'PQ-2546775564' has been updated.\n",
      "Row with Unique_ID 'PQ-2546689009' has been updated.\n",
      "Row with Unique_ID 'PQ-2546598031' has been updated.\n",
      "Iteration: 150, Elapsed Time: 31 minutes and 17.63 seconds\n",
      "Row with Unique_ID 'PQ-2543901239' has been updated.\n",
      "Row with Unique_ID 'PQ-2543700151' has been updated.\n",
      "Row with Unique_ID 'PQ-2540764382' has been updated.\n",
      "Row with Unique_ID 'PQ-2540046220' has been updated.\n",
      "Row with Unique_ID 'PQ-2540027812' has been updated.\n",
      "Row with Unique_ID 'PQ-2539904842' has been updated.\n",
      "Row with Unique_ID 'PQ-2539860166' has been updated.\n",
      "Row with Unique_ID 'PQ-2539449111' has been updated.\n",
      "Row with Unique_ID 'PQ-2539190206' has been updated.\n",
      "Row with Unique_ID 'PQ-2539121095' has been updated.\n",
      "Iteration: 160, Elapsed Time: 33 minutes and 4.82 seconds\n",
      "Row with Unique_ID 'PQ-2537420770' has been updated.\n",
      "Row with Unique_ID 'PQ-2537420768' has been updated.\n",
      "Row with Unique_ID 'PQ-2537293691' has been updated.\n",
      "Row with Unique_ID 'PQ-2537271943' has been updated.\n",
      "Row with Unique_ID 'PQ-2548036404' has been updated.\n",
      "Row with Unique_ID 'PQ-2536720302' has been updated.\n",
      "Row with Unique_ID 'PQ-2536600755' has been updated.\n",
      "Row with Unique_ID 'PQ-2536391023' has been updated.\n",
      "Row with Unique_ID 'PQ-2536263233' has been updated.\n",
      "Row with Unique_ID 'PQ-2535802630' has been updated.\n",
      "Iteration: 170, Elapsed Time: 34 minutes and 49.53 seconds\n",
      "Row with Unique_ID 'PQ-2536017983' has been updated.\n",
      "Row with Unique_ID 'PQ-2535999807' has been updated.\n",
      "Row with Unique_ID 'PQ-2535761746' has been updated.\n",
      "Row with Unique_ID 'PQ-2535548374' has been updated.\n",
      "Row with Unique_ID 'PQ-2545445113' has been updated.\n",
      "Row with Unique_ID 'PQ-2545444747' has been updated.\n",
      "Row with Unique_ID 'PQ-2545441616' has been updated.\n",
      "Row with Unique_ID 'PQ-2533651137' has been updated.\n",
      "Row with Unique_ID 'PQ-2533532967' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2533577878' has been updated.\n",
      "Iteration: 180, Elapsed Time: 36 minutes and 37.26 seconds\n",
      "Row with Unique_ID 'PQ-2533515116' has been updated.\n",
      "Row with Unique_ID 'PQ-2533460415' has been updated.\n",
      "Row with Unique_ID 'PQ-2533432114' has been updated.\n",
      "Row with Unique_ID 'PQ-2533109988' has been updated.\n",
      "Row with Unique_ID 'PQ-2774491523' has been updated.\n",
      "Row with Unique_ID 'PQ-2545003804' has been updated.\n",
      "Row with Unique_ID 'PQ-2533068003' has been updated.\n",
      "Row with Unique_ID 'PQ-2532779344' has been updated.\n",
      "Row with Unique_ID 'PQ-2532635021' has been updated.\n",
      "Row with Unique_ID 'PQ-2532613067' has been updated.\n",
      "Iteration: 190, Elapsed Time: 38 minutes and 22.61 seconds\n",
      "Row with Unique_ID 'PQ-2532502678' has been updated.\n",
      "Row with Unique_ID 'PQ-2532491185' has been updated.\n",
      "Row with Unique_ID 'PQ-2532442220' has been updated.\n",
      "Row with Unique_ID 'PQ-2532442208' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2532442173' has been updated.\n",
      "Row with Unique_ID 'PQ-2532335151' has been updated.\n",
      "Row with Unique_ID 'PQ-2532303015' has been updated.\n",
      "Row with Unique_ID 'PQ-2532283339' has been updated.\n",
      "Row with Unique_ID 'PQ-2532265642' has been updated.\n",
      "Row with Unique_ID 'PQ-2532229346' has been updated.\n",
      "Iteration: 200, Elapsed Time: 40 minutes and 14.56 seconds\n",
      "Row with Unique_ID 'PQ-2532173695' has been updated.\n",
      "Row with Unique_ID 'PQ-2532105497' has been updated.\n",
      "Row with Unique_ID 'PQ-2532105495' has been updated.\n",
      "Row with Unique_ID 'PQ-2531963321' has been updated.\n",
      "Row with Unique_ID 'PQ-2544335234' has been updated.\n",
      "Row with Unique_ID 'PQ-2531559000' has been updated.\n",
      "Row with Unique_ID 'PQ-2531508479' has been updated.\n",
      "Row with Unique_ID 'PQ-2542951398' has been updated.\n",
      "Row with Unique_ID 'PQ-2529614951' has been updated.\n",
      "Row with Unique_ID 'PQ-2529424479' has been updated.\n",
      "Iteration: 210, Elapsed Time: 42 minutes and 7.58 seconds\n",
      "Row with Unique_ID 'PQ-2528419472' has been updated.\n",
      "Row with Unique_ID 'PQ-2528048175' has been updated.\n",
      "Row with Unique_ID 'PQ-2540428980' has been updated.\n",
      "Row with Unique_ID 'PQ-2540427563' has been updated.\n",
      "Row with Unique_ID 'PQ-2540424643' has been updated.\n",
      "Row with Unique_ID 'PQ-2540423426' has been updated.\n",
      "Row with Unique_ID 'PQ-2527736409' has been updated.\n",
      "Row with Unique_ID 'PQ-2527593910' has been updated.\n",
      "Row with Unique_ID 'PQ-2526945255' has been updated.\n",
      "Row with Unique_ID 'PQ-2526062774' has been updated.\n",
      "Iteration: 220, Elapsed Time: 43 minutes and 54.11 seconds\n",
      "Row with Unique_ID 'PQ-2520398375' has been updated.\n",
      "Row with Unique_ID 'PQ-2520398244' has been updated.\n",
      "Row with Unique_ID 'PQ-2534497496' has been updated.\n",
      "Row with Unique_ID 'PQ-2519977383' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 2/5\n",
      "Row with Unique_ID 'PQ-2519926881' has been updated.\n",
      "Row with Unique_ID 'PQ-2519891876' has been updated.\n",
      "Row with Unique_ID 'PQ-2519678492' has been updated.\n",
      "Row with Unique_ID 'PQ-2519202085' has been updated.\n",
      "Row with Unique_ID 'PQ-2519132871' has been updated.\n",
      "Row with Unique_ID 'PQ-2519079155' has been updated.\n",
      "Iteration: 230, Elapsed Time: 45 minutes and 42.53 seconds\n",
      "Row with Unique_ID 'PQ-2518891590' has been updated.\n",
      "Row with Unique_ID 'PQ-2518676093' has been updated.\n",
      "Row with Unique_ID 'PQ-2518151497' has been updated.\n",
      "Row with Unique_ID 'PQ-2531869129' has been updated.\n",
      "Row with Unique_ID 'PQ-2518153748' has been updated.\n",
      "Row with Unique_ID 'PQ-2517909950' has been updated.\n",
      "Row with Unique_ID 'PQ-2516641409' has been updated.\n",
      "Row with Unique_ID 'PQ-2516621501' has been updated.\n",
      "Row with Unique_ID 'PQ-2516340721' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2529617166' has been updated.\n",
      "Iteration: 240, Elapsed Time: 47 minutes and 30.26 seconds\n",
      "Row with Unique_ID 'PQ-2514964124' has been updated.\n",
      "Row with Unique_ID 'PQ-2514818614' has been updated.\n",
      "Row with Unique_ID 'PQ-2514565663' has been updated.\n",
      "Row with Unique_ID 'PQ-2512407602' has been updated.\n",
      "Row with Unique_ID 'PQ-2509527057' has been updated.\n",
      "Row with Unique_ID 'PQ-2509222441' has been updated.\n",
      "Row with Unique_ID 'PQ-2508218688' has been updated.\n",
      "Row with Unique_ID 'PQ-2505402825' has been updated.\n",
      "Row with Unique_ID 'PQ-2505700559' has been updated.\n",
      "Row with Unique_ID 'PQ-2500297665' has been updated.\n",
      "Iteration: 250, Elapsed Time: 49 minutes and 14.01 seconds\n",
      "Row with Unique_ID 'PQ-2499282938' has been updated.\n",
      "Row with Unique_ID 'PQ-2499254435' has been updated.\n",
      "Row with Unique_ID 'PQ-2499015285' has been updated.\n",
      "Row with Unique_ID 'PQ-2498877583' has been updated.\n",
      "Row with Unique_ID 'PQ-2496992509' has been updated.\n",
      "Row with Unique_ID 'PQ-2497249743' has been updated.\n",
      "Row with Unique_ID 'PQ-2508031384' has been updated.\n",
      "Row with Unique_ID 'PQ-2508031368' has been updated.\n",
      "Row with Unique_ID 'PQ-2508030824' has been updated.\n",
      "Row with Unique_ID 'PQ-2496747696' has been updated.\n",
      "Iteration: 260, Elapsed Time: 50 minutes and 56.02 seconds\n",
      "Row with Unique_ID 'PQ-2496126853' has been updated.\n",
      "Row with Unique_ID 'PQ-2495976192' has been updated.\n",
      "Row with Unique_ID 'PQ-2495961141' has been updated.\n",
      "Row with Unique_ID 'PQ-2496337702' has been updated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row with Unique_ID 'PQ-2495484749' has been updated.\n",
      "Row with Unique_ID 'PQ-2495428733' has been updated.\n",
      "Row with Unique_ID 'PQ-2495264684' has been updated.\n",
      "Row with Unique_ID 'PQ-2507369490' has been updated.\n",
      "Row with Unique_ID 'PQ-2495216532' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2495216374' has been updated.\n",
      "Iteration: 270, Elapsed Time: 52 minutes and 49.25 seconds\n",
      "Row with Unique_ID 'PQ-2494781505' has been updated.\n",
      "Row with Unique_ID 'PQ-2494318073' has been updated.\n",
      "Row with Unique_ID 'PQ-2494214384' has been updated.\n",
      "Row with Unique_ID 'PQ-2494185849' has been updated.\n",
      "Row with Unique_ID 'PQ-2493895108' has been updated.\n",
      "Row with Unique_ID 'PQ-2493715668' has been updated.\n",
      "Row with Unique_ID 'PQ-2493505019' has been updated.\n",
      "Row with Unique_ID 'PQ-2505571935' has been updated.\n",
      "Row with Unique_ID 'PQ-2491612337' has been updated.\n",
      "Row with Unique_ID 'PQ-2490405032' has been updated.\n",
      "Iteration: 280, Elapsed Time: 54 minutes and 33.94 seconds\n",
      "Row with Unique_ID 'PQ-2489184288' has been updated.\n",
      "Row with Unique_ID 'PQ-2488070034' has been updated.\n",
      "Row with Unique_ID 'PQ-2487359190' has been updated.\n",
      "Row with Unique_ID 'PQ-2487322583' has been updated.\n",
      "Row with Unique_ID 'PQ-2487118910' has been updated.\n",
      "Row with Unique_ID 'PQ-2486610930' has been updated.\n",
      "Row with Unique_ID 'PQ-2486500044' has been updated.\n",
      "Row with Unique_ID 'PQ-2486496971' has been updated.\n",
      "Row with Unique_ID 'PQ-2485947715' has been updated.\n",
      "Row with Unique_ID 'PQ-2485384511' has been updated.\n",
      "Iteration: 290, Elapsed Time: 56 minutes and 19.48 seconds\n",
      "Row with Unique_ID 'PQ-2485321890' has been updated.\n",
      "Row with Unique_ID 'PQ-2485321172' has been updated.\n",
      "Row with Unique_ID 'PQ-2484918239' has been updated.\n",
      "Row with Unique_ID 'PQ-2484883015' has been updated.\n",
      "Row with Unique_ID 'PQ-2484876808' has been updated.\n",
      "Row with Unique_ID 'PQ-2484837375' has been updated.\n",
      "Row with Unique_ID 'PQ-2484603484' has been updated.\n",
      "Row with Unique_ID 'PQ-2484603479' has been updated.\n",
      "Row with Unique_ID 'PQ-2484403928' has been updated.\n",
      "Row with Unique_ID 'PQ-2484308028' has been updated.\n",
      "Iteration: 300, Elapsed Time: 58 minutes and 3.85 seconds\n",
      "Row with Unique_ID 'PQ-2484290042' has been updated.\n",
      "Row with Unique_ID 'PQ-2484282957' has been updated.\n",
      "Row with Unique_ID 'PQ-2484154061' has been updated.\n",
      "Row with Unique_ID 'PQ-2483940903' has been updated.\n",
      "Row with Unique_ID 'PQ-2483799889' has been updated.\n",
      "Row with Unique_ID 'PQ-2483771734' has been updated.\n",
      "Row with Unique_ID 'PQ-2484145247' has been updated.\n",
      "Row with Unique_ID 'PQ-2483671975' has been updated.\n",
      "Row with Unique_ID 'PQ-2483404623' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2483205634' has been updated.\n",
      "Iteration: 310, Elapsed Time: 59 minutes and 48.89 seconds\n",
      "Row with Unique_ID 'PQ-2483015245' has been updated.\n",
      "Row with Unique_ID 'PQ-2482345098' has been updated.\n",
      "Row with Unique_ID 'PQ-2481455550' has been updated.\n",
      "Row with Unique_ID 'PQ-2481455407' has been updated.\n",
      "Row with Unique_ID 'PQ-2481396359' has been updated.\n",
      "Row with Unique_ID 'PQ-2481221073' has been updated.\n",
      "Row with Unique_ID 'PQ-2481189625' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2481096772' has been updated.\n",
      "Row with Unique_ID 'PQ-2480891534' has been updated.\n",
      "Row with Unique_ID 'PQ-2480650347' has been updated.\n",
      "Iteration: 320, Elapsed Time: 61 minutes and 39.53 seconds\n",
      "Row with Unique_ID 'PQ-2480089072' has been updated.\n",
      "Row with Unique_ID 'PQ-2478817926' has been updated.\n",
      "Row with Unique_ID 'PQ-2478726100' has been updated.\n",
      "Row with Unique_ID 'PQ-2478657204' has been updated.\n",
      "Row with Unique_ID 'PQ-2478200471' has been updated.\n",
      "Row with Unique_ID 'PQ-2478150121' has been updated.\n",
      "Row with Unique_ID 'PQ-2478163348' has been updated.\n",
      "Row with Unique_ID 'PQ-2478076294' has been updated.\n",
      "Row with Unique_ID 'PQ-2478010213' has been updated.\n",
      "Row with Unique_ID 'PQ-2478009015' has been updated.\n",
      "Iteration: 330, Elapsed Time: 63 minutes and 22.64 seconds\n",
      "Row with Unique_ID 'PQ-2477722476' has been updated.\n",
      "Row with Unique_ID 'PQ-2477267544' has been updated.\n",
      "Row with Unique_ID 'PQ-2476560818' has been updated.\n",
      "Row with Unique_ID 'PQ-2475861185' has been updated.\n",
      "Row with Unique_ID 'PQ-2512319253' has been updated.\n",
      "Row with Unique_ID 'PQ-2475619025' has been updated.\n",
      "Row with Unique_ID 'PQ-2474589347' has been updated.\n",
      "Row with Unique_ID 'PQ-2474588815' has been updated.\n",
      "Row with Unique_ID 'PQ-2474348564' has been updated.\n",
      "Row with Unique_ID 'PQ-2474275984' has been updated.\n",
      "Iteration: 340, Elapsed Time: 65 minutes and 8.96 seconds\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2471081277' has been updated.\n",
      "Row with Unique_ID 'PQ-2471053937' has been updated.\n",
      "Row with Unique_ID 'PQ-2470787272' has been updated.\n",
      "Row with Unique_ID 'PQ-2478178985' has been updated.\n",
      "Row with Unique_ID 'PQ-2470409504' has been updated.\n",
      "Row with Unique_ID 'PQ-2477837886' has been updated.\n",
      "Row with Unique_ID 'PQ-2477837382' has been updated.\n",
      "Row with Unique_ID 'PQ-2470134262' has been updated.\n",
      "Row with Unique_ID 'PQ-2470059362' has been updated.\n",
      "Row with Unique_ID 'PQ-2470009859' has been updated.\n",
      "Iteration: 350, Elapsed Time: 66 minutes and 58.68 seconds\n",
      "Row with Unique_ID 'PQ-2469952574' has been updated.\n",
      "Row with Unique_ID 'PQ-2469847982' has been updated.\n",
      "Row with Unique_ID 'PQ-2469823580' has been updated.\n",
      "Row with Unique_ID 'PQ-2469556812' has been updated.\n",
      "Row with Unique_ID 'PQ-2469417618' has been updated.\n",
      "Row with Unique_ID 'PQ-2476490890' has been updated.\n",
      "Row with Unique_ID 'PQ-2469350826' has been updated.\n",
      "Row with Unique_ID 'PQ-2469112294' has been updated.\n",
      "Row with Unique_ID 'PQ-2468959576' has been updated.\n",
      "Row with Unique_ID 'PQ-2468818093' has been updated.\n",
      "Iteration: 360, Elapsed Time: 68 minutes and 41.68 seconds\n",
      "Row with Unique_ID 'PQ-2468590320' has been updated.\n",
      "Row with Unique_ID 'PQ-2468022942' has been updated.\n",
      "Row with Unique_ID 'PQ-2467848547' has been updated.\n",
      "Row with Unique_ID 'PQ-2467782548' has been updated.\n",
      "Row with Unique_ID 'PQ-2467659348' has been updated.\n",
      "Row with Unique_ID 'PQ-2467551252' has been updated.\n",
      "Row with Unique_ID 'PQ-2467525973' has been updated.\n",
      "Row with Unique_ID 'PQ-2467506874' has been updated.\n",
      "Row with Unique_ID 'PQ-2466104230' has been updated.\n",
      "Row with Unique_ID 'PQ-2466104062' has been updated.\n",
      "Iteration: 370, Elapsed Time: 70 minutes and 28.18 seconds\n",
      "Row with Unique_ID 'PQ-2465948139' has been updated.\n",
      "Row with Unique_ID 'PQ-2474392357' has been updated.\n",
      "Row with Unique_ID 'PQ-2465901936' has been updated.\n",
      "Row with Unique_ID 'PQ-2465821388' has been updated.\n",
      "Row with Unique_ID 'PQ-2465669543' has been updated.\n",
      "Row with Unique_ID 'PQ-2465669525' has been updated.\n",
      "Row with Unique_ID 'PQ-2465570612' has been updated.\n",
      "Row with Unique_ID 'PQ-2465487384' has been updated.\n",
      "Row with Unique_ID 'PQ-2465486904' has been updated.\n",
      "Row with Unique_ID 'PQ-2464845343' has been updated.\n",
      "Iteration: 380, Elapsed Time: 72 minutes and 11.27 seconds\n",
      "Row with Unique_ID 'PQ-2464566814' has been updated.\n",
      "Row with Unique_ID 'PQ-2464604359' has been updated.\n",
      "Row with Unique_ID 'PQ-2464541547' has been updated.\n",
      "Row with Unique_ID 'PQ-2464494204' has been updated.\n",
      "Row with Unique_ID 'PQ-2967214043' has been updated.\n",
      "Row with Unique_ID 'PQ-2901792732' has been updated.\n",
      "Row with Unique_ID 'PQ-2900527371' has been updated.\n",
      "Row with Unique_ID 'PQ-2900817989' has been updated.\n",
      "Row with Unique_ID 'PQ-2897461879' has been updated.\n",
      "Row with Unique_ID 'PQ-2895137937' has been updated.\n",
      "Iteration: 390, Elapsed Time: 73 minutes and 58.27 seconds\n",
      "Row with Unique_ID 'PQ-2895038109' has been updated.\n",
      "Row with Unique_ID 'PQ-2880296882' has been updated.\n",
      "Row with Unique_ID 'PQ-2853253212' has been updated.\n",
      "Row with Unique_ID 'PQ-2831429113' has been updated.\n",
      "Row with Unique_ID 'PQ-2830473201' has been updated.\n",
      "Row with Unique_ID 'PQ-2828934022' has been updated.\n",
      "Row with Unique_ID 'PQ-2821029800' has been updated.\n",
      "Row with Unique_ID 'PQ-2813853758' has been updated.\n",
      "Row with Unique_ID 'PQ-2813715564' has been updated.\n",
      "Row with Unique_ID 'PQ-2810235907' has been updated.\n",
      "Iteration: 400, Elapsed Time: 75 minutes and 40.46 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row with Unique_ID 'PQ-2810039935' has been updated.\n",
      "Row with Unique_ID 'PQ-2776475145' has been updated.\n",
      "Row with Unique_ID 'PQ-2776232171' has been updated.\n",
      "Row with Unique_ID 'PQ-2747239066' has been updated.\n",
      "Row with Unique_ID 'PQ-2715702627' has been updated.\n",
      "Row with Unique_ID 'PQ-2714237491' has been updated.\n",
      "Row with Unique_ID 'PQ-2712884191' has been updated.\n",
      "Row with Unique_ID 'PQ-2712125875' has been updated.\n",
      "Row with Unique_ID 'PQ-2712111967' has been updated.\n",
      "Row with Unique_ID 'PQ-2706110677' has been updated.\n",
      "Iteration: 410, Elapsed Time: 77 minutes and 29.77 seconds\n",
      "Row with Unique_ID 'PQ-2705184310' has been updated.\n",
      "Row with Unique_ID 'PQ-2705174835' has been updated.\n",
      "Row with Unique_ID 'PQ-2705118452' has been updated.\n",
      "Row with Unique_ID 'PQ-2705046498' has been updated.\n",
      "Row with Unique_ID 'PQ-2704990848' has been updated.\n",
      "Row with Unique_ID 'PQ-2704785118' has been updated.\n",
      "Row with Unique_ID 'PQ-2704701727' has been updated.\n",
      "Row with Unique_ID 'PQ-2704810862' has been updated.\n",
      "Row with Unique_ID 'PQ-2704162185' has been updated.\n",
      "Row with Unique_ID 'PQ-2704112885' has been updated.\n",
      "Iteration: 420, Elapsed Time: 79 minutes and 12.35 seconds\n",
      "Row with Unique_ID 'PQ-2704062353' has been updated.\n",
      "Row with Unique_ID 'PQ-2701133433' has been updated.\n",
      "Row with Unique_ID 'PQ-2699929060' has been updated.\n",
      "Row with Unique_ID 'PQ-2699842960' has been updated.\n",
      "Row with Unique_ID 'PQ-2699818476' has been updated.\n",
      "Row with Unique_ID 'PQ-2697460053' has been updated.\n",
      "Row with Unique_ID 'PQ-2697139949' has been updated.\n",
      "Row with Unique_ID 'PQ-2694229078' has been updated.\n",
      "Row with Unique_ID 'PQ-2693284635' has been updated.\n",
      "Row with Unique_ID 'PQ-2692022760' has been updated.\n",
      "Iteration: 430, Elapsed Time: 80 minutes and 59.47 seconds\n",
      "Row with Unique_ID 'PQ-2692589992' has been updated.\n",
      "Row with Unique_ID 'PQ-2691422296' has been updated.\n",
      "Row with Unique_ID 'PQ-2691196096' has been updated.\n",
      "Row with Unique_ID 'PQ-2688971597' has been updated.\n",
      "Row with Unique_ID 'PQ-2688770018' has been updated.\n",
      "Row with Unique_ID 'PQ-2688471279' has been updated.\n",
      "Row with Unique_ID 'PQ-2685980259' has been updated.\n",
      "Row with Unique_ID 'PQ-2685931087' has been updated.\n",
      "Row with Unique_ID 'PQ-2685773566' has been updated.\n",
      "Row with Unique_ID 'PQ-2684680370' has been updated.\n",
      "Iteration: 440, Elapsed Time: 82 minutes and 40.70 seconds\n",
      "Row with Unique_ID 'PQ-2683958151' has been updated.\n",
      "Row with Unique_ID 'PQ-2683126959' has been updated.\n",
      "Row with Unique_ID 'PQ-2681503286' has been updated.\n",
      "Row with Unique_ID 'PQ-2681382178' has been updated.\n",
      "Row with Unique_ID 'PQ-2681200985' has been updated.\n",
      "Row with Unique_ID 'PQ-2680317469' has been updated.\n",
      "Row with Unique_ID 'PQ-2680250511' has been updated.\n",
      "Row with Unique_ID 'PQ-2679710973' has been updated.\n",
      "Row with Unique_ID 'PQ-2679655952' has been updated.\n",
      "Row with Unique_ID 'PQ-2679590031' has been updated.\n",
      "Iteration: 450, Elapsed Time: 84 minutes and 22.20 seconds\n",
      "Row with Unique_ID 'PQ-2663982026' has been updated.\n",
      "Row with Unique_ID 'PQ-2663430881' has been updated.\n",
      "Row with Unique_ID 'PQ-2663408630' has been updated.\n",
      "Row with Unique_ID 'PQ-2661844360' has been updated.\n",
      "Row with Unique_ID 'PQ-2660777234' has been updated.\n",
      "Row with Unique_ID 'PQ-2660673363' has been updated.\n",
      "Row with Unique_ID 'PQ-2659697669' has been updated.\n",
      "Row with Unique_ID 'PQ-2659394452' has been updated.\n",
      "Row with Unique_ID 'PQ-2656541574' has been updated.\n",
      "Row with Unique_ID 'PQ-2645378438' has been updated.\n",
      "Iteration: 460, Elapsed Time: 86 minutes and 2.72 seconds\n",
      "Row with Unique_ID 'PQ-2640917431' has been updated.\n",
      "Row with Unique_ID 'PQ-2650975123' has been updated.\n",
      "Row with Unique_ID 'PQ-2650975114' has been updated.\n",
      "Row with Unique_ID 'PQ-2650974750' has been updated.\n",
      "Row with Unique_ID 'PQ-2650974598' has been updated.\n",
      "Row with Unique_ID 'PQ-2640096684' has been updated.\n",
      "Row with Unique_ID 'PQ-2640012358' has been updated.\n",
      "Row with Unique_ID 'PQ-2640009105' has been updated.\n",
      "Row with Unique_ID 'PQ-2639875424' has been updated.\n",
      "Row with Unique_ID 'PQ-2638574684' has been updated.\n",
      "Iteration: 470, Elapsed Time: 87 minutes and 47.87 seconds\n",
      "Row with Unique_ID 'PQ-2638461684' has been updated.\n",
      "Row with Unique_ID 'PQ-2638275804' has been updated.\n",
      "Row with Unique_ID 'PQ-2637586235' has been updated.\n",
      "Row with Unique_ID 'PQ-2647676194' has been updated.\n",
      "Row with Unique_ID 'PQ-2647628812' has been updated.\n",
      "Row with Unique_ID 'PQ-2647477413' has been updated.\n",
      "Row with Unique_ID 'PQ-2637265939' has been updated.\n",
      "Row with Unique_ID 'PQ-2636364828' has been updated.\n",
      "Row with Unique_ID 'PQ-2636155200' has been updated.\n",
      "Row with Unique_ID 'PQ-2636123812' has been updated.\n",
      "Iteration: 480, Elapsed Time: 89 minutes and 37.08 seconds\n",
      "Row with Unique_ID 'PQ-2636118044' has been updated.\n",
      "Row with Unique_ID 'PQ-2635919334' has been updated.\n",
      "Row with Unique_ID 'PQ-2635876825' has been updated.\n",
      "Row with Unique_ID 'PQ-2635844203' has been updated.\n",
      "Row with Unique_ID 'PQ-2635700101' has been updated.\n",
      "Row with Unique_ID 'PQ-2635684695' has been updated.\n",
      "Row with Unique_ID 'PQ-2632124547' has been updated.\n",
      "Row with Unique_ID 'PQ-2603938368' has been updated.\n",
      "Row with Unique_ID 'PQ-2580664569' has been updated.\n",
      "Row with Unique_ID 'PQ-2559402711' has been updated.\n",
      "Iteration: 490, Elapsed Time: 91 minutes and 18.97 seconds\n",
      "Row with Unique_ID 'PQ-2552585208' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 2/5\n",
      "Row with Unique_ID 'PQ-2525093646' has been updated.\n",
      "Row with Unique_ID 'PQ-2496990457' has been updated.\n",
      "Row with Unique_ID 'PQ-2489528304' has been updated.\n",
      "Row with Unique_ID 'PQ-2490851281' has been updated.\n",
      "Row with Unique_ID 'PQ-2490848845' has been updated.\n",
      "Row with Unique_ID 'PQ-2490848793' has been updated.\n",
      "Row with Unique_ID 'PQ-2474378114' has been updated.\n",
      "Row with Unique_ID 'PQ-2469983703' has been updated.\n",
      "Row with Unique_ID 'PQ-2463570155' has been updated.\n",
      "Iteration: 500, Elapsed Time: 93 minutes and 13.47 seconds\n",
      "Row with Unique_ID 'PQ-2460078898' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2451256483' has been updated.\n",
      "Row with Unique_ID 'PQ-2447800621' has been updated.\n",
      "Row with Unique_ID 'PQ-2434446208' has been updated.\n",
      "Row with Unique_ID 'PQ-2434110218' has been updated.\n",
      "Row with Unique_ID 'PQ-2421891493' has been updated.\n",
      "Row with Unique_ID 'PQ-2410842915' has been updated.\n",
      "Row with Unique_ID 'PQ-2409726865' has been updated.\n",
      "Row with Unique_ID 'PQ-2407626409' has been updated.\n",
      "Row with Unique_ID 'PQ-2398826370' has been updated.\n",
      "Iteration: 510, Elapsed Time: 95 minutes and 1.43 seconds\n",
      "Row with Unique_ID 'PQ-2398423909' has been updated.\n",
      "Row with Unique_ID 'PQ-2398312028' has been updated.\n",
      "Row with Unique_ID 'PQ-2409946584' has been updated.\n",
      "Row with Unique_ID 'PQ-2405422022' has been updated.\n",
      "Row with Unique_ID 'PQ-2405410674' has been updated.\n",
      "Row with Unique_ID 'PQ-2403415450' has been updated.\n",
      "Row with Unique_ID 'PQ-2403409522' has been updated.\n",
      "Row with Unique_ID 'PQ-2403409209' has been updated.\n",
      "Row with Unique_ID 'PQ-2403408849' has been updated.\n",
      "Row with Unique_ID 'PQ-2390060390' has been updated.\n",
      "Iteration: 520, Elapsed Time: 96 minutes and 43.49 seconds\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2390207567' has been updated.\n",
      "Row with Unique_ID 'PQ-2389699880' has been updated.\n",
      "Row with Unique_ID 'PQ-2386682270' has been updated.\n",
      "Row with Unique_ID 'PQ-2386392174' has been updated.\n",
      "Row with Unique_ID 'PQ-2386258973' has been updated.\n",
      "Row with Unique_ID 'PQ-2386109619' has been updated.\n",
      "Row with Unique_ID 'PQ-2386069708' has been updated.\n",
      "Row with Unique_ID 'PQ-2385836977' has been updated.\n",
      "Row with Unique_ID 'PQ-2385127261' has been updated.\n",
      "Row with Unique_ID 'PQ-2384905013' has been updated.\n",
      "Iteration: 530, Elapsed Time: 98 minutes and 36.70 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m company, source, headline, text \u001b[38;5;241m=\u001b[39m get_gemini_inputs(text_df, unique_id)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Query Gemini\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mquery_gemini\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheadline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Parse text\u001b[39;00m\n\u001b[0;32m     21\u001b[0m sentiment_dict \u001b[38;5;241m=\u001b[39m parse_sentiment(response, CATEGORIES)\n",
      "Cell \u001b[1;32mIn[10], line 95\u001b[0m, in \u001b[0;36mquery_gemini\u001b[1;34m(company, source, headline, text, model)\u001b[0m\n\u001b[0;32m     40\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124mGiven the text from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_source\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m about \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, analyze the content and perform sentiment analysis across multiple predefined categories.\u001b[39m\n\u001b[0;32m     42\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124mDO NOT bold or bullet the output summary.\u001b[39m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# print(prompt)\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\capstone\\lib\\site-packages\\google\\generativeai\\generative_models.py:262\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[1;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mgenerate_content(\n\u001b[0;32m    263\u001b[0m             request,\n\u001b[0;32m    264\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest_options,\n\u001b[0;32m    265\u001b[0m         )\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\capstone\\lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:812\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 812\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    820\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\capstone\\lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\capstone\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\capstone\\lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\capstone\\lib\\site-packages\\google\\api_core\\timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\capstone\\lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(callable_)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror_remapped_callable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\capstone\\lib\\site-packages\\grpc\\_channel.py:1178\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1168\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1173\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1174\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m   1175\u001b[0m     (\n\u001b[0;32m   1176\u001b[0m         state,\n\u001b[0;32m   1177\u001b[0m         call,\n\u001b[1;32m-> 1178\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\capstone\\lib\\site-packages\\grpc\\_channel.py:1162\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1145\u001b[0m state\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_target)\n\u001b[0;32m   1146\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msegregated_call(\n\u001b[0;32m   1147\u001b[0m     cygrpc\u001b[38;5;241m.\u001b[39mPropagationConstants\u001b[38;5;241m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registered_call_handle,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[1;32m-> 1162\u001b[0m event \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1163\u001b[0m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_deserializer)\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
      "File \u001b[1;32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/channel.pyx.pxi:400\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/channel.pyx.pxi:211\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/channel.pyx.pxi:205\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/completion_queue.pyx.pxi:78\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\python\\\\grpcio\\\\grpc\\\\_cython\\\\_cygrpc/completion_queue.pyx.pxi:42\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "unique_id = find_first_unique_id_with_empty_values(SENTIMENT_RESULTS_FILE_PATH, CATEGORIES)\n",
    "count = 0\n",
    "max_tries = 5\n",
    "\n",
    "# Iterate through the CSV using the functions\n",
    "while unique_id:\n",
    "    retries = 0\n",
    "    success = False\n",
    "\n",
    "    # There are multiple errors that can happen, many of them simply need another try\n",
    "    while retries < max_tries and not success:\n",
    "        try:\n",
    "            # Get gemini inputs\n",
    "            company, source, headline, text = get_gemini_inputs(text_df, unique_id)\n",
    "\n",
    "            # Query Gemini\n",
    "            response = query_gemini(company, source, headline, text, model)\n",
    "\n",
    "            # Parse text\n",
    "            sentiment_dict = parse_sentiment(response, CATEGORIES)\n",
    "\n",
    "            # Update the csv\n",
    "            update_csv(SENTIMENT_RESULTS_FILE_PATH, unique_id, sentiment_dict)\n",
    "\n",
    "            success = True\n",
    "\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            print(f\"Error encountered: {e}. Retry {retries}/{max_tries}\")\n",
    "\n",
    "            if retries >= max_tries:\n",
    "                print(f\"Failed to process unique_id {unique_id} after {max_tries} attempts. Stopping.\")\n",
    "                # Exit both loops\n",
    "                unique_id = None\n",
    "                break\n",
    "\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Get an update every 10 rows\n",
    "    count += 1\n",
    "    if count % 10 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        minutes, seconds = divmod(elapsed_time, 60)\n",
    "        print(f\"Iteration: {count}, Elapsed Time: {int(minutes)} minutes and {seconds:.2f} seconds\")\n",
    "\n",
    "    # Find the next empty row\n",
    "    unique_id = find_first_unique_id_with_empty_values(SENTIMENT_RESULTS_FILE_PATH, CATEGORIES)\n",
    "\n",
    "    # Test print statements\n",
    "    # print(unique_id)\n",
    "    # print(f\"Company: {company}\\n\")\n",
    "    # print(f\"Source: {source}\\n\")\n",
    "    # print(f\"Headline: {headline}\\n\")\n",
    "    # print(f\"Text:\\n{text}\")\n",
    "    # print(response)\n",
    "    # print(sentiment_dict)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (capstone)",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
