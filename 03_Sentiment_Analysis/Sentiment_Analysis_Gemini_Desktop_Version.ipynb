{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zy9ROy-SV-U8"
   },
   "source": [
    "# Import Libraries and Clone Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pRIDt_1bVnna"
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hq_SPv2LyIPv"
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "CATEGORIES = [\n",
    "        \"Finance\",\n",
    "        \"Production\",\n",
    "        \"Reserves / Exploration / Acquisitions / Mergers / Divestments\",\n",
    "        \"Environment / Regulatory / Geopolitics\",\n",
    "        \"Alternative Energy / Lower Carbon\",\n",
    "        \"Oil Price / Natural Gas Price / Gasoline Price\"]\n",
    "SENTIMENT_RESULTS_FILE_PATH = 'Sentiment_Analysis_Results.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzRf877uW7h0"
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "QQwgVqdqXFqZ",
    "outputId": "8d908241-04e1-4f45-e407-699f50d1f94b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10126, 7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Article Headline</th>\n",
       "      <th>Article Text</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Investment Research</td>\n",
       "      <td>IR-1</td>\n",
       "      <td>MRO</td>\n",
       "      <td>2024-05-16</td>\n",
       "      <td>Marathon Oil Corporation</td>\n",
       "      <td>Stock Report | May 16, 2024 | NYSESymbol: MRO ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Investment Research</td>\n",
       "      <td>IR-2</td>\n",
       "      <td>EOG</td>\n",
       "      <td>2024-05-14</td>\n",
       "      <td>EOG Resources, Inc.</td>\n",
       "      <td>Stock Report | May 14, 2024 | NYSESymbol: EOG ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Investment Research</td>\n",
       "      <td>IR-3</td>\n",
       "      <td>EOG</td>\n",
       "      <td>2024-05-11</td>\n",
       "      <td>EOG Resources, Inc.</td>\n",
       "      <td>Stock Report | May 11, 2024 | NYSESymbol: EOG ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Investment Research</td>\n",
       "      <td>IR-4</td>\n",
       "      <td>DVN</td>\n",
       "      <td>2024-05-11</td>\n",
       "      <td>Devon Energy Corporation</td>\n",
       "      <td>Stock Report | May 11, 2024 | NYSESymbol: DVN ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Investment Research</td>\n",
       "      <td>IR-5</td>\n",
       "      <td>COP</td>\n",
       "      <td>2024-05-07</td>\n",
       "      <td>ConocoPhillips</td>\n",
       "      <td>Stock Report | May 07, 2024 | NYSESymbol: COP ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Source Unique_ID Ticker        Date          Article Headline  \\\n",
       "0  Investment Research      IR-1    MRO  2024-05-16  Marathon Oil Corporation   \n",
       "1  Investment Research      IR-2    EOG  2024-05-14       EOG Resources, Inc.   \n",
       "2  Investment Research      IR-3    EOG  2024-05-11       EOG Resources, Inc.   \n",
       "3  Investment Research      IR-4    DVN  2024-05-11  Devon Energy Corporation   \n",
       "4  Investment Research      IR-5    COP  2024-05-07            ConocoPhillips   \n",
       "\n",
       "                                        Article Text  URL  \n",
       "0  Stock Report | May 16, 2024 | NYSESymbol: MRO ...  NaN  \n",
       "1  Stock Report | May 14, 2024 | NYSESymbol: EOG ...  NaN  \n",
       "2  Stock Report | May 11, 2024 | NYSESymbol: EOG ...  NaN  \n",
       "3  Stock Report | May 11, 2024 | NYSESymbol: DVN ...  NaN  \n",
       "4  Stock Report | May 07, 2024 | NYSESymbol: COP ...  NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Date</th>\n",
       "      <th>Article Headline</th>\n",
       "      <th>Article Text</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10121</th>\n",
       "      <td>Earnings Call Q&amp;A</td>\n",
       "      <td>EQ-338</td>\n",
       "      <td>XOM</td>\n",
       "      <td>Feb-02-2021</td>\n",
       "      <td>Exxon Mobil Corporation, Q4 2020 Earnings Call...</td>\n",
       "      <td>Question and Answer\\r\\nOperator\\r\\n[Operator I...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10122</th>\n",
       "      <td>Earnings Call Q&amp;A</td>\n",
       "      <td>EQ-339</td>\n",
       "      <td>COP</td>\n",
       "      <td>Feb-02-2021</td>\n",
       "      <td>ConocoPhillips, Q4 2020 Earnings Call, Feb 02,...</td>\n",
       "      <td>Question and Answer\\r\\nOperator\\r\\n[Operator I...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10123</th>\n",
       "      <td>Earnings Call Q&amp;A</td>\n",
       "      <td>EQ-340</td>\n",
       "      <td>EOG</td>\n",
       "      <td>May-03-2019</td>\n",
       "      <td>EOG Resources, Inc., Q1 2019 Earnings Call, Ma...</td>\n",
       "      <td>Question and Answer\\r\\nOperator\\r\\n[Operator I...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10124</th>\n",
       "      <td>Earnings Call Q&amp;A</td>\n",
       "      <td>EQ-341</td>\n",
       "      <td>SHEL</td>\n",
       "      <td>May-02-2019</td>\n",
       "      <td>Royal Dutch Shell plc, Q1 2019 Earnings Call, ...</td>\n",
       "      <td>Question and Answer\\r\\nOperator\\r\\n[Operator I...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10125</th>\n",
       "      <td>Earnings Call Q&amp;A</td>\n",
       "      <td>EQ-342</td>\n",
       "      <td>COP</td>\n",
       "      <td>Apr-30-2019</td>\n",
       "      <td>ConocoPhillips, Q1 2019 Earnings Call, Apr 30,...</td>\n",
       "      <td>Question and Answer\\r\\nOperator\\r\\n[Operator I...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Source Unique_ID Ticker         Date  \\\n",
       "10121  Earnings Call Q&A    EQ-338    XOM  Feb-02-2021   \n",
       "10122  Earnings Call Q&A    EQ-339    COP  Feb-02-2021   \n",
       "10123  Earnings Call Q&A    EQ-340    EOG  May-03-2019   \n",
       "10124  Earnings Call Q&A    EQ-341   SHEL  May-02-2019   \n",
       "10125  Earnings Call Q&A    EQ-342    COP  Apr-30-2019   \n",
       "\n",
       "                                        Article Headline  \\\n",
       "10121  Exxon Mobil Corporation, Q4 2020 Earnings Call...   \n",
       "10122  ConocoPhillips, Q4 2020 Earnings Call, Feb 02,...   \n",
       "10123  EOG Resources, Inc., Q1 2019 Earnings Call, Ma...   \n",
       "10124  Royal Dutch Shell plc, Q1 2019 Earnings Call, ...   \n",
       "10125  ConocoPhillips, Q1 2019 Earnings Call, Apr 30,...   \n",
       "\n",
       "                                            Article Text  URL  \n",
       "10121  Question and Answer\\r\\nOperator\\r\\n[Operator I...  NaN  \n",
       "10122  Question and Answer\\r\\nOperator\\r\\n[Operator I...  NaN  \n",
       "10123  Question and Answer\\r\\nOperator\\r\\n[Operator I...  NaN  \n",
       "10124  Question and Answer\\r\\nOperator\\r\\n[Operator I...  NaN  \n",
       "10125  Question and Answer\\r\\nOperator\\r\\n[Operator I...  NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import all cleaned data files\n",
    "invest_df1 = pd.read_csv('../02_Cleaned_Data/Investment_Research_Part1.csv')\n",
    "invest_df2 = pd.read_csv('../02_Cleaned_Data/Investment_Research_Part2.csv')\n",
    "proquest_df = pd.read_csv('../02_Cleaned_Data/ProQuest_Articles.csv')\n",
    "earnings_presentations = pd.read_csv('../02_Cleaned_Data/Earnings_Presentations.csv')\n",
    "earnings_qa = pd.read_csv('../02_Cleaned_Data/Earnings_QA.csv')\n",
    "sec_df = pd.read_csv('../02_Cleaned_Data/SEC_Filings.csv')\n",
    "\n",
    "# Merge into single df\n",
    "text_df = pd.concat([invest_df1, invest_df2, proquest_df, sec_df, earnings_presentations, earnings_qa], ignore_index=True)\n",
    "display(text_df.shape)\n",
    "display(text_df.head())\n",
    "display(text_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([5379], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "# Drop rows google gemini will not process\n",
    "rows_to_drop = ['PQ-2840736837']\n",
    "index_to_drops = text_df[text_df['Unique_ID'].isin(rows_to_drop)].index\n",
    "text_df.drop(index_to_drops, inplace=True)\n",
    "print(index_to_drops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qqZ2XDGuWNLM"
   },
   "source": [
    "# Find or Create Results CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DgjxgwbZoY0b",
    "outputId": "bda7c259-5480-483f-d674-016402ddf9a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file exists in the current directory.\n"
     ]
    }
   ],
   "source": [
    "# Determine if the Sentiment Analysis Results file already exists\n",
    "file_exists = os.path.isfile(SENTIMENT_RESULTS_FILE_PATH)\n",
    "\n",
    "# Print the result\n",
    "if file_exists:\n",
    "    print(f\"The file exists in the current directory.\")\n",
    "else:\n",
    "    print(f\"The file does not exist in the current directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3_gsRCosWRfE"
   },
   "outputs": [],
   "source": [
    "# Create an empty file if the file does not exist\n",
    "if not file_exists:\n",
    "    # Copy text df and drop the text and headline column due to size\n",
    "    empty_sentiment_df = text_df.copy()\n",
    "    empty_sentiment_df = empty_sentiment_df.drop(['Article Text', 'Article Headline'], axis=1)\n",
    "\n",
    "    for category in CATEGORIES:\n",
    "        empty_sentiment_df[category] = \"\"\n",
    "        empty_sentiment_df[category] = empty_sentiment_df[category].astype('object')\n",
    "\n",
    "    # Display results\n",
    "    display(empty_sentiment_df.head())\n",
    "\n",
    "    # Save as CSV\n",
    "    empty_sentiment_df.to_csv(SENTIMENT_RESULTS_FILE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WQ56LlUp-rH"
   },
   "source": [
    "### NOTE: If you want to re-run the sentiment analysis, delete or archive the csv to create a blank one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMeL1ZOcXSz9"
   },
   "source": [
    "# Sentiment Analysis\n",
    "NOTE: Google gemini **currently** has a daily query limit of 1,500 requests per day.  As we have over 10,000 documents, the code will be designed to run over multiple days and pick up where we last left off.  After the code is run, the user will still need to manually download the csv and upload to github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmpkXUk-DZfr"
   },
   "source": [
    "**Gemini Free Rate Limits**\n",
    "*   15 RPM (requests per minute)\n",
    "*   1 million TPM (tokens per minute)\n",
    "*   1,500 RPD (requests per day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nVdZDCVf2iHr"
   },
   "outputs": [],
   "source": [
    "# Set up Gemini. (API key needs to be in your environment variables)\n",
    "key = 'GOOGLE_API_KEY'\n",
    "GOOGLE_API_KEY = os.getenv(key)\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# So it doesn't block the output\n",
    "safety_settings = [\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",},]\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest', safety_settings=safety_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22Zt879CulED",
    "outputId": "59e59419-e4a3-4ea9-a151-5d84a8799f0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PQ-2840643340\n"
     ]
    }
   ],
   "source": [
    "# Function to find the first empty row of the csv\n",
    "def find_first_unique_id_with_empty_values(file_path, categories):\n",
    "    \"\"\"\n",
    "    Finds the first unique ID where any of the specified columns have empty values in a CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        categories (list of str): List of column names to check for empty values.\n",
    "\n",
    "    Returns:\n",
    "        str: The first Unique_ID where any of the specified columns have empty values.\n",
    "        None: If no such row is found.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Iterate over each row to find the first unique ID with any empty values in the specified columns\n",
    "    for index, row in df.iterrows():\n",
    "        if row[categories].isnull().any() or (row[categories] == '').any():\n",
    "            return row['Unique_ID']\n",
    "\n",
    "    return None  # Return None if no such row is found\n",
    "\n",
    "# Test function\n",
    "unique_id = find_first_unique_id_with_empty_values(SENTIMENT_RESULTS_FILE_PATH, CATEGORIES)\n",
    "print(unique_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0Gg79X82NYq",
    "outputId": "dd6b7aa5-40c0-4155-bc76-d647f9b9d8e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company: XOM\n",
      "\n",
      "Source: ProQuest\n",
      "\n",
      "Headline: Google Mcdonalds Exxon Headline Busy Earnings\n",
      "\n",
      "Text:\n",
      "Microsoft, Procter & Gamble and Coca-Cola are among the household names set to report their quarterly results in the coming week, offering insights into consumer and corporate spending as inflation cools .\r\n",
      "Others scheduled to issue updates include tech leader Meta Platforms and Google parent Alphabet, energy heavyweights Exxon Mobil and Chevron, payment firms Visa and Mastercard, and restaurant operators Chipotle Mexican Grill and McDonald's.\r\n",
      "Overall, 166 S&P 500 companies, including 12 Dow Jones Industrial Average components, are scheduled to report next week, according to FactSet.\r\n",
      "Together, their results will offer a clearer picture of the pace of the economy and the financial health of the American consumer as overall economic growth slows . Companies continue to hire, fueling a hot job market, and the swell of pandemic-era inflation appears to have subsided ; meanwhile, Fed officials recently signaled that more interest rate increases are coming. \r\n",
      "The McDonald's earnings report will help gauge consumers' enthusiasm for spending money on discretionary purchases—such as going out to eat. PHOTO: Hannah Beier for The Wall Street Journal\r\n",
      "So far, almost one-fifth of S&P 500 companies have reported this quarter, with earnings on track to fall 9% from the year-ago period, according to FactSet.\r\n",
      "Microsoft and Alphabet, reporting Tuesday, as well as Facebook and Instagram parent Meta Platforms, reporting Wednesday, are expected to update investors on artificial- intelligence investments even as efforts develop to rein in the nascent technology . Investors have added billions of dollars to big tech companies' market values on news of AI advancements, sending stocks surging. Google and Microsoft both backed their own AI chatbots to compete with ChatGPT in recent weeks.\r\n",
      "Intel on Thursday will give clues about the state of semiconductor production and its search for big-name customers for its chips , amid tensions between the U.S. and China. The Wall Street Journal reported in June that the Biden administration is considering limiting certain chip sales to China.\r\n",
      "Reports from Southwest Airlines on Thursday, Alaska Air Group on Tuesday and hotel chain Hilton Worldwide on Wednesday are expected to highlight the strength of travel demand in the busy summer season and how companies are handling a surge of vacation demand. American Airlines and Delta have signaled in recent weeks that travelers are back in force, especially internationally , but the airline industry has faced hurdles from summer storms and personnel shortages.\r\n",
      "Earnings from payment companies Visa and Mastercard on Tuesday and Thursday, respectively, will point to whether Americans are still spending big on credit cards. Americans are struggling to pay back credit-card debt , a problem pressured by the resumption of federal student loan payments. Earlier this month, big-bank earnings showed that consumers were largely resilient , with customers raising their credit-card spending and loan defaults remaining historically low.\r\n",
      "Procter & Gamble and Colgate-Palmolive will shed light on consumers' willingness to foot the bill for more expensive name-brand products, with reports on Friday. Consumer-goods prices have risen sharply over the last year, lifting top-lines for suppliers, but consumers have cut back on how many products they buy or traded down to cheaper alternatives to cope.\r\n",
      "Consumers' willingness to spend on discretionary purchases—such as going out to eat—will be evident when Chipotle Mexican Grill and McDonald's report on Wednesday and Thursday, respectively. An indicator of big-ticket purchases will come when automakers General Motors and Ford report on Tuesday and Thursday.\r\n",
      "Mondelez and Hershey on Thursday will show how snacking demand is faring, while Coca-Cola on Wednesday will tell investors the latest on beverage sales. PepsiCo earlier this month raised its outlook for organic sales on the year as its snack and beverage divisions continued to increase their revenue. \r\n",
      "Exxon Mobil is expected to detail solid profits when it issues its newest earnings report. PHOTO: MERIDITH KOHUT for The Wall Street Journal\r\n",
      "Quarterly earnings from rail carriers Union Pacific and Old Dominion Freight on Wednesday will clarify the extent of labor disruptions in the shipping sector. Various strikes have caused delays for shippers in the past few months, and Union Pacific said in June that a new staffing agreement would result in a $70 million charge.\r\n",
      "Exxon and Chevron on Friday will report their results. Analysts expect the companies to deliver solid profits, although well off their record levels from a year ago. General Electric, in the midst of spinning off parts of its business, is set to report Tuesday after announcing it would change its chief financial officer .\r\n",
      "Write to Ben Glickman at ben.glickman@wsj.com\n"
     ]
    }
   ],
   "source": [
    "# Helper function to get Gemini query function inputs\n",
    "def get_gemini_inputs(text_df, unique_id):\n",
    "    \"\"\"\n",
    "    Retrieves information from the DataFrame based on the unique ID and outputs company, source, headline, and text.\n",
    "\n",
    "    Args:\n",
    "        text_df (pd.DataFrame): The DataFrame containing the text data.\n",
    "        unique_id (str): The unique ID to search for.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing company, source, headline, and text.\n",
    "    \"\"\"\n",
    "    # Find the row with the specified unique ID\n",
    "    row = text_df[text_df['Unique_ID'] == unique_id]\n",
    "\n",
    "    # Extract the required information\n",
    "    company = row['Ticker'].values[0]\n",
    "    source = row['Source'].values[0]\n",
    "    headline = row['Article Headline'].values[0]\n",
    "    text = row['Article Text'].values[0]\n",
    "\n",
    "    return company, source, headline, text\n",
    "\n",
    "# Test function\n",
    "company, source, headline, text = get_gemini_inputs(text_df, unique_id)\n",
    "print(f\"Company: {company}\\n\")\n",
    "print(f\"Source: {source}\\n\")\n",
    "print(f\"Headline: {headline}\\n\")\n",
    "print(f\"Text:\\n{text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "id": "eRjhvk_9Xapi",
    "outputId": "2fe3961b-a3bb-4e4a-9255-f5b040302e4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article focuses on various companies reporting their quarterly earnings, providing insights into the overall economic health and consumer spending patterns. ExxonMobil is mentioned briefly as an energy heavyweight expected to deliver solid profits, although lower than the record levels from the previous year.\n",
      "\n",
      "Here's the sentiment analysis across the predefined categories:\n",
      "\n",
      "- Finance - Positive (The article mentions ExxonMobil is expected to deliver solid profits, suggesting a positive financial outlook.)\n",
      "- Production - Neutral (The article doesn't provide specific information regarding ExxonMobil's production activities.)\n",
      "- Reserves / Exploration / Acquisitions / Mergers / Divestments - Neutral (The article doesn't discuss any of these aspects related to ExxonMobil.)\n",
      "- Environment / Regulatory / Geopolitics - Neutral (The article doesn't mention any environmental, regulatory, or geopolitical issues related to ExxonMobil.)\n",
      "- Alternative Energy / Lower Carbon - Neutral (The article doesn't discuss ExxonMobil's investments or strategies related to alternative energy or lower carbon initiatives.)\n",
      "- Oil Price / Natural Gas Price / Gasoline Price - Neutral (The article doesn't mention any specific information regarding oil, natural gas, or gasoline prices in relation to ExxonMobil.)\n",
      "\n",
      "- Finance - Positive\n",
      "- Production - Neutral\n",
      "- Reserves / Exploration / Acquisitions / Mergers / Divestments - Neutral\n",
      "- Environment / Regulatory / Geopolitics - Neutral\n",
      "- Alternative Energy / Lower Carbon - Neutral\n",
      "- Oil Price / Natural Gas Price / Gasoline Price - Neutral \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to query Gemini\n",
    "def query_gemini(company, source, headline, text, model):\n",
    "    \"\"\"\n",
    "    Query Gemini to perform sentiment analysis on text from various sources about a company.\n",
    "\n",
    "    Parameters:\n",
    "    company (str): The name of the company the text is about.\n",
    "    source (str): The source of the text. Valid values are \"Investment Research\", \"ProQuest\", \"SEC Filings\", \"Earnings Call Presentations\", \"Earnings Call Q&A\".\n",
    "    headline (str): The headline or title of the text.\n",
    "    text (str): The body of the text to be analyzed.\n",
    "    model: The model object used to generate content and analyze the text.\n",
    "\n",
    "    Returns:\n",
    "    str: The sentiment analysis results for predefined categories in the specified format.\n",
    "\n",
    "    The function constructs a prompt based on the source of the text and performs sentiment analysis on the given text using the provided model.\n",
    "    It analyzes the content across multiple predefined categories, determining the sentiment (Positive, Neutral, Negative) for each category.\n",
    "    If a category is not mentioned or relevant based on the text content, it is marked as 'Neutral'.\n",
    "    The final output is summarized in a specified format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Source Variables\n",
    "    if source == \"Investment Research\":\n",
    "        text_source = \"an analyst report\"\n",
    "        text_source2 = \"the analyst report\"\n",
    "    elif source == \"ProQuest\":\n",
    "        text_source = \"a news article\"\n",
    "        text_source2 = \"the news article\"\n",
    "    elif source == \"SEC Filings\":\n",
    "        text_source = \"an SEC filing\"\n",
    "        text_source2 = \"the SEC filing\"\n",
    "    elif source == \"Earnings Call Presentations\":\n",
    "        text_source = \"an earnings call presentation\"\n",
    "        text_source2 = \"the earnings call presentation\"\n",
    "    elif source == \"Earnings Call Q&A\":\n",
    "        text_source = \"an earnings call Q&A session\"\n",
    "        text_source2 = \"the earnings call Q&A session\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt = f\"\"\"\n",
    "Given the text from {text_source} about {company}, analyze the content and perform sentiment analysis across multiple predefined categories.\n",
    "\n",
    "Sentiment options:\n",
    "  - Positive\n",
    "  - Neutral\n",
    "  - Negative\n",
    "\n",
    "Categories:\n",
    "  - Finance\n",
    "  - Production\n",
    "  - Reserves / Exploration / Acquisitions / Mergers / Divestments\n",
    "  - Environment / Regulatory / Geopolitics\n",
    "  - Alternative Energy / Lower Carbon\n",
    "  - Oil Price / Natural Gas Price / Gasoline Price\n",
    "\n",
    "Each category should be evaluated and given a sentiment output derived from the text.\n",
    "If a category is not mentioned or relevant based on the text content, mark it as 'Neutral'.\n",
    "\n",
    "Before giving your answer, explain your reasoning and reference the article.\n",
    "After going through all the categories, provide a summary in the following format:\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "\n",
    "Example Output:\n",
    "- Finance - Positive\n",
    "- Production - Neutral\n",
    "- Reserves / Exploration / Acquisitions / Mergers / Divestments - Negative\n",
    "- Environment / Regulatory / Geopolitics - Neutral\n",
    "- Alternative Energy / Lower Carbon - Positive\n",
    "- Oil Price / Natural Gas Price / Gasoline Price - Neutral\n",
    "\n",
    "Make sure to use plain text, do not bold or bullet the output summary.\n",
    "\n",
    "The text from {text_source2} is below:\n",
    "{headline}\n",
    "{text}\n",
    "\n",
    "Remember to summarize your final answers in the following format exactly:\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "- Category - Sentiment\n",
    "\n",
    "Make sure to use plain text and stick to the given categories and sentiment options.\n",
    "DO NOT bold or bullet the output summary.\n",
    "    \"\"\"\n",
    "\n",
    "    # print(prompt)\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Test function\n",
    "response = query_gemini(company, source, headline, text, model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DbTgA_znnYIX",
    "outputId": "b1ef5cb2-60e2-47ac-abfc-5b66e194eee1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Finance': 'Positive', 'Production': 'Neutral', 'Reserves / Exploration / Acquisitions / Mergers / Divestments': 'Neutral', 'Environment / Regulatory / Geopolitics': 'Neutral', 'Alternative Energy / Lower Carbon': 'Neutral', 'Oil Price / Natural Gas Price / Gasoline Price': 'Neutral'}\n",
      "Did not find all categories\n"
     ]
    }
   ],
   "source": [
    "# Function to parse text\n",
    "def parse_sentiment(text, categories):\n",
    "    \"\"\"\n",
    "    Parses a given text for specified categories and their sentiments.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing categories and their sentiments.\n",
    "        categories (list of str): List of category names to look for in the text.\n",
    "\n",
    "    Returns:\n",
    "        dict or str: A dictionary with categories as keys and their corresponding sentiments as values,\n",
    "                     or \"Did not find all categories\" if any sentiment is not Positive, Neutral, or Negative.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    valid_sentiments = {\"Positive\", \"Neutral\", \"Negative\"}\n",
    "\n",
    "    for category in categories:\n",
    "        # Create a regex pattern to match the format \"- Category - Sentiment\"\n",
    "        # The category name is escaped to handle any special characters\n",
    "        pattern = rf\"- {re.escape(category)} - (\\w+)\"\n",
    "\n",
    "        # Search for the pattern in the text\n",
    "        match = re.search(pattern, text)\n",
    "\n",
    "        # If a match is found, extract the sentiment and add it to the results dictionary\n",
    "        if match:\n",
    "            sentiment = match.group(1)\n",
    "            if sentiment not in valid_sentiments:\n",
    "                return \"Did not find all categories\"\n",
    "            results[category] = sentiment\n",
    "        else:\n",
    "            return \"Did not find all categories\"\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test function\n",
    "fail_text = \"\"\"\n",
    "- Finance - Positive\n",
    "- Production - Neutral\n",
    "- Reserves / Exploration / Acquisitions / Mergers / Divestments - Bad\n",
    "- Environment / Regulatory / Geopolitics - Neutral\n",
    "- Alternative Energy / Lower Carbon - Positive\n",
    "- Oil Price / Natural Gas Price / Gasoline Price - Neutral\n",
    "\"\"\"\n",
    "\n",
    "sentiment_dict = parse_sentiment(response, CATEGORIES)\n",
    "fail_sentiment = parse_sentiment(\"fail_text\", CATEGORIES)\n",
    "print(sentiment_dict)\n",
    "print(fail_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rfIujTIrXj18",
    "outputId": "c6713621-57ff-4b11-9e19-ccd6675e55a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row with Unique_ID 'PQ-2840643340' has been updated.\n"
     ]
    }
   ],
   "source": [
    "# Function to update the csv\n",
    "def update_csv(file_path, unique_id, sentiment_dict):\n",
    "    \"\"\"\n",
    "    Updates the columns of a CSV file based on the unique ID and sentiment dictionary.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the CSV file.\n",
    "        unique_id (str): The unique ID of the row to be updated.\n",
    "        sentiment_dict (dict): A dictionary with categories as keys and their corresponding sentiments as values.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Find the index of the row with the specified unique ID\n",
    "    row_index = df[df['Unique_ID'] == unique_id].index\n",
    "\n",
    "    # Update the columns based on the sentiment dictionary\n",
    "    for category, sentiment in sentiment_dict.items():\n",
    "        df.loc[row_index, category] = sentiment\n",
    "\n",
    "    # Save the updated DataFrame back to the CSV file\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"Row with Unique_ID '{unique_id}' has been updated.\")\n",
    "\n",
    "# Test function\n",
    "update_csv(SENTIMENT_RESULTS_FILE_PATH, unique_id, sentiment_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ANJgrUqEr1tE",
    "outputId": "3c81cf36-fad8-4a43-d935-8395884731e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row with Unique_ID 'PQ-2839716013' has been updated.\n",
      "Row with Unique_ID 'PQ-2849393419' has been updated.\n",
      "Row with Unique_ID 'PQ-2837179244' has been updated.\n",
      "Row with Unique_ID 'PQ-2836987362' has been updated.\n",
      "Row with Unique_ID 'PQ-2836633384' has been updated.\n",
      "Row with Unique_ID 'PQ-2836280272' has been updated.\n",
      "Row with Unique_ID 'PQ-2836280268' has been updated.\n",
      "Row with Unique_ID 'PQ-2833537536' has been updated.\n",
      "Row with Unique_ID 'PQ-2833537533' has been updated.\n",
      "Row with Unique_ID 'PQ-2832301073' has been updated.\n",
      "Iteration: 10, Elapsed Time: 1 minutes and 52.15 seconds\n",
      "Row with Unique_ID 'PQ-2831167818' has been updated.\n",
      "Row with Unique_ID 'PQ-2830511882' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2829984660' has been updated.\n",
      "Row with Unique_ID 'PQ-2825668794' has been updated.\n",
      "Row with Unique_ID 'PQ-2823402122' has been updated.\n",
      "Row with Unique_ID 'PQ-2822700172' has been updated.\n",
      "Row with Unique_ID 'PQ-2822407748' has been updated.\n",
      "Row with Unique_ID 'PQ-2821940063' has been updated.\n",
      "Row with Unique_ID 'PQ-2821802467' has been updated.\n",
      "Row with Unique_ID 'PQ-2821691077' has been updated.\n",
      "Iteration: 20, Elapsed Time: 3 minutes and 57.31 seconds\n",
      "Row with Unique_ID 'PQ-2821572552' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2821394129' has been updated.\n",
      "Row with Unique_ID 'PQ-2821394035' has been updated.\n",
      "Row with Unique_ID 'PQ-2821268389' has been updated.\n",
      "Row with Unique_ID 'PQ-2821233463' has been updated.\n",
      "Row with Unique_ID 'PQ-2821070941' has been updated.\n",
      "Row with Unique_ID 'PQ-2829326286' has been updated.\n",
      "Row with Unique_ID 'PQ-2829323324' has been updated.\n",
      "Row with Unique_ID 'PQ-2819504464' has been updated.\n",
      "Row with Unique_ID 'PQ-2817796154' has been updated.\n",
      "Iteration: 30, Elapsed Time: 5 minutes and 54.10 seconds\n",
      "Row with Unique_ID 'PQ-2816770023' has been updated.\n",
      "Row with Unique_ID 'PQ-2816702643' has been updated.\n",
      "Row with Unique_ID 'PQ-2816293368' has been updated.\n",
      "Row with Unique_ID 'PQ-2813811880' has been updated.\n",
      "Row with Unique_ID 'PQ-2813581079' has been updated.\n",
      "Row with Unique_ID 'PQ-2813488272' has been updated.\n",
      "Row with Unique_ID 'PQ-2813235293' has been updated.\n",
      "Row with Unique_ID 'PQ-2811145375' has been updated.\n",
      "Row with Unique_ID 'PQ-2811145335' has been updated.\n",
      "Row with Unique_ID 'PQ-2810819955' has been updated.\n",
      "Iteration: 40, Elapsed Time: 7 minutes and 36.14 seconds\n",
      "Row with Unique_ID 'PQ-2810819945' has been updated.\n",
      "Row with Unique_ID 'PQ-2809213358' has been updated.\n",
      "Row with Unique_ID 'PQ-2807827845' has been updated.\n",
      "Row with Unique_ID 'PQ-2819914123' has been updated.\n",
      "Row with Unique_ID 'PQ-2807402301' has been updated.\n",
      "Row with Unique_ID 'PQ-2807290202' has been updated.\n",
      "Row with Unique_ID 'PQ-2807290186' has been updated.\n",
      "Row with Unique_ID 'PQ-2807011780' has been updated.\n",
      "Row with Unique_ID 'PQ-2806923369' has been updated.\n",
      "Row with Unique_ID 'PQ-2806856847' has been updated.\n",
      "Iteration: 50, Elapsed Time: 9 minutes and 31.43 seconds\n",
      "Row with Unique_ID 'PQ-2805467773' has been updated.\n",
      "Row with Unique_ID 'PQ-2805431767' has been updated.\n",
      "Row with Unique_ID 'PQ-2805358632' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2805145239' has been updated.\n",
      "Row with Unique_ID 'PQ-2804807522' has been updated.\n",
      "Row with Unique_ID 'PQ-2804697207' has been updated.\n",
      "Row with Unique_ID 'PQ-2804590579' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2804573695' has been updated.\n",
      "Row with Unique_ID 'PQ-2802216153' has been updated.\n",
      "Row with Unique_ID 'PQ-2813082720' has been updated.\n",
      "Iteration: 60, Elapsed Time: 11 minutes and 22.60 seconds\n",
      "Row with Unique_ID 'PQ-2800483236' has been updated.\n",
      "Row with Unique_ID 'PQ-2800292502' has been updated.\n",
      "Row with Unique_ID 'PQ-2800229004' has been updated.\n",
      "Row with Unique_ID 'PQ-2799367512' has been updated.\n",
      "Row with Unique_ID 'PQ-2799227519' has been updated.\n",
      "Row with Unique_ID 'PQ-2798959112' has been updated.\n",
      "Row with Unique_ID 'PQ-2798922482' has been updated.\n",
      "Row with Unique_ID 'PQ-2797596333' has been updated.\n",
      "Row with Unique_ID 'PQ-2797505155' has been updated.\n",
      "Row with Unique_ID 'PQ-2797306977' has been updated.\n",
      "Iteration: 70, Elapsed Time: 13 minutes and 0.88 seconds\n",
      "Row with Unique_ID 'PQ-2797161593' has been updated.\n",
      "Row with Unique_ID 'PQ-2795935353' has been updated.\n",
      "Row with Unique_ID 'PQ-2795654651' has been updated.\n",
      "Row with Unique_ID 'PQ-2795229025' has been updated.\n",
      "Row with Unique_ID 'PQ-2795203424' has been updated.\n",
      "Row with Unique_ID 'PQ-2792295849' has been updated.\n",
      "Row with Unique_ID 'PQ-2789211401' has been updated.\n",
      "Row with Unique_ID 'PQ-2786942127' has been updated.\n",
      "Row with Unique_ID 'PQ-2786023227' has been updated.\n",
      "Row with Unique_ID 'PQ-2783561898' has been updated.\n",
      "Iteration: 80, Elapsed Time: 14 minutes and 39.28 seconds\n",
      "Row with Unique_ID 'PQ-2783284096' has been updated.\n",
      "Row with Unique_ID 'PQ-2782861221' has been updated.\n",
      "Row with Unique_ID 'PQ-2780270335' has been updated.\n",
      "Row with Unique_ID 'PQ-2778771427' has been updated.\n",
      "Row with Unique_ID 'PQ-2778032390' has been updated.\n",
      "Row with Unique_ID 'PQ-2776447033' has been updated.\n",
      "Row with Unique_ID 'PQ-2776060278' has been updated.\n",
      "Row with Unique_ID 'PQ-2775607864' has been updated.\n",
      "Row with Unique_ID 'PQ-2775365971' has been updated.\n",
      "Row with Unique_ID 'PQ-2774807398' has been updated.\n",
      "Iteration: 90, Elapsed Time: 16 minutes and 15.96 seconds\n",
      "Row with Unique_ID 'PQ-2774759025' has been updated.\n",
      "Row with Unique_ID 'PQ-2774630976' has been updated.\n",
      "Row with Unique_ID 'PQ-2774552858' has been updated.\n",
      "Row with Unique_ID 'PQ-2774052488' has been updated.\n",
      "Row with Unique_ID 'PQ-2773734099' has been updated.\n",
      "Row with Unique_ID 'PQ-2772193026' has been updated.\n",
      "Row with Unique_ID 'PQ-2772165273' has been updated.\n",
      "Row with Unique_ID 'PQ-2771642663' has been updated.\n",
      "Row with Unique_ID 'PQ-2771527492' has been updated.\n",
      "Row with Unique_ID 'PQ-2771430953' has been updated.\n",
      "Iteration: 100, Elapsed Time: 17 minutes and 58.80 seconds\n",
      "Row with Unique_ID 'PQ-2781402434' has been updated.\n",
      "Row with Unique_ID 'PQ-2771237400' has been updated.\n",
      "Row with Unique_ID 'PQ-2771237365' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2771079076' has been updated.\n",
      "Row with Unique_ID 'PQ-2770992722' has been updated.\n",
      "Row with Unique_ID 'PQ-2770936261' has been updated.\n",
      "Row with Unique_ID 'PQ-2770775585' has been updated.\n",
      "Row with Unique_ID 'PQ-2770645068' has been updated.\n",
      "Row with Unique_ID 'PQ-2770475524' has been updated.\n",
      "Row with Unique_ID 'PQ-2768825153' has been updated.\n",
      "Iteration: 110, Elapsed Time: 19 minutes and 43.77 seconds\n",
      "Row with Unique_ID 'PQ-2768713957' has been updated.\n",
      "Row with Unique_ID 'PQ-2768609000' has been updated.\n",
      "Row with Unique_ID 'PQ-2765791749' has been updated.\n",
      "Row with Unique_ID 'PQ-2765112507' has been updated.\n",
      "Row with Unique_ID 'PQ-2765180132' has been updated.\n",
      "Row with Unique_ID 'PQ-2764532627' has been updated.\n",
      "Row with Unique_ID 'PQ-2764016600' has been updated.\n",
      "Row with Unique_ID 'PQ-2762994721' has been updated.\n",
      "Row with Unique_ID 'PQ-2761401021' has been updated.\n",
      "Row with Unique_ID 'PQ-2760658015' has been updated.\n",
      "Iteration: 120, Elapsed Time: 21 minutes and 25.44 seconds\n",
      "Row with Unique_ID 'PQ-2760426793' has been updated.\n",
      "Row with Unique_ID 'PQ-2771487742' has been updated.\n",
      "Row with Unique_ID 'PQ-2760153151' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2759940391' has been updated.\n",
      "Row with Unique_ID 'PQ-2759688645' has been updated.\n",
      "Row with Unique_ID 'PQ-2759014958' has been updated.\n",
      "Row with Unique_ID 'PQ-2770177139' has been updated.\n",
      "Row with Unique_ID 'PQ-2758949216' has been updated.\n",
      "Row with Unique_ID 'PQ-2758693578' has been updated.\n",
      "Row with Unique_ID 'PQ-2767355732' has been updated.\n",
      "Iteration: 130, Elapsed Time: 23 minutes and 20.67 seconds\n",
      "Row with Unique_ID 'PQ-2756541137' has been updated.\n",
      "Row with Unique_ID 'PQ-2756219044' has been updated.\n",
      "Row with Unique_ID 'PQ-2755906144' has been updated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row with Unique_ID 'PQ-2755778203' has been updated.\n",
      "Row with Unique_ID 'PQ-2755623669' has been updated.\n",
      "Row with Unique_ID 'PQ-2754348506' has been updated.\n",
      "Row with Unique_ID 'PQ-2749593707' has been updated.\n",
      "Row with Unique_ID 'PQ-2748975713' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2761605208' has been updated.\n",
      "Row with Unique_ID 'PQ-2761604191' has been updated.\n",
      "Iteration: 140, Elapsed Time: 25 minutes and 11.65 seconds\n",
      "Row with Unique_ID 'PQ-2748550915' has been updated.\n",
      "Row with Unique_ID 'PQ-2748108991' has been updated.\n",
      "Row with Unique_ID 'PQ-2747998016' has been updated.\n",
      "Row with Unique_ID 'PQ-2747921559' has been updated.\n",
      "Row with Unique_ID 'PQ-2747892302' has been updated.\n",
      "Row with Unique_ID 'PQ-2740397294' has been updated.\n",
      "Row with Unique_ID 'PQ-2740248802' has been updated.\n",
      "Row with Unique_ID 'PQ-2739956339' has been updated.\n",
      "Row with Unique_ID 'PQ-2738233243' has been updated.\n",
      "Row with Unique_ID 'PQ-2736343736' has been updated.\n",
      "Iteration: 150, Elapsed Time: 27 minutes and 0.40 seconds\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2736486385' has been updated.\n",
      "Row with Unique_ID 'PQ-2734363757' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2733340210' has been updated.\n",
      "Row with Unique_ID 'PQ-2732964779' has been updated.\n",
      "Row with Unique_ID 'PQ-2731952884' has been updated.\n",
      "Row with Unique_ID 'PQ-2731867378' has been updated.\n",
      "Row with Unique_ID 'PQ-2731683130' has been updated.\n",
      "Row with Unique_ID 'PQ-2730643322' has been updated.\n",
      "Row with Unique_ID 'PQ-2730387602' has been updated.\n",
      "Row with Unique_ID 'PQ-2730299995' has been updated.\n",
      "Iteration: 160, Elapsed Time: 28 minutes and 54.88 seconds\n",
      "Row with Unique_ID 'PQ-2740366125' has been updated.\n",
      "Row with Unique_ID 'PQ-2740365152' has been updated.\n",
      "Row with Unique_ID 'PQ-2729870703' has been updated.\n",
      "Row with Unique_ID 'PQ-2729575893' has been updated.\n",
      "Row with Unique_ID 'PQ-2729575890' has been updated.\n",
      "Row with Unique_ID 'PQ-2729474251' has been updated.\n",
      "Row with Unique_ID 'PQ-2729462797' has been updated.\n",
      "Row with Unique_ID 'PQ-2727632203' has been updated.\n",
      "Row with Unique_ID 'PQ-2727611952' has been updated.\n",
      "Row with Unique_ID 'PQ-2727536681' has been updated.\n",
      "Iteration: 170, Elapsed Time: 30 minutes and 36.66 seconds\n",
      "Row with Unique_ID 'PQ-2727444230' has been updated.\n",
      "Row with Unique_ID 'PQ-2727216922' has been updated.\n",
      "Row with Unique_ID 'PQ-2726789296' has been updated.\n",
      "Row with Unique_ID 'PQ-2726576010' has been updated.\n",
      "Row with Unique_ID 'PQ-2725637421' has been updated.\n",
      "Row with Unique_ID 'PQ-2725594954' has been updated.\n",
      "Row with Unique_ID 'PQ-2730242387' has been updated.\n",
      "Row with Unique_ID 'PQ-2725408178' has been updated.\n",
      "Row with Unique_ID 'PQ-2725367731' has been updated.\n",
      "Row with Unique_ID 'PQ-2725266341' has been updated.\n",
      "Iteration: 180, Elapsed Time: 32 minutes and 14.97 seconds\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2724932876' has been updated.\n",
      "Row with Unique_ID 'PQ-2723894585' has been updated.\n",
      "Row with Unique_ID 'PQ-2723817322' has been updated.\n",
      "Row with Unique_ID 'PQ-2723799904' has been updated.\n",
      "Row with Unique_ID 'PQ-2723196043' has been updated.\n",
      "Row with Unique_ID 'PQ-2722889446' has been updated.\n",
      "Row with Unique_ID 'PQ-2722637419' has been updated.\n",
      "Row with Unique_ID 'PQ-2722584954' has been updated.\n",
      "Row with Unique_ID 'PQ-2722561907' has been updated.\n",
      "Row with Unique_ID 'PQ-2723672546' has been updated.\n",
      "Iteration: 190, Elapsed Time: 34 minutes and 1.08 seconds\n",
      "Row with Unique_ID 'PQ-2721146911' has been updated.\n",
      "Row with Unique_ID 'PQ-2719621436' has been updated.\n",
      "Row with Unique_ID 'PQ-2719578240' has been updated.\n",
      "Row with Unique_ID 'PQ-2719406620' has been updated.\n",
      "Row with Unique_ID 'PQ-2716336116' has been updated.\n",
      "Row with Unique_ID 'PQ-2715810208' has been updated.\n",
      "Row with Unique_ID 'PQ-2711146716' has been updated.\n",
      "Row with Unique_ID 'PQ-2709542294' has been updated.\n",
      "Row with Unique_ID 'PQ-2719231527' has been updated.\n",
      "Row with Unique_ID 'PQ-2708146379' has been updated.\n",
      "Iteration: 200, Elapsed Time: 35 minutes and 43.31 seconds\n",
      "Row with Unique_ID 'PQ-2707865985' has been updated.\n",
      "Row with Unique_ID 'PQ-2707818035' has been updated.\n",
      "Row with Unique_ID 'PQ-2706189226' has been updated.\n",
      "Row with Unique_ID 'PQ-2703522103' has been updated.\n",
      "Row with Unique_ID 'PQ-2703271455' has been updated.\n",
      "Row with Unique_ID 'PQ-2703226662' has been updated.\n",
      "Row with Unique_ID 'PQ-2703181557' has been updated.\n",
      "Row with Unique_ID 'PQ-2703181254' has been updated.\n",
      "Row with Unique_ID 'PQ-2703158337' has been updated.\n",
      "Row with Unique_ID 'PQ-2703040579' has been updated.\n",
      "Iteration: 210, Elapsed Time: 37 minutes and 17.11 seconds\n",
      "Row with Unique_ID 'PQ-2701399485' has been updated.\n",
      "Row with Unique_ID 'PQ-2712349386' has been updated.\n",
      "Row with Unique_ID 'PQ-2701033262' has been updated.\n",
      "Row with Unique_ID 'PQ-2700782090' has been updated.\n",
      "Row with Unique_ID 'PQ-2711090748' has been updated.\n",
      "Row with Unique_ID 'PQ-2700003285' has been updated.\n",
      "Row with Unique_ID 'PQ-2699774741' has been updated.\n",
      "Row with Unique_ID 'PQ-2699719937' has been updated.\n",
      "Row with Unique_ID 'PQ-2698717018' has been updated.\n",
      "Row with Unique_ID 'PQ-2697294455' has been updated.\n",
      "Iteration: 220, Elapsed Time: 38 minutes and 55.57 seconds\n",
      "Row with Unique_ID 'PQ-2697255029' has been updated.\n",
      "Row with Unique_ID 'PQ-2697103825' has been updated.\n",
      "Row with Unique_ID 'PQ-2697091129' has been updated.\n",
      "Row with Unique_ID 'PQ-2697059958' has been updated.\n",
      "Row with Unique_ID 'PQ-2697036334' has been updated.\n",
      "Row with Unique_ID 'PQ-2696857272' has been updated.\n",
      "Row with Unique_ID 'PQ-2696787260' has been updated.\n",
      "Row with Unique_ID 'PQ-2707469032' has been updated.\n",
      "Row with Unique_ID 'PQ-2707468939' has been updated.\n",
      "Row with Unique_ID 'PQ-2696495451' has been updated.\n",
      "Iteration: 230, Elapsed Time: 40 minutes and 36.04 seconds\n",
      "Row with Unique_ID 'PQ-2696126791' has been updated.\n",
      "Row with Unique_ID 'PQ-2695846408' has been updated.\n",
      "Row with Unique_ID 'PQ-2695783603' has been updated.\n",
      "Row with Unique_ID 'PQ-2695438006' has been updated.\n",
      "Row with Unique_ID 'PQ-2695222768' has been updated.\n",
      "Row with Unique_ID 'PQ-2693759111' has been updated.\n",
      "Row with Unique_ID 'PQ-2693725264' has been updated.\n",
      "Row with Unique_ID 'PQ-2693443737' has been updated.\n",
      "Row with Unique_ID 'PQ-2693025355' has been updated.\n",
      "Row with Unique_ID 'PQ-2689159657' has been updated.\n",
      "Iteration: 240, Elapsed Time: 42 minutes and 8.50 seconds\n",
      "Row with Unique_ID 'PQ-2689078191' has been updated.\n",
      "Row with Unique_ID 'PQ-2688944104' has been updated.\n",
      "Row with Unique_ID 'PQ-2688305212' has been updated.\n",
      "Row with Unique_ID 'PQ-2686426651' has been updated.\n",
      "Row with Unique_ID 'PQ-2685341860' has been updated.\n",
      "Row with Unique_ID 'PQ-2684573725' has been updated.\n",
      "Row with Unique_ID 'PQ-2684470823' has been updated.\n",
      "Row with Unique_ID 'PQ-2683933392' has been updated.\n",
      "Row with Unique_ID 'PQ-2683033095' has been updated.\n",
      "Row with Unique_ID 'PQ-2682933954' has been updated.\n",
      "Iteration: 250, Elapsed Time: 44 minutes and 2.36 seconds\n",
      "Row with Unique_ID 'PQ-2682090660' has been updated.\n",
      "Row with Unique_ID 'PQ-2681960602' has been updated.\n",
      "Row with Unique_ID 'PQ-2681833385' has been updated.\n",
      "Row with Unique_ID 'PQ-2681513314' has been updated.\n",
      "Row with Unique_ID 'PQ-2681050307' has been updated.\n",
      "Row with Unique_ID 'PQ-2679800058' has been updated.\n",
      "Row with Unique_ID 'PQ-2679650839' has been updated.\n",
      "Row with Unique_ID 'PQ-2678827581' has been updated.\n",
      "Row with Unique_ID 'PQ-2678826869' has been updated.\n",
      "Row with Unique_ID 'PQ-2678738601' has been updated.\n",
      "Iteration: 260, Elapsed Time: 45 minutes and 46.86 seconds\n",
      "Row with Unique_ID 'PQ-2677047887' has been updated.\n",
      "Row with Unique_ID 'PQ-2676785980' has been updated.\n",
      "Row with Unique_ID 'PQ-2676428715' has been updated.\n",
      "Row with Unique_ID 'PQ-2675402052' has been updated.\n",
      "Row with Unique_ID 'PQ-2675039389' has been updated.\n",
      "Row with Unique_ID 'PQ-2675024496' has been updated.\n",
      "Row with Unique_ID 'PQ-2675024459' has been updated.\n",
      "Row with Unique_ID 'PQ-2674633042' has been updated.\n",
      "Row with Unique_ID 'PQ-2674431573' has been updated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row with Unique_ID 'PQ-2674426098' has been updated.\n",
      "Iteration: 270, Elapsed Time: 47 minutes and 39.34 seconds\n",
      "Row with Unique_ID 'PQ-2674204861' has been updated.\n",
      "Row with Unique_ID 'PQ-2674035264' has been updated.\n",
      "Row with Unique_ID 'PQ-2673613426' has been updated.\n",
      "Row with Unique_ID 'PQ-2672770935' has been updated.\n",
      "Row with Unique_ID 'PQ-2672589258' has been updated.\n",
      "Row with Unique_ID 'PQ-2673735765' has been updated.\n",
      "Row with Unique_ID 'PQ-2672375851' has been updated.\n",
      "Row with Unique_ID 'PQ-2672345624' has been updated.\n",
      "Row with Unique_ID 'PQ-2681634520' has been updated.\n",
      "Row with Unique_ID 'PQ-2670756198' has been updated.\n",
      "Iteration: 280, Elapsed Time: 49 minutes and 29.69 seconds\n",
      "Row with Unique_ID 'PQ-2670487140' has been updated.\n",
      "Row with Unique_ID 'PQ-2670092171' has been updated.\n",
      "Row with Unique_ID 'PQ-2669881917' has been updated.\n",
      "Row with Unique_ID 'PQ-2669497405' has been updated.\n",
      "Row with Unique_ID 'PQ-2680439898' has been updated.\n",
      "Row with Unique_ID 'PQ-2669586752' has been updated.\n",
      "Row with Unique_ID 'PQ-2669340791' has been updated.\n",
      "Row with Unique_ID 'PQ-2669290927' has been updated.\n",
      "Row with Unique_ID 'PQ-2679950831' has been updated.\n",
      "Row with Unique_ID 'PQ-2669144601' has been updated.\n",
      "Iteration: 290, Elapsed Time: 51 minutes and 25.55 seconds\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2668632263' has been updated.\n",
      "Row with Unique_ID 'PQ-2668525240' has been updated.\n",
      "Row with Unique_ID 'PQ-2667606988' has been updated.\n",
      "Row with Unique_ID 'PQ-2667516922' has been updated.\n",
      "Row with Unique_ID 'PQ-2667511619' has been updated.\n",
      "Row with Unique_ID 'PQ-2666781612' has been updated.\n",
      "Row with Unique_ID 'PQ-2664883723' has been updated.\n",
      "Row with Unique_ID 'PQ-2663888676' has been updated.\n",
      "Row with Unique_ID 'PQ-2661569145' has been updated.\n",
      "Row with Unique_ID 'PQ-2661140059' has been updated.\n",
      "Iteration: 300, Elapsed Time: 53 minutes and 20.28 seconds\n",
      "Row with Unique_ID 'PQ-2660865237' has been updated.\n",
      "Row with Unique_ID 'PQ-2657145917' has been updated.\n",
      "Row with Unique_ID 'PQ-2656885317' has been updated.\n",
      "Row with Unique_ID 'PQ-2656808396' has been updated.\n",
      "Row with Unique_ID 'PQ-2656732672' has been updated.\n",
      "Row with Unique_ID 'PQ-2656622328' has been updated.\n",
      "Row with Unique_ID 'PQ-2656303716' has been updated.\n",
      "Row with Unique_ID 'PQ-2655576608' has been updated.\n",
      "Row with Unique_ID 'PQ-2655520383' has been updated.\n",
      "Row with Unique_ID 'PQ-2655389560' has been updated.\n",
      "Iteration: 310, Elapsed Time: 55 minutes and 7.21 seconds\n",
      "Row with Unique_ID 'PQ-2654978907' has been updated.\n",
      "Row with Unique_ID 'PQ-2653834594' has been updated.\n",
      "Row with Unique_ID 'PQ-2653834467' has been updated.\n",
      "Row with Unique_ID 'PQ-2653535693' has been updated.\n",
      "Row with Unique_ID 'PQ-2652883540' has been updated.\n",
      "Row with Unique_ID 'PQ-2647545036' has been updated.\n",
      "Row with Unique_ID 'PQ-2647108708' has been updated.\n",
      "Row with Unique_ID 'PQ-2646866012' has been updated.\n",
      "Row with Unique_ID 'PQ-2646715325' has been updated.\n",
      "Row with Unique_ID 'PQ-2644070723' has been updated.\n",
      "Iteration: 320, Elapsed Time: 57 minutes and 4.41 seconds\n",
      "Row with Unique_ID 'PQ-2643841527' has been updated.\n",
      "Row with Unique_ID 'PQ-2641736904' has been updated.\n",
      "Row with Unique_ID 'PQ-2641705139' has been updated.\n",
      "Row with Unique_ID 'PQ-2640980866' has been updated.\n",
      "Row with Unique_ID 'PQ-2640422215' has been updated.\n",
      "Row with Unique_ID 'PQ-2640211713' has been updated.\n",
      "Row with Unique_ID 'PQ-2640062387' has been updated.\n",
      "Row with Unique_ID 'PQ-2637157878' has been updated.\n",
      "Row with Unique_ID 'PQ-2636796999' has been updated.\n",
      "Row with Unique_ID 'PQ-2636788649' has been updated.\n",
      "Iteration: 330, Elapsed Time: 58 minutes and 58.27 seconds\n",
      "Row with Unique_ID 'PQ-2636155197' has been updated.\n",
      "Row with Unique_ID 'PQ-2635684693' has been updated.\n",
      "Row with Unique_ID 'PQ-2635256694' has been updated.\n",
      "Row with Unique_ID 'PQ-2635314949' has been updated.\n",
      "Row with Unique_ID 'PQ-2635161746' has been updated.\n",
      "Row with Unique_ID 'PQ-2634870869' has been updated.\n",
      "Row with Unique_ID 'PQ-2512376328' has been updated.\n",
      "Row with Unique_ID 'PQ-2464144160' has been updated.\n",
      "Row with Unique_ID 'PQ-2460769247' has been updated.\n",
      "Row with Unique_ID 'PQ-2460088266' has been updated.\n",
      "Iteration: 340, Elapsed Time: 60 minutes and 52.91 seconds\n",
      "Row with Unique_ID 'PQ-2458488855' has been updated.\n",
      "Row with Unique_ID 'PQ-2458407133' has been updated.\n",
      "Row with Unique_ID 'PQ-2467512336' has been updated.\n",
      "Row with Unique_ID 'PQ-2467512046' has been updated.\n",
      "Row with Unique_ID 'PQ-2458216645' has been updated.\n",
      "Row with Unique_ID 'PQ-2457706001' has been updated.\n",
      "Row with Unique_ID 'PQ-2457420881' has been updated.\n",
      "Row with Unique_ID 'PQ-2456396416' has been updated.\n",
      "Row with Unique_ID 'PQ-2455950447' has been updated.\n",
      "Row with Unique_ID 'PQ-2462670688' has been updated.\n",
      "Iteration: 350, Elapsed Time: 62 minutes and 37.58 seconds\n",
      "Row with Unique_ID 'PQ-2465586187' has been updated.\n",
      "Row with Unique_ID 'PQ-2465586065' has been updated.\n",
      "Row with Unique_ID 'PQ-2462488152' has been updated.\n",
      "Row with Unique_ID 'PQ-2456191848' has been updated.\n",
      "Row with Unique_ID 'PQ-2456156613' has been updated.\n",
      "Row with Unique_ID 'PQ-2456134408' has been updated.\n",
      "Row with Unique_ID 'PQ-2456109214' has been updated.\n",
      "Row with Unique_ID 'PQ-2456040178' has been updated.\n",
      "Row with Unique_ID 'PQ-2456039502' has been updated.\n",
      "Row with Unique_ID 'PQ-2456028131' has been updated.\n",
      "Iteration: 360, Elapsed Time: 64 minutes and 25.66 seconds\n",
      "Row with Unique_ID 'PQ-2455949221' has been updated.\n",
      "Row with Unique_ID 'PQ-2455822853' has been updated.\n",
      "Row with Unique_ID 'PQ-2455784674' has been updated.\n",
      "Row with Unique_ID 'PQ-2455687806' has been updated.\n",
      "Row with Unique_ID 'PQ-2464945933' has been updated.\n",
      "Row with Unique_ID 'PQ-2455645121' has been updated.\n",
      "Row with Unique_ID 'PQ-2455552740' has been updated.\n",
      "Row with Unique_ID 'PQ-2455201343' has been updated.\n",
      "Row with Unique_ID 'PQ-2455201332' has been updated.\n",
      "Row with Unique_ID 'PQ-2455169836' has been updated.\n",
      "Iteration: 370, Elapsed Time: 66 minutes and 32.96 seconds\n",
      "Row with Unique_ID 'PQ-2454328119' has been updated.\n",
      "Row with Unique_ID 'PQ-2454091719' has been updated.\n",
      "Row with Unique_ID 'PQ-2454056062' has been updated.\n",
      "Row with Unique_ID 'PQ-2454036495' has been updated.\n",
      "Row with Unique_ID 'PQ-2453915666' has been updated.\n",
      "Row with Unique_ID 'PQ-2452248911' has been updated.\n",
      "Row with Unique_ID 'PQ-2451442571' has been updated.\n",
      "Row with Unique_ID 'PQ-2450647558' has been updated.\n",
      "Row with Unique_ID 'PQ-2450305143' has been updated.\n",
      "Row with Unique_ID 'PQ-2450196544' has been updated.\n",
      "Iteration: 380, Elapsed Time: 68 minutes and 18.37 seconds\n",
      "Row with Unique_ID 'PQ-2450194061' has been updated.\n",
      "Error encountered: 'str' object has no attribute 'items'. Retry 1/5\n",
      "Row with Unique_ID 'PQ-2449967350' has been updated.\n",
      "Row with Unique_ID 'PQ-2449571674' has been updated.\n",
      "Row with Unique_ID 'PQ-2449399881' has been updated.\n",
      "Row with Unique_ID 'PQ-2449154930' has been updated.\n",
      "Row with Unique_ID 'PQ-2449146170' has been updated.\n",
      "Row with Unique_ID 'PQ-2449125988' has been updated.\n",
      "Row with Unique_ID 'PQ-2458223659' has been updated.\n",
      "Row with Unique_ID 'PQ-2449005401' has been updated.\n",
      "Row with Unique_ID 'PQ-2448932892' has been updated.\n",
      "Iteration: 390, Elapsed Time: 70 minutes and 17.84 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Elapsed Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(minutes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseconds\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Find the next empty row\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m unique_id \u001b[38;5;241m=\u001b[39m \u001b[43mfind_first_unique_id_with_empty_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSENTIMENT_RESULTS_FILE_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCATEGORIES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Test print statements\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# print(unique_id)\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# print(f\"Company: {company}\\n\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# print(response)\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# print(sentiment_dict)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m, in \u001b[0;36mfind_first_unique_id_with_empty_values\u001b[1;34m(file_path, categories)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Iterate over each row to find the first unique ID with any empty values in the specified columns\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m (row[categories] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnique_ID\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\core\\series.py:1153\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_rows_with_mask(key)\n\u001b[1;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\core\\series.py:1194\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1191\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[key]\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;66;03m# handle the dup indexing case GH#4246\u001b[39;00m\n\u001b[1;32m-> 1194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\core\\indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\core\\indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1363\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\core\\indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6192\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6190\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m key\n\u001b[0;32m   6191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(keyarr, Index):\n\u001b[1;32m-> 6192\u001b[0m     keyarr \u001b[38;5;241m=\u001b[39m \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray_tuplesafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_as_unique:\n\u001b[0;32m   6195\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer_for(keyarr)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\capstone\\lib\\site-packages\\pandas\\core\\common.py:247\u001b[0m, in \u001b[0;36masarray_tuplesafe\u001b[1;34m(values, dtype)\u001b[0m\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np_version_gte1p24:\n\u001b[0;32m    246\u001b[0m             warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mVisibleDeprecationWarning)\n\u001b[1;32m--> 247\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;66;03m# Using try/except since it's more performant than checking is_list_like\u001b[39;00m\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# over each element\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"construct_1d_object_array_from_listlike\"\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;66;03m# has incompatible type \"Iterable[Any]\"; expected \"Sized\"\u001b[39;00m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m construct_1d_object_array_from_listlike(values)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "unique_id = find_first_unique_id_with_empty_values(SENTIMENT_RESULTS_FILE_PATH, CATEGORIES)\n",
    "count = 0\n",
    "max_tries = 5\n",
    "\n",
    "# Iterate through the CSV using the functions\n",
    "while unique_id:\n",
    "    retries = 0\n",
    "    success = False\n",
    "\n",
    "    # There are multiple errors that can happen, many of them simply need another try\n",
    "    while retries < max_tries and not success:\n",
    "        try:\n",
    "            # Get gemini inputs\n",
    "            company, source, headline, text = get_gemini_inputs(text_df, unique_id)\n",
    "\n",
    "            # Query Gemini\n",
    "            response = query_gemini(company, source, headline, text, model)\n",
    "\n",
    "            # Parse text\n",
    "            sentiment_dict = parse_sentiment(response, CATEGORIES)\n",
    "\n",
    "            # Update the csv\n",
    "            update_csv(SENTIMENT_RESULTS_FILE_PATH, unique_id, sentiment_dict)\n",
    "\n",
    "            success = True\n",
    "\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            print(f\"Error encountered: {e}. Retry {retries}/{max_tries}\")\n",
    "\n",
    "            if retries >= max_tries:\n",
    "                print(f\"Failed to process unique_id {unique_id} after {max_tries} attempts. Stopping.\")\n",
    "                # Exit both loops\n",
    "                unique_id = None\n",
    "                break\n",
    "\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Get an update every 10 rows\n",
    "    count += 1\n",
    "    if count % 10 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        minutes, seconds = divmod(elapsed_time, 60)\n",
    "        print(f\"Iteration: {count}, Elapsed Time: {int(minutes)} minutes and {seconds:.2f} seconds\")\n",
    "\n",
    "    # Find the next empty row\n",
    "    unique_id = find_first_unique_id_with_empty_values(SENTIMENT_RESULTS_FILE_PATH, CATEGORIES)\n",
    "\n",
    "    # Test print statements\n",
    "    # print(unique_id)\n",
    "    # print(f\"Company: {company}\\n\")\n",
    "    # print(f\"Source: {source}\\n\")\n",
    "    # print(f\"Headline: {headline}\\n\")\n",
    "    # print(f\"Text:\\n{text}\")\n",
    "    # print(response)\n",
    "    # print(sentiment_dict)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (capstone)",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
