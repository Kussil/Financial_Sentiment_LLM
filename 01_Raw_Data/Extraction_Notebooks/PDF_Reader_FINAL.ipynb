{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb9f753",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5c44df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import socket\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import xlrd\n",
    "import xlwt\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec6f33",
   "metadata": {},
   "source": [
    "### Load data search file and create DF / .csv for final formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20802e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Contributor              Analyst              Date/Time Company  \\\n",
      "0  CFRA Equity Research  Handshoe, Jonnathan  May 16, 2024 10:10 PM     MRO   \n",
      "1  CFRA Equity Research    Glickman, Stewart  May 14, 2024 10:02 PM     EOG   \n",
      "2  CFRA Equity Research    Glickman, Stewart  May 11, 2024 06:08 PM     EOG   \n",
      "3  CFRA Equity Research    Glickman, Stewart  May 11, 2024 05:57 PM     DVN   \n",
      "4  CFRA Equity Research    Glickman, Stewart  May 07, 2024 10:03 PM     COP   \n",
      "\n",
      "                   Headline  Pages       Date               Headline2  \\\n",
      "0  Marathon Oil Corporation      9 2024-05-16  MarathonOilCorporation   \n",
      "1       EOG Resources, Inc.      9 2024-05-14       EOGResources,Inc.   \n",
      "2       EOG Resources, Inc.      9 2024-05-11       EOGResources,Inc.   \n",
      "3  Devon Energy Corporation      9 2024-05-11  DevonEnergyCorporation   \n",
      "4            ConocoPhillips      9 2024-05-07          ConocoPhillips   \n",
      "\n",
      "   Unnamed: 8 Month  Day  Year                             Join_Key  \n",
      "0         NaN   May   16  2024  MarathonOilCorporation_May_16,_2024  \n",
      "1         NaN   May   14  2024       EOGResources,Inc._May_14,_2024  \n",
      "2         NaN   May   11  2024       EOGResources,Inc._May_11,_2024  \n",
      "3         NaN   May   11  2024  DevonEnergyCorporation_May_11,_2024  \n",
      "4         NaN   May    7  2024           ConocoPhillips_May_7,_2024  \n"
     ]
    }
   ],
   "source": [
    "# Load the combined search output CSV file\n",
    "df = pd.read_csv('Investment Research/Investment_Research_Test1.csv')\n",
    "\n",
    "# Clean the date column to remove any extra characters or spaces\n",
    "df['Date'] = df['Date'].str.strip()\n",
    "\n",
    "# Define a function to parse dates with various formats\n",
    "def parse_date(date_str):\n",
    "    for fmt in ('%d-%b-%y', '%d-%b-%Y', '%d-%B-%y', '%d-%B-%Y'):\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    raise ValueError(f\"Date format not recognized: {date_str}\")\n",
    "\n",
    "# Apply the date parsing function to the Date column\n",
    "df['Date'] = df['Date'].apply(parse_date)\n",
    "\n",
    "# Extract the month, day, and year from the date column\n",
    "df['Month'] = df['Date'].dt.strftime('%b')\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# Create the new Join_Key column\n",
    "df['Join_Key'] = df['Headline2'] + '_' + df['Month'] + '_' + df['Day'].astype(str) + ',_' + df['Year'].astype(str)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "df.to_csv('Investment Research/Investment_Research_Test3_Formatted.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ff00d9",
   "metadata": {},
   "source": [
    "### Extract text from directory with pdf analyst reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "373404a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5 of 4846 PDFs. Elapsed time: 173.08 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_5_DF.csv\n",
      "Processed 10 of 4846 PDFs. Elapsed time: 329.21 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_10_DF.csv\n",
      "Processed 15 of 4846 PDFs. Elapsed time: 502.76 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_15_DF.csv\n",
      "Processed 20 of 4846 PDFs. Elapsed time: 658.33 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_20_DF.csv\n",
      "Processed 25 of 4846 PDFs. Elapsed time: 827.88 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_25_DF.csv\n",
      "Processed 30 of 4846 PDFs. Elapsed time: 974.07 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_30_DF.csv\n",
      "Processed 35 of 4846 PDFs. Elapsed time: 1098.95 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_35_DF.csv\n",
      "Processed 40 of 4846 PDFs. Elapsed time: 1266.56 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_40_DF.csv\n",
      "Processed 45 of 4846 PDFs. Elapsed time: 1415.37 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_45_DF.csv\n",
      "Processed 50 of 4846 PDFs. Elapsed time: 1558.42 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_50_DF.csv\n",
      "Processed 55 of 4846 PDFs. Elapsed time: 1704.36 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_55_DF.csv\n",
      "Processed 60 of 4846 PDFs. Elapsed time: 1854.82 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_60_DF.csv\n",
      "Processed 65 of 4846 PDFs. Elapsed time: 2000.62 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_65_DF.csv\n",
      "Processed 70 of 4846 PDFs. Elapsed time: 2168.39 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_70_DF.csv\n",
      "Processed 75 of 4846 PDFs. Elapsed time: 2339.81 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_75_DF.csv\n",
      "Processed 80 of 4846 PDFs. Elapsed time: 2508.24 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_80_DF.csv\n",
      "Processed 85 of 4846 PDFs. Elapsed time: 2678.46 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_85_DF.csv\n",
      "Processed 90 of 4846 PDFs. Elapsed time: 2850.16 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_90_DF.csv\n",
      "Processed 95 of 4846 PDFs. Elapsed time: 3001.96 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_95_DF.csv\n",
      "Processed 100 of 4846 PDFs. Elapsed time: 3174.26 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_100_DF.csv\n",
      "Processed 105 of 4846 PDFs. Elapsed time: 3324.61 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_105_DF.csv\n",
      "Processed 110 of 4846 PDFs. Elapsed time: 3576.79 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_110_DF.csv\n",
      "Processed 115 of 4846 PDFs. Elapsed time: 3730.49 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_115_DF.csv\n",
      "Processed 120 of 4846 PDFs. Elapsed time: 3871.65 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_120_DF.csv\n",
      "Processed 125 of 4846 PDFs. Elapsed time: 4020.50 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_125_DF.csv\n",
      "Processed 130 of 4846 PDFs. Elapsed time: 4187.72 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_130_DF.csv\n",
      "Processed 135 of 4846 PDFs. Elapsed time: 4353.84 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_135_DF.csv\n",
      "Processed 140 of 4846 PDFs. Elapsed time: 4507.85 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_140_DF.csv\n",
      "Processed 145 of 4846 PDFs. Elapsed time: 4671.91 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_145_DF.csv\n",
      "Processed 150 of 4846 PDFs. Elapsed time: 4817.88 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_150_DF.csv\n",
      "Processed 155 of 4846 PDFs. Elapsed time: 4989.46 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_155_DF.csv\n",
      "Processed 160 of 4846 PDFs. Elapsed time: 5163.78 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_160_DF.csv\n",
      "Processed 165 of 4846 PDFs. Elapsed time: 5337.66 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_165_DF.csv\n",
      "Processed 170 of 4846 PDFs. Elapsed time: 5507.71 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_170_DF.csv\n",
      "Processed 175 of 4846 PDFs. Elapsed time: 5642.34 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_175_DF.csv\n",
      "Processed 180 of 4846 PDFs. Elapsed time: 5789.50 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_180_DF.csv\n",
      "Processed 185 of 4846 PDFs. Elapsed time: 5965.96 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_185_DF.csv\n",
      "Processed 190 of 4846 PDFs. Elapsed time: 6107.32 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_190_DF.csv\n",
      "Processed 195 of 4846 PDFs. Elapsed time: 6242.69 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_195_DF.csv\n",
      "Processed 200 of 4846 PDFs. Elapsed time: 6409.76 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_200_DF.csv\n",
      "Processed 205 of 4846 PDFs. Elapsed time: 6559.61 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_205_DF.csv\n",
      "Processed 210 of 4846 PDFs. Elapsed time: 6687.75 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_210_DF.csv\n",
      "Processed 215 of 4846 PDFs. Elapsed time: 6858.57 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_215_DF.csv\n",
      "Processed 220 of 4846 PDFs. Elapsed time: 7029.32 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_220_DF.csv\n",
      "Processed 225 of 4846 PDFs. Elapsed time: 7174.42 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_225_DF.csv\n",
      "Processed 230 of 4846 PDFs. Elapsed time: 7344.45 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_230_DF.csv\n",
      "Processed 235 of 4846 PDFs. Elapsed time: 7489.16 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_235_DF.csv\n",
      "Processed 240 of 4846 PDFs. Elapsed time: 7660.93 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_240_DF.csv\n",
      "Processed 245 of 4846 PDFs. Elapsed time: 7830.45 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_245_DF.csv\n",
      "Processed 250 of 4846 PDFs. Elapsed time: 7966.45 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_250_DF.csv\n",
      "Processed 255 of 4846 PDFs. Elapsed time: 8121.24 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_255_DF.csv\n",
      "Processed 260 of 4846 PDFs. Elapsed time: 8226.53 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_260_DF.csv\n",
      "Processed 265 of 4846 PDFs. Elapsed time: 8427.50 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_265_DF.csv\n",
      "Processed 270 of 4846 PDFs. Elapsed time: 8554.36 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_270_DF.csv\n",
      "Processed 275 of 4846 PDFs. Elapsed time: 8720.87 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_275_DF.csv\n",
      "Processed 280 of 4846 PDFs. Elapsed time: 8861.89 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_280_DF.csv\n",
      "Processed 285 of 4846 PDFs. Elapsed time: 9031.57 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_285_DF.csv\n",
      "Processed 290 of 4846 PDFs. Elapsed time: 9168.10 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_290_DF.csv\n",
      "Processed 295 of 4846 PDFs. Elapsed time: 9273.33 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_295_DF.csv\n",
      "Processed 300 of 4846 PDFs. Elapsed time: 9419.16 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_300_DF.csv\n",
      "Processed 305 of 4846 PDFs. Elapsed time: 9586.99 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_305_DF.csv\n",
      "Processed 310 of 4846 PDFs. Elapsed time: 9720.79 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_310_DF.csv\n",
      "Processed 315 of 4846 PDFs. Elapsed time: 9852.82 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_315_DF.csv\n",
      "Processed 320 of 4846 PDFs. Elapsed time: 10021.80 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_320_DF.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 325 of 4846 PDFs. Elapsed time: 10169.21 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_325_DF.csv\n",
      "Processed 330 of 4846 PDFs. Elapsed time: 10304.39 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_330_DF.csv\n",
      "Processed 335 of 4846 PDFs. Elapsed time: 10452.55 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_335_DF.csv\n",
      "Processed 340 of 4846 PDFs. Elapsed time: 10625.67 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_340_DF.csv\n",
      "Processed 345 of 4846 PDFs. Elapsed time: 10791.46 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_345_DF.csv\n",
      "Processed 350 of 4846 PDFs. Elapsed time: 10940.34 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_350_DF.csv\n",
      "Processed 355 of 4846 PDFs. Elapsed time: 11089.59 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_355_DF.csv\n",
      "Processed 360 of 4846 PDFs. Elapsed time: 11259.97 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_360_DF.csv\n",
      "Processed 365 of 4846 PDFs. Elapsed time: 11383.22 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_365_DF.csv\n",
      "Processed 370 of 4846 PDFs. Elapsed time: 11548.92 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_370_DF.csv\n",
      "Processed 375 of 4846 PDFs. Elapsed time: 11648.39 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_375_DF.csv\n",
      "Processed 380 of 4846 PDFs. Elapsed time: 11815.81 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_380_DF.csv\n",
      "Processed 385 of 4846 PDFs. Elapsed time: 11968.20 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_385_DF.csv\n",
      "Processed 390 of 4846 PDFs. Elapsed time: 12114.51 seconds.\n",
      "Intermediate data saved to Investment Research/Intermediate_390_DF.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, pdf_file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pdf_files):\n\u001b[1;32m     54\u001b[0m     pdf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pdf_dir, pdf_file)\n\u001b[0;32m---> 55\u001b[0m     text \u001b[38;5;241m=\u001b[39m extract_text_and_title_with_ocr(pdf_path)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip files that couldn't be read\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m, in \u001b[0;36mextract_text_and_title_with_ocr\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_text_and_title_with_ocr\u001b[39m(pdf_path):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# Convert PDF to images\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m         pages \u001b[38;5;241m=\u001b[39m convert_from_path(pdf_path, \u001b[38;5;241m300\u001b[39m)\n\u001b[1;32m      6\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pages:\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;66;03m# Convert each page to text\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pdf2image/pdf2image.py:251\u001b[0m, in \u001b[0;36mconvert_from_path\u001b[0;34m(pdf_path, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, ownerpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo, timeout, hide_annotations)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m uid, proc \u001b[38;5;129;01min\u001b[39;00m processes:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m         data, err \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mcommunicate(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired:\n\u001b[1;32m    253\u001b[0m         proc\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_communicate(\u001b[38;5;28minput\u001b[39m, endtime, timeout)\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/subprocess.py:2128\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2126\u001b[0m             key\u001b[38;5;241m.\u001b[39mfileobj\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m   2127\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr):\n\u001b[0;32m-> 2128\u001b[0m     data \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mread(key\u001b[38;5;241m.\u001b[39mfd, \u001b[38;5;241m32768\u001b[39m)\n\u001b[1;32m   2129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m   2130\u001b[0m         selector\u001b[38;5;241m.\u001b[39munregister(key\u001b[38;5;241m.\u001b[39mfileobj)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to extract text and title with OCR\n",
    "def extract_text_and_title_with_ocr(pdf_path):\n",
    "    try:\n",
    "        # Convert PDF to images\n",
    "        pages = convert_from_path(pdf_path, 300)\n",
    "        text = \"\"\n",
    "        for page in pages:\n",
    "            # Convert each page to text\n",
    "            page_text = pytesseract.image_to_string(page, lang='eng')\n",
    "            text += page_text + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(\"Error reading \" + pdf_path + \": \" + str(e))\n",
    "        return None\n",
    "\n",
    "# Function to extract date from filename\n",
    "def extract_date_from_filename(filename):\n",
    "    date_pattern = r'(\\w{3})_(\\d{1,2}),_(\\d{4})\\.pdf'\n",
    "    match = re.search(date_pattern, filename)\n",
    "    if match:\n",
    "        return match.group(1) + \" \" + match.group(2) + \", \" + match.group(3)\n",
    "    else:\n",
    "        return 'Date not found'\n",
    "\n",
    "# Function to extract document ID\n",
    "def extract_document_id(title):\n",
    "    doc_id_pattern = r'(\\d+)[^_]*'\n",
    "    match = re.search(doc_id_pattern, title)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return 'ID not found'\n",
    "\n",
    "# Function to extract join key from filename\n",
    "def extract_join_key(filename):\n",
    "    join_key_pattern = r'_(.*?)\\.pdf'\n",
    "    match = re.search(join_key_pattern, filename)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return 'Join_Key not found'\n",
    "\n",
    "# Directory containing PDFs\n",
    "pdf_dir = 'Investment Research/PDFs_All_V2'\n",
    "pdf_files = [f for f in os.listdir(pdf_dir) if f.endswith('.pdf')]\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Extract data from each PDF\n",
    "data = []\n",
    "total_pdfs = len(pdf_files)\n",
    "for i, pdf_file in enumerate(pdf_files):\n",
    "    pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "    text = extract_text_and_title_with_ocr(pdf_path)\n",
    "    if text is None:\n",
    "        continue  # Skip files that couldn't be read\n",
    "    title = pdf_file  # Use file name as title\n",
    "    source = pdf_file.split('_')[0]  # Extract source from file name\n",
    "    date = extract_date_from_filename(pdf_file)  # Extract date from filename\n",
    "    unique_id = extract_document_id(title)\n",
    "    join_key = extract_join_key(pdf_file)  # Extract join key from filename\n",
    "    data.append({'Unique_ID': unique_id, 'Join_Key': join_key, 'Date': date, 'Title': title, 'Source': source, 'Text': text})\n",
    "\n",
    "    # Print progress and save intermediate CSV every 5 PDFs\n",
    "    if (i + 1) % 5 == 0 or (i + 1) == total_pdfs:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Processed {i + 1} of {total_pdfs} PDFs. Elapsed time: {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "        # Create a DataFrame for intermediate data\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Ensure Join_Key is a string and clean it\n",
    "        def clean_join_key(key):\n",
    "            key = re.sub(r'[^\\w\\s]', '', key.lower())\n",
    "            key = re.sub(r'_(0)', '_', key)  # Remove '0' following the second underscore\n",
    "            return key\n",
    "\n",
    "        df['Join_Key'] = df['Join_Key'].astype(str).apply(clean_join_key)\n",
    "\n",
    "        # Save intermediate DataFrame to CSV\n",
    "        intermediate_csv_path = f'Investment Research/Intermediate_DF.csv'\n",
    "        df.to_csv(intermediate_csv_path, index=False)\n",
    "        print(f\"Intermediate data saved to {intermediate_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b756218",
   "metadata": {},
   "source": [
    "### Clean and merge with data search file to create final DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9172358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a05029",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure Join_Key is a string and clean it\n",
    "df['Join_Key'] = df['Join_Key'].astype(str).apply(clean_join_key)\n",
    "\n",
    "# Save to CSV for new reference file\n",
    "csv_path = 'Investment Research/Investment_Research_Test3_DF.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "print('Data saved to ' + csv_path)\n",
    "\n",
    "# Load existing CSV from search .csv\n",
    "existing_csv_path = 'Investment Research/Investment_Research_Test3_Formatted.csv'\n",
    "existing_df = pd.read_csv(existing_csv_path)\n",
    "\n",
    "# Ensure Join_Key is a string and clean it in the existing DataFrame\n",
    "existing_df['Join_Key'] = existing_df['Join_Key'].astype(str).apply(clean_join_key)\n",
    "\n",
    "# Merge the 'Text' column from new data to the existing DataFrame based on 'Join_Key'\n",
    "merged_df = existing_df.merge(df[['Join_Key', 'Text']], on='Join_Key', how='left')\n",
    "\n",
    "# Select only the desired columns\n",
    "columns_to_keep = ['Contributor', 'Date/Time', 'Date', 'Company', 'Headline', 'Text']\n",
    "final_df = merged_df[columns_to_keep]\n",
    "\n",
    "# Replace values in the \"Company\" column\n",
    "replacements = {\n",
    "    \"Pioneer Natural Resources Company\": \"PXD\",\n",
    "    \"Concho Resources Inc.\": \"CXO\",\n",
    "    \"BP.\": \"BP\",\n",
    "    \"PDC Energy, Inc.\": \"PDCE\"\n",
    "}\n",
    "\n",
    "final_df['Company'] = final_df['Company'].replace(replacements)\n",
    "\n",
    "# Rename the \"Company\" column to \"Ticker\"\n",
    "final_df.rename(columns={\"Company\": \"Ticker\"}, inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path2 = 'Investment Research/Investment_Research_Test3_Final.csv'\n",
    "final_df.to_csv(csv_path2, index=False)\n",
    "print('Updated data saved to ' + csv_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3cced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee8d566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "csv_path2 = 'Investment Research/Investment_Research_Test3_Final.csv'\n",
    "df = pd.read_csv(csv_path2)\n",
    "\n",
    "# Function to truncate text at \"Glossary\" or \"Analyst Research Notes and other Company News\"\n",
    "def truncate_text_cf(text):\n",
    "    if isinstance(text, str):\n",
    "        glossary_index = text.find(\"Glossary\")\n",
    "        analyst_notes_index = text.find(\"Analyst Research Notes and other Company News\")\n",
    "        \n",
    "        # Find the earliest occurrence of either string\n",
    "        if glossary_index != -1 and analyst_notes_index != -1:\n",
    "            truncate_index = min(glossary_index, analyst_notes_index)\n",
    "        elif glossary_index != -1:\n",
    "            truncate_index = glossary_index\n",
    "        elif analyst_notes_index != -1:\n",
    "            truncate_index = analyst_notes_index\n",
    "        else:\n",
    "            truncate_index = -1\n",
    "        \n",
    "        if truncate_index != -1:\n",
    "            return text[:truncate_index]\n",
    "    return text\n",
    "\n",
    "# Function to truncate text at \"Argus Research Disclaimer\"\n",
    "def truncate_text_argus(text):\n",
    "    if isinstance(text, str):\n",
    "        argus_disclaimer_index = text.lower().find(\"argus research disclaimer\")\n",
    "        if argus_disclaimer_index != -1:\n",
    "            return text[:argus_disclaimer_index]\n",
    "    return text\n",
    "\n",
    "# Apply the function to rows where \"Contributor\" contains \"CFRA\"\n",
    "df.loc[df['Contributor'].str.contains('CFRA', na=False, case=False), 'Text'] = df.loc[df['Contributor'].str.contains('CFRA', na=False, case=False), 'Text'].apply(truncate_text_cf)\n",
    "\n",
    "# Apply the function to rows where \"Contributor\" contains \"Argus\" (case insensitive)\n",
    "df.loc[df['Contributor'].str.contains('Argus', na=False, case=False), 'Text'] = df.loc[df['Contributor'].str.contains('Argus', na=False, case=False), 'Text'].apply(truncate_text_argus)\n",
    "\n",
    "# Save the modified dataframe to a new CSV file\n",
    "new_csv_path = csv_path2.replace('.csv', '_Trimmed.csv')\n",
    "df.to_csv(new_csv_path, index=False)\n",
    "\n",
    "print(\"File saved as \" + new_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6e4fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "csv_path2 = 'Investment Research/Investment_Research_Test3_Final_Trimmed.csv'\n",
    "df = pd.read_csv(csv_path2)\n",
    "\n",
    "# Function to find the nearest closing quote around the midpoint\n",
    "def find_nearest_closing_quote(df, midpoint):\n",
    "    # Search for the closing quote in the \"Text\" column\n",
    "    closing_quotes = df['Text'].str.endswith('\"', na=False)\n",
    "    \n",
    "    # Find the index of the nearest closing quote before or after the midpoint\n",
    "    before_midpoint = closing_quotes[:midpoint][::-1].idxmax()\n",
    "    after_midpoint = closing_quotes[midpoint:].idxmax() + midpoint\n",
    "    \n",
    "    # Choose the nearest one\n",
    "    if midpoint - before_midpoint <= after_midpoint - midpoint:\n",
    "        return before_midpoint + 1  # +1 to include the closing quote line\n",
    "    else:\n",
    "        return after_midpoint + 1  # +1 to include the closing quote line\n",
    "\n",
    "# Calculate the midpoint of the dataframe\n",
    "midpoint = len(df) // 2\n",
    "\n",
    "# Find the nearest closing quote around the midpoint\n",
    "split_point = find_nearest_closing_quote(df, midpoint)\n",
    "\n",
    "# Split the dataframe into two halves\n",
    "df1 = df.iloc[:split_point]\n",
    "df2 = df.iloc[split_point:]\n",
    "\n",
    "# Save each half to a new CSV file\n",
    "csv_path1 = csv_path2.replace('.csv', '_1.csv')\n",
    "csv_path2 = csv_path2.replace('.csv', '_2.csv')\n",
    "\n",
    "df1.to_csv(csv_path1, index=False)\n",
    "df2.to_csv(csv_path2, index=False)\n",
    "\n",
    "print(\"Files saved as \" + csv_path1 + \" and \" + csv_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a1fcd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
